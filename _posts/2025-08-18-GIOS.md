---
title: CS 6200 - Graduate Introduction to Operating Systems
date: 2025-08-18 13:19:00 +0800
categories: [GaTech OMSCS, Operating Systems]
tags: [c, c++, powershell, linux, wsl]     # TAG names should always be lowercase
description: Overview & Projects from GIOS
---


# Welcome
#### Note
For all OMSCS courses - I will provide pseudocode and video links, but I need to keep code in a private repository to uphold the honor code. Descriptions, notes, etc are below. 

# Projects

## Setting up my test environment 
The first thing you do in GIOS is set up your test environment. There are a million different ways to do this, I settled on VS code and docker following the tutorial below: 
- [Set Up for Docker with VSCode and WSL2](https://docs.google.com/document/d/1-TLnnWRfcqFjuzlaAvsxXppVnlklKBb9FBv_stK86tg/edit?tab=t.0)


### VSCode and Docker 
GIOS repo: 
- https://github.gatech.edu/gios-fall-25/environment

#### Step 1
- Install Linux on Windows with WSL (for GIOSFA25 get UBUNTU 20.04)
- Install Docker Desktop and add GIOS image
- Resource: https://www.docker.com/ && 
https://learn.microsoft.com/en-us/windows/wsl/install
```powershell
wsl.exe --install Ubuntu-20.04
```
Notes:
1. Make sure you set your linux default WSL to the 20.04 ubuntu for GIOS (see Microsoft documentation)
2. Make sure you are on WSL 2 not 1 to avoid errors

#### Step 2
- Open WSL and get vscode on wsl
```bash
wsl
code .
```
Notes:
1. in VSCode get the Docker, Remote Development, and C/C++ extensions

#### Step 3
- Download all the absolutely necessary code provided in the GIOS repo in vscode (since it should have opened after it downloaded) or linux terminal 
Including: Add the course PPA, update the repos, install requirements (gcc should work after you've done this)

#### Step 4
- Add the tutorial's configure.yml file to a home folder you plan on using for the projects 
```yaml
name: cs6200-gios            # see Ref 1 below
services:
  cs6200-gios:               # see Ref 1 below
    image: gtomscs6200/fall25-environment:latest # note: set appropriately to current semester
    container_name: dev-env  # see Ref 1 below
    tty: true                # see Ref 2 below
    working_dir: /home/files
    restart: unless-stopped
    volumes:
      - .:/home/files:rw     # see Ref 3 below
```

- Ref 1: These names/labels can be set according to your preference

- Ref 2: https://stackoverflow.com/a/42597165

- Ref 3: general form is `<host-path>:<container-path>:rw`, where `.` here denotes the host-system 
- location in which `docker compose up -d` command is run on file `compose.yaml`, with
- corresponding bind mounting to container location `/home/files` as designated above. Note that you
- may designate `<container-path>` per your own preference. Suffix `:rw` denotes read & write modes.
- See https://docs.docker.com/compose/compose-file/ and https://docs.docker.com/compose/compose-file/compose-file-v3/#volumes for more details.

##### [Reference & Source for Step 4](https://docs.google.com/document/d/1-TLnnWRfcqFjuzlaAvsxXppVnlklKBb9FBv_stK86tg/edit?tab=t.0)
#### Step 5
Open the Dev Containers extension in VS Code. You should see cs6200-gios, and inside it, dev-env.
   

- If the dev environment is not running, right-click on dev-env and select Start.

- Once it’s running, right-click on dev-env again and select Attach Visual Studio Code to open the Docker container in VS Code.

You’ll know you’re inside the container if the remote connection status in the bottom left corner shows something like Container: gtomscs6200/fa25-environment.

#### Step 6
Open up a new terminal in your container in VSCode and run inotify
Resources:
- [Man Page](https://www.man7.org/linux/man-pages/man7/inotify.7.html)
- [inotify code](https://drive.google.com/file/d/1dMIPMESO3iK41j5fvkbTF4JVBU7XS0ej/view)

Make a new c file called inotify (or whatever you want), copy in the inotify code linked above and run the following

```bash
make inotify

./inotify . 
```
Output should look something like
```bash
root@54c5497edb9c:/home/files# ./inotify .
Press ENTER key to terminate.
Listening for events.
```
#### Step 7
Open up the split screen terminal windows and test out inotify
```bash
//on right side of the split terminal run the following tests:

ll //not one-one

cat inotify.c 
```
Output should look like this for >11
- LEFT
```bash
root@54c5497edb9c:/home/files# ./inotify .
Press ENTER key to terminate.
Listening for events.
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
```
- RIGHT
```bash
root@54c5497edb9c:/home/files# ll
total 56
drwxr-xr-x 6 1000 1000  4096 Aug 22 16:07 ./
drwxr-xr-x 1 root root  4096 Aug 22 15:25 ../
-rw-r--r-- 1 1000 1000   318 Aug 22 15:25 compose.yaml
-rwxr-xr-x 1 root root 17400 Aug 22 15:45 inotify*
-rw-r--r-- 1 root root  4677 Aug 22 15:44 inotify.c
drwxr-xr-x 3 1000 1000  4096 Aug 22 16:14 munit/
drwxr-xr-x 2 1000 1000  4096 Aug 22 15:21 p1/
drwxr-xr-x 2 1000 1000  4096 Aug 22 15:21 p3/
drwxr-xr-x 2 1000 1000  4096 Aug 22 15:21 p4/
```
AND this for >cat inotify.c
- LEFT
```bash
root@54c5497edb9c:/home/files# ./inotify .
Press ENTER key to terminate.
Listening for events.
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./inotify.c [file]
IN_CLOSE_NOWRITE: ./inotify.c [file]
```
- RIGHT

```bash
root@54c5497edb9c:/home/files# cat inotify.c
#include <errno.h>
#include <poll.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/inotify.h>
#include <unistd.h>
#include <string.h>

/* Read all available inotify events from the file descriptor 'fd'.
  wd is the table of watch descriptors for the directories in argv.
  argc is the length of wd and argv.
  argv is the list of watched directories.
  Entry 0 of wd and argv is unused. */

static void
handle_events(int fd, int *wd, int argc, char* argv[])
{
    /* Some systems cannot read integer variables if they are not
      properly aligned. On other systems, incorrect alignment may
      decrease performance. Hence, the buffer used for reading from
      the inotify file descriptor should have the same alignment as
      struct inotify_event. */

    char buf[4096]
        __attribute__ ((aligned(__alignof__(struct inotify_event))));
    const struct inotify_event *event;
    ssize_t len;

    /* Loop while events can be read from inotify file descriptor. */

    for (;;) {

        /* Read some events. */

        len = read(fd, buf, sizeof(buf));
        if (len == -1 && errno != EAGAIN) {
            perror("read");
            exit(EXIT_FAILURE);
        }

        /* If the nonblocking read() found no events to read, then
          it returns -1 with errno set to EAGAIN. In that case,
          we exit the loop. */

        if (len <= 0)
            break;

        /* Loop over all events in the buffer. */

        for (char *ptr = buf; ptr < buf + len;
                ptr += sizeof(struct inotify_event) + event->len) {

            event = (const struct inotify_event *) ptr;

            /* Print event type. */

            if (event->mask & IN_OPEN)
                printf("IN_OPEN: ");
            if (event->mask & IN_CLOSE_NOWRITE)
                printf("IN_CLOSE_NOWRITE: ");
            if (event->mask & IN_CLOSE_WRITE)
                printf("IN_CLOSE_WRITE: ");

            /* Print the name of the watched directory. */

            for (int i = 1; i < argc; ++i) {
                if (wd[i] == event->wd) {
                    printf("%s/", argv[i]);
                    break;
                }
            }

            /* Print the name of the file. */

            if (event->len)
                printf("%s", event->name);

            /* Print type of filesystem object. */

            if (event->mask & IN_ISDIR)
                printf(" [directory]\n");
            else
                printf(" [file]\n");
        }
    }
}

int
main(int argc, char* argv[])
{
    char buf;
    int fd, i, poll_num;
    int *wd;
    nfds_t nfds;
    struct pollfd fds[2];

    if (argc < 2) {
        printf("Usage: %s PATH [PATH ...]\n", argv[0]);
        exit(EXIT_FAILURE);
    }

    printf("Press ENTER key to terminate.\n");

    /* Create the file descriptor for accessing the inotify API. */

    fd = inotify_init1(IN_NONBLOCK);
    if (fd == -1) {
        perror("inotify_init1");
        exit(EXIT_FAILURE);
    }

    /* Allocate memory for watch descriptors. */

    wd = calloc(argc, sizeof(int));
    if (wd == NULL) {
        perror("calloc");
        exit(EXIT_FAILURE);
    }

    /* Mark directories for events
      - file was opened
      - file was closed */

    for (i = 1; i < argc; i++) {
        wd[i] = inotify_add_watch(fd, argv[i],
                                  IN_OPEN | IN_CLOSE);
        if (wd[i] == -1) {
            fprintf(stderr, "Cannot watch '%s': %s\n",
                    argv[i], strerror(errno));
            exit(EXIT_FAILURE);
        }
    }

    /* Prepare for polling. */

    nfds = 2;

    fds[0].fd = STDIN_FILENO;       /* Console input */
    fds[0].events = POLLIN;

    fds[1].fd = fd;                 /* Inotify input */
    fds[1].events = POLLIN;

    /* Wait for events and/or terminal input. */

    printf("Listening for events.\n");
    while (1) {
        poll_num = poll(fds, nfds, -1);
        if (poll_num == -1) {
            if (errno == EINTR)
                continue;
            perror("poll");
            exit(EXIT_FAILURE);
        }

        if (poll_num > 0) {

            if (fds[0].revents & POLLIN) {

                /* Console input is available. Empty stdin and quit. */

                while (read(STDIN_FILENO, &buf, 1) > 0 && buf != '\n')
                    continue;
                break;
            }

            if (fds[1].revents & POLLIN) {

                /* Inotify events are available. */

                handle_events(fd, wd, argc, argv);
            }
        }
    }

    printf("Listening for events stopped.\n");

    /* Close inotify file descriptor. */

    close(fd);

    free(wd);
    exit(EXIT_SUCCESS);
}
root@54c5497edb9c:/home/files# 
```

#### Other Unit Tests
```
git clone https://github.com/nemequ/munit.git

gcc -Wall example.c munit.c munit.h -o example

./example
```
Output should look like 
```bash
Running test suite with seed 0x5a13d21b...
/example/compare                     [ OK    ] [ 0.00006970 / 0.00000230 CPU ]
/example/rand                        [ OK    ] [ 0.00000620 / 0.00000560 CPU ]
/example/parameters                  
  foo=one, bar=red                   [ OK    ] [ 0.00000610 / 0.00000540 CPU ]
  foo=one, bar=green                 [ OK    ] [ 0.00000610 / 0.00000540 CPU ]
  foo=one, bar=blue                  [ OK    ] [ 0.00000600 / 0.00000540 CPU ]
  foo=two, bar=red                   [ OK    ] [ 0.00000600 / 0.00000550 CPU ]
  foo=two, bar=green                 [ OK    ] [ 0.00000600 / 0.00000540 CPU ]
  foo=two, bar=blue                  [ OK    ] [ 0.00000870 / 0.00000750 CPU ]
  foo=three, bar=red                 [ OK    ] [ 0.00000620 / 0.00000570 CPU ]
  foo=three, bar=green               [ OK    ] [ 0.00000640 / 0.00000570 CPU ]
  foo=three, bar=blue                [ OK    ] [ 0.00000600 / 0.00000540 CPU ]
11 of 11 (100%) tests successful, 0 (0%) test skipped.
```
#### Notes for me:
- project is held on ubuntu 20.04 callled 'gios'
```bash
gios@DESKTOP-QUQS18F://home/gios/projects$
```
## Overview - Sequence of Projects 
1. Threads
3. Concurrency
4. Synchronization

Projects will cover single-node OS mechanisms (inter-process communication, scheduling, etc) and multi-node OS mechanisms (remote procedure calls (RPC), etc) and experimental design and evaluation 
All programs are in C and Linux
## Project 1

### Warm Up 

### gfclient:
#### libcurl breakdown
The libcurl “easy” workflow

Using libcurl, the steps look like this:

1. Global init (optional)

```c
curl_global_init(CURL_GLOBAL_DEFAULT);
```


Sets up any shared resources. In project this maps to gfc_global_init().

2. Create handle

```c
CURL *curl = curl_easy_init();
```


Allocates a request object. In project: *gfc_create()*.

3. Set options on the handle
```c
curl_easy_setopt(curl, CURLOPT_URL, "http://example.com");
curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, my_write_func);
curl_easy_setopt(curl, CURLOPT_WRITEDATA, my_data);
```

Each call changes a field inside the handle.
In your project: gfc_set_server(), gfc_set_path(), gfc_set_writefunc(), gfc_set_writearg(), etc.

4. Perform the transfer
```c
res = curl_easy_perform(curl);
```

Blocking call — does DNS lookup, connects, sends request, streams data, invokes callbacks.
In project: gfc_perform().

5. Check result & get information
```c
curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE, &code);
```

In project: gfc_get_status(), gfc_get_filelen(), gfc_get_bytesreceived().

6. Cleanup handle
```c
curl_easy_cleanup(curl);
```

Frees memory for the request. In project: gfc_cleanup().

7. Global cleanup (optional)
```c
curl_global_cleanup();
```

Tears down global state. In project: gfc_global_cleanup().

### Mapping libcurl's API to GETFILE protocol

| libcurl easy function          | gfclient equivalent                                                                                         |
| ------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| `curl_global_init()`           | `gfc_global_init()`                                                                                         |
| `curl_easy_init()`             | `gfc_create()`                                                                                              |
| `curl_easy_setopt(handle, …)`  | `gfc_set_server()`, `gfc_set_port()`, `gfc_set_path()`, `gfc_set_headerfunc()`, `gfc_set_writefunc()`, etc. |
| `curl_easy_perform(handle)`    | `gfc_perform()`                                                                                             |
| `curl_easy_getinfo(handle, …)` | `gfc_get_status()`, `gfc_get_filelen()`, `gfc_get_bytesreceived()`                                          |
| `curl_easy_cleanup(handle)`    | `gfc_cleanup()`                                                                                             |
| `curl_global_cleanup()`        | `gfc_global_cleanup()`                                                                                      |

## Project 3

## Project 4

# Lecture Notes

## P1L1 - Introduction to Operating Systems 
The course covers:
- what are operating systems
- why are operating systems needed
- how are operating systems designed and implemented? 

Course Topics include OS abstractions, mechanisms, and policies for
- processes and process management
- threads and concurrency
- resource management: scheduling, memory management
- OS services for communication and I/O
- OS support for distributed services
## P1L2 - What is a Operating System?

In simplest terms, it is a piece of software that abstracts and arbitrates the hardware of a system. 

#### 2. An operating system is like a toy shop manager:

Toy Shop Manager
- Directs operational Resources by controlling use of employee time, parts, tools
- Enforces working policies - fairness safety, clean up
- Mitigates difficulty of complex tasks - simplifies operation and optimizes performance

Operating systems
- Directs operational Resources - controls use of CPU, memory, peripheral devices
- Enforces working policies - fair resouce access, limits to resource usage
- Mitigates difficulty of complex tasks - abstract hardware details (system calls)

Operating System Definition

![OS Definition](/assets/img/Posts/GIOS/P1L2whatisanOS.png){: width="972" height="589" .w-50 .left}

An operating system is a layer of systems software that:
- directly has priviledged access to the underlying hardware;
- hides the hardware complexity;
- manages hardware on behalf of one or more applications according to some predefined policies
- In addition, it ensures that applications are isolated and protected from one another 

Important definitions: abstraction versus arbitration, scheduler, device driver, file system, memory management, OS Design principals 

#### 8. Operating systems Examples
Desktop (microsoft Windows, UNIX-based OS (including mac OS X (BSD), Linux)) and Embedded (Android, iOS, Symbian) are the focus of the class 

#### 9. OS Elements
Abstractions (nouns):
- process, thread, file, socket, memory page

Mechanisms (verbs):
- create, schedule, open, write, allocate

Policies:
- least recently used (LRU), earliest deadline first (EDF)

#### 10. Design Prinipals 
- Separation of mechanism & policy: implement flexible mechanisms to support many policies (e.g. LRU, LFU, random)
- Optimize for the common case: where will the OS be used? What will the user want to execute on that machine? What are the workload requirements? 

#### 11. User/Kernel Protection Boundary 
![User Kernel](/assets/img/Posts/GIOS/P1L2Userkernelbundary.png){: width="972" height="589" .w-50 .left}
The user/kernel protection boundary exists to keep users outside of the kernel level. There is a user-kernel switch that is supported by hardware that include trap instructions (unauthorized kernel access), system call (open (file), send (socket), malloc (memory)), and signals 

#### 12. System Calls Flowchart

![System Call Flowchart](/assets/img/Posts/GIOS/P1L2 Systemcallflowchart.png){: width="972" height="589" .w-50 .left}

To make a system call, an application must
•	write arguments,
•	save all relevant data at a well-defined location (The well-defined location is necessary so that the operating system kernel, based on the system call number, can determine which, how many arguments it should retrieve and where they are)
•	make the actual system call using this specific system call number. 
The arguments can either be passed directly between the user program and the operating system, or they can be passed indirectly by specifying their address.

#### 13. Crossing the User/Kernel Protection Boundary 
User/kernel transitions are:
- hardware supported (e.g. traps on illegal instructions or memory accesses requiring special priviledge)
- involves a number of instructions (e.g. ~50-100ns on a 2 GHz machine running linux
- switches locality (affects hardware cache)
- NOT CHEAP!
#### 14. Basic OS Services

Scheduler - responsible for controlling access to the CPU

Memory Manager - responsible for allocating the underlying physical memory to one or more co-running applications. It also maskes sure that applications don't overwrite eachothers memory. 

Block device driver - responsible for access to a block device like a disk

File system - part of the higher level services that are not necessarily related to the hardware but are one of the higher level abstractions supported by the operating system. 

![Basic OS](/assets/img/Posts/GIOS/P1L2BasicOSServices.png){: width="972" height="589" }

#### 14. Monolithic OS 
![Mono OS](/assets/img/Posts/GIOS/P1L2Mono.png){: width="972" height="589" .w-50 .left}
Historically, operating systems were monolithic. This could include file systems that were specialized in sequential workloads, or file systems optimized for accessing databases. 
- Pros: The benefit of this approach is that everything is included in the operating system. The abstractions, all the services, and everything is packaged at the same time. And because of that, there's some possibilities for some compile-time optimizations.
- Cons: The downside is that there is too much state, too much code that's hard to maintain, debug, upgrade. And then its large size also poses large memory requirements, and that can always impact the performance that the applications are able to observe.

#### 15. Modular OS (more modern)
![Modular OS](/assets/img/Posts/GIOS/P1L2Modular.png){: width="972" height="589" .w-50 .left}
The modular approach is the more modern/common approach. It is used with the Linux OS. Everything can be added as a module. With this approach, you can customize which particular file system or scheduler the operating system uses. 
You can also dynamically install new modules in the operating system. 
- Pros: easier to maintain/upgrade, less resource intensive.
- Cons: Can reduce potential optimizations because of the level of indirection required because you have to go through interface specification before you can go to the implementation of a service. 

#### 16. Microkernel (embedded systems)
![Modular OS](/assets/img/Posts/GIOS/P1L2Microkernel.png){: width="972" height="589" .w-50 .left} Require only the basic primitives at the operating system level. For instance, at the OS level, the microkernel can support some basic services such as representing an executing application, its address space, and threads. 
- Pros: smol, easy to verify and test code. This is particularly important for operating systems that need to behave properly. 
- Cons: portability is questionable. Because there are so many one off instances of embedded systems, it can become difficult to find common components which leads to complexity. Also there is the potential for frequent user/kernel crossings which is costly. 

#### 17. Linux Architecture + Mac OS
Below is what the Linux environment looks like.
- 	Starting at the bottom, we have the hardware, and the Linux kernel abstracts and manages that hardware by supporting a number of abstractions and the associated mechanisms.
-	Then comes a number of standard libraries, such as those that implement the system call interfaces,
-	Then a number of utility programs that make it easier for users and developers to interact with the operating system.
-	Finally, at the very top, you have the user developed applications.

![Linux OS](/assets/img/Posts/GIOS/P1L2Linuxarch.png){: width="972" height="589" .w-50 .left}The kernel, itself, consists of several logical components, like all of the I/O management, memory management, process management. 

The Mac OSX operating system, from Apple, uses a different organization.
•	At the core is the Mach microkernel which implements key primitives like memory management, thread scheduling and inter-process communication mechanisms including what we call RPC.
•	The BSD component provides Unix interoperability via a BSD command line interface, POSIX API support as well as network I/O.
•	All application environments sit above this layer.
•	The bottom two modules are environments for development of drivers, and for kernel modules that can be dynamically loaded into the kernel.

#### Summary
In this lesson, we answered the big question, what is an operating system? And we saw that it's important because it helps abstract and arbitrate the use of the underlying hardware system.
-	We explained that to achieve this, an operating system relies on
-	abstractions, such as processes and threads
-	mechanisms that allow OS to manipulate abstractions
-	policies that specify how abstractions can be modified.
-	We saw that operating systems support a system call interface that allows applications to interact with them.
-	We looked at several alternatives in organizational structures for operating systems.
-	Then very briefly, we looked at some specific examples of operating systems, Windows, Linux, and Mac OS to see some examples of their system call interfaces or their organization.

## P2L1 - Introduction to Processes and Process Management
A process is an instance of an executing program, can be synonymous with task or job

#### Visual Metaphor
A process is like an order of toys
- State of execution (completed toys and toys waitig to be built)
- Parts amd temporary holding area (plastic pieces and containers, etc)
- May require special hardware (sewing machine, glue gun, etc)
A process equivalent:
- State of execution (program counter, stack)
- Parts amd temporary holding area (data, register state occupies state in memory)
- May require special hardware (I/O devices like disks/network devices)

#### What is a Process?
One of the roles of the OS is to manage hardware on behalf of the applications
- Application == program on disk, flash memory, etc (static entity)
- Process == state of a program when executing loaded in memory  (active entity)
An example could be a txt editor with saved lecture notes versus an untitled txt editor open, both are examples of processes

#### What does a process look like? (important)
![Process example](/assets/img/Posts/GIOS/P2L1Processexamp.png){: width="972" height="589" .w-50 .left} 
A process encapsulates all states of a running application. This includes the code, the data, all the variables that the application needs to allocate. Every single aspect of the process state has to be uniquely identified by its address. So an OS abstraction used to encapsulate all of the process state is an address space, which is shown in the image.*The address space is defined by a range of addresses from v0 to some vmax and different types of process state will appear in different regions in this address space.*

What are the different types of state in a process?
- test and data (static state when process first loads)
- heap (dynamically created during execution)
- stack (grows and shrinks during execution in a LIFO way)

#### Process Address Space
![Address example](/assets/img/Posts/GIOS/P2L1address.png){: width="972" height="589" .w-50 .left} The process representation is referred to as an address space. The potential range of addresses from vo to vmax, v in that range will be called a virtual address. They care called virtual because they don't have to correspond to actual locations in the physcial memory. Instead the memory management hardware and operating system components responsible for memory management, like page tables, maintain a mapping between the virtual addresses and the physical addresses. 

#### Process Address Space and Memory Management
Not all processes require the entire address space , on the flip side, you can run into the situation where you do not have enough memory to run your processes. To deal with this the operating system dynamically decides which portion of which address space will be present and where in physical memory. Regions of the physical memory can be occupied by different physical processes. With that said - each process the operating system must maintain some information regarding the process address space
#### Virtual Addresses Quiz

If two processes, P1 and P2, are running at the same time, what are the ranges of their virtual address space that they will have?

•	P1 has address ranges from 0 to 32,000, and P2 from 32,001 until 64,000.
•	Both P1 and P2 have address ranges from 0 to 64,000. (correct)
•	P1 has an address space range from 32,001 to 64,000, and P2 has address ranges from 0 to 32,000. 

#### How does the OS know what a process is doing?
- Program Counter (PC) maintained on the CPU while the process is executing in a register and there are other registers that are maintained on the CPU. They may have information like addresses for data, or they may have some status information that somehow affects the execution of the sequence
- CPU registers
- Stack Pointer, the top of th stack if defined by the stack pointer. Because the stack exhibits LIFO behavior, we need to know whats on top. 
All of this is maintained in the OS using a Process Control Block (PCB)

#### Process Control Block
A Process Control Block (PCB) is a data structure that the operating system maintains for every one of the processes that it manages.
![PCB](/assets/img/Posts/GIOS/P2L1PCB.png){: width="972" height="589" .w-50 .left}
From what we’ve seen so far the PCB must contain:
-	processes state like the program counter, the stack pointer, really all of the CPU registers (their values as they relate to the particular process),
-	various memory mappings that are necessary for the virtual to physical address translation for the process,
-	and other things. Some of the other useful information includes a list of open files for instance, information that’s useful for scheduling like how much time this particular process has executed on the CPU, how much time it should be allocated in the future (this can depend on the process priority), etc.

some aspects of the PCB change when a process state changes. An example being when a process requests more memory the OS will allocate more memory and establish a new valid virtual to physical memory mapping for this process. 
Other fields of the PCB change more frequently. During the execution of a program the program counter changes on every single instruction on a dedicated register. 

#### How is a PCB Used? 
![PCB how is used](/assets/img/Posts/GIOS/P2L1howispcbused.png){: width="972" height="589" .w-50 .left} Running through an example to explain. Assume you have an OS managing two processes, P1 and P2. They are created and their PCBs are stored in memory. If P1 is running on the CPU and P2 is idle, then the OS decides to interrupt P1, then P1 will become idle. At that point the OS will have to save the state information regarding P1, including the CPU registers into the PCB for P1. Then the OS needs to restore the state for P2 so it can execute (updating the CPU registers). When P2 is done or interupted, the PCB will beed to save the P2 information and the P1 PCB will need to be restored. P1 will now be running and the CPU registers will reflect the state of P1. Given that the value of the PCB for P1 corresponds exactly to the values it had when we interrupted P1 earlier. That means that P1 will resume its execution at the exact same point where it was interrupted earlier by the OS.

#### What is a context switch?

![Context Switch text](/assets/img/Posts/GIOS/P2L1ContextSwitch.png){: width="972" height="589" .w-50 .left} A context switch is when we switch the CPU from the context of one process to the context of another process. It requires an update in PCB memory or an update in the CPU cache. formally stated, a context switch is the mechanism used by the operating system to switch the execution from the context of one process to the context of another process. It can be expensive for two reasons, direct costs: the number of cycles that have to be executed to simply load and store all the values of the process control blocks to and from memory, and indriect costs: needing to access the memory when you context switch if you need any information from the previous process (cold cache), where as if you are running P1 and you need to access P1 information, the cache is readily accessible. 

#### Hot Cache Quiz
13 - Hot Cache Quiz
Here's a quick quiz about the processor cache. 
❏	When a cache is hot, it can malfunction, so we must context switch to another process. 
❏	When a cache is hot most process data is in the cache, so the process performance will be at its best. (correct)
❏	When a cache is hot, sometimes we must context switch. (correct)

#### Process Lifecycle 

![Process Lifecycle](/assets/img/Posts/GIOS/P2L1ProcessLifecycle.png){: width="972" height="589" .w-50 .left} All processes can have two states, running or idling. When a process is running, it can be interrupted and context-switched. At this point, the process is idle, but it's in what we call a ready state. It is ready to execute, except it is not the current process that is running from the CPU. At some later point, the scheduler would schedule that process again, and it will start executing on the CPU, so it will move into the running state.
![Process states](/assets/img/Posts/GIOS/P2L1Processstates.png){: width="972" height="589" .w-50 .left} Other states that a process can be in are shown in the img. There is an example to run through the process. 
1. when a process is created, it enters the new state. This is when the OS will perform admission control, and if it's determined that it's okay, the operating system will allocate and initiate a process control block (PCB) and some initial resources for this process. Provided that there are some minimum available resources, the process is admitted, and at that point, it is ready to start executing.
2. It is ready to start executing, but it isn't executing on the CPU. It will have to wait in this ready state until the scheduler is ready to move it into a running state when it schedules it on the CPU.
3. Once the scheduler gives the CPU to a ready process, that ready process is in the running state. And from here, a number of things can happen. First, the running process can be interrupted so that a context switch is performed. This would move the running process back into the ready state. Another possibility is that a running process may need to initiate some longer operation, like reading data from disk or to wait on some event like a timer or input from a keyboard. 
3. *At that point, the process enters a waiting state. When the event occurs or the I/O operation completes, the process will become ready again.*
4. Finally, when a running process finishes all operations in the program or when it encounters some kinds of error, it will exit. It will return the appropriate exit code, either success or error, and at that point, the process is terminated.

#### Process State Quiz
The CPU is able to execute a process when the process is in which of the following states?
- Running (correct)
- Ready (correct)
- New
- Waiting

#### Process Creation
In OS, a process can create child processes. All processes come from a single root and they have some relationship to one another through parent child behavior. Priviledged processes are root processes, most of the OS processes are root processes, so the OS will be loaded onto the machine after it boots up and it will create a number of initial processes. 
![Process creation](/assets/img/Posts/GIOS/P2L1Processcreate.png){: width="972" height="589" .w-50 .left} When a user logs into a system a user shell process is created and then when the user types in command like ls, or emacs, then new processes get spawned from that shell parent process. So the final relationship looks like this tree. Most operating systems support two basic mechanisms for process creation, fork and exec. A process can create a child via either one these mechanisms.
- With the fork() mechanism the OS will create a new PCB for the child and copy the exact same values from parent PCB into the child PCB.
- Exec() behaves differently. It will take a PCB structure created via fork, but it will not leave its values to match the parent’s values. 

#### Parent Process Quiz
On UNIX-based systems, init is the first process that starts after the system boots. And because all other processes can ultimately be traced to init, it's referred to as the parent of all processes.
On the Android OS, Zygote is a daemon process which has the single purpose of launching app processes. 

#### Role of the CPU Scheduler
![CPU Scheduler](/assets/img/Posts/GIOS/P2L1CPUscheduler.png){: width="972" height="589" .w-50 .left} . For the CPU to start executing a process the process must be ready first. The problem is, however, there will be multiple ready processes waiting in the ready queue. How do we pick what is the right process that should be given the CPU next, that should be scheduled on the CPU? So the question is which process do we run next? This is determined by a component call the CPU scheduler.
- The CPU Scheduler is an OS component that manages how processes use the CPU resources. It decides which one of the currently ready processes will be dispatched to the CPU to start running and how long it should run for, and it also determines how long this process should be allowed to run for.

Over time this means that in order to manage the CPU:
- The OS must be able to preempt, to interrupt the executing process and save its current context. This operation is called preemption.
- Then the OS must run the scheduling algorithm to choose one of the ready processes that should be run next.
- Once the processes are chosen, the OS must dispatch this process onto the CPU and switch into its context so that that process can finally start executing.

Given that the CPU resources are precious, the OS needs to make sure that CPU time is spent running processes and not executing scheduling algorithms and other OS operations. So OS should minimize the amount of time that it takes to perform these tasks (preemption, scheduling, and dispatching), i.e., the OS must be efficient. 

#### Length of process
How often do we run the scheduler? The more frequently we run it the more CPU time is wasted on running the scheduler vs running application processes. So another way to ask the same question is: How long should a process run? The longer we run a process the less frequently we are invoking the scheduler to execute.
- Useful CPU work == Total processing time/Total time = (2 * Tp)/(2 * Tp+2*t_scheduled) if Tp == t_scheduled => only 50% of CPU time spent of useful work!
- If Tp (processing time) is much larger than scheduling time, t_scheduled then ~ 91% of the CPU time is spent doing useful work, we want a big Tp. 
- timeslice == time Tp allocated to a process on the CPU. 
![timeslice](/assets/img/Posts/GIOS/P2L1timeslice.png){: width="972" height="589" .w-50 .left} As you can see there are a lot of design decisions and tradeoffs that we must make when we’re considering how to design a scheduler. Some of these include deciding what are appropriate timeslice values, or deciding what would be good metrics that are useful when the scheduler is choosing what’s the next process it should run.

#### What about I/O?
So far we know the OS manages how processes access resource on the hardware platform and this in addition to the CPU and memory will include I/O devices, peripherals like keyboards, network cards, disks, etc.
1. imagine a process had made an I/O request, the OS delivered that request, for instance, it was a read request to disk.
2. then the process is placed on the I/O queue that’s associated with that particular disk device. So the process is now waiting in the I/O queue.
3. the process will remain waiting in the queue until the device completes the operations, so the I/O event is complete, and responds to that particular request.
4. once the I/O request is met, the process is ready to run again and depending on the current load in the system it may be placed in the ready queue, or it may be scheduled on the CPU if there is nothing else waiting in the ready queue before it.
![I/O](/assets/img/Posts/GIOS/P2L1IO.png){: width="972" height="589" .w-50 .left} 
So to summarize, a process can make its way into the ready queue in a number of ways. 
-	A process which was waiting on an I/O event ultimately found its way into the ready queue.
-	A process which was running on the CPU but its timeslice expired goes back on the ready queue.
-	When a new process is created via the fork call it ultimately ends its way on the ready queue.
-	For a process which was waiting for interrupt, once the interrupt occurs it will also be placed on the ready queue.

#### Scheduler Responsibility Quiz
The question is, which of the following are NOT a responsibility of the CPU scheduler? The options are:
❏	maintaining the I/O queue (correct)
❏	maintaining the ready queue
❏	deciding when to context switch
❏	or deciding when to generate an event that a process is waiting on. (correct)

#### Can Processes Interact 
Another natural question can be, can processes interact? And the simple answer is YES. An operating system must provide mechanisms to allow processes to interact with one another.
![Process Interaction](/assets/img/Posts/GIOS/P2L1processinteract.png){: width="972" height="589" .w-50 .left} Remember that the operating systems go through a great deal to protect and isolate processes from one another. Each of them is a separate address space. They control the amount of CPU each process gets, which memory is allocated, and accessible to each process. So these communication mechanisms that we will talk about somehow have to be built around those protection mechanisms. These kinds of mechanisms are called inter-process communication mechanisms, or we refer to them as IPC (interprocess communication)

The IPC mechanisms:
-	Help transfer data and information from one address space to another
-	Maintain the protection and isolation that operating systems are trying to enforce
-	IPC mechanisms need to provide flexibility as well as clearly performance, because different types of interactions between processes may exhibit different properties. Periodic data exchanges, continuous stream of data flowing between the processes, or coordinated applet, to some shared single piece of information.

The Message Passing IPC:
One mechanism that operating systems support is message passing IPC. The operating system establishes a communication channel, like a shared buffer, and the processes interact with it by writing or sending a message into that buffer. Or, reading or receiving a message from that shared communication channel. 
![Process Interaction Message](/assets/img/Posts/GIOS/P2L1messagepass.png){: width="972" height="589" .w-50 .left} The benefit of this approach is that it's the operating system who will manage this channel, and it's the operating system that provides the exact same APIs, the exact same system calls for writing or sending data, and the reading or receiving data from this communication channel. The downside is the overhead. For every single piece of information that we want to pass between these two processes, we have to copy from the user space of the first process into this channel that's sitting in the OS, in the kernel memory. And then back into the address space of the second process.

The shared memory IPC:
The other type of IPC mechanism is what we call shared memory IPC. The way this works is the operating system establishes the shared memory channel, and then it maps it into the address space of both processes. The processes are then allowed to directly read and write from this memory, as if they would to any memory location that's part of their virtual address space. So the operating system is completely out of the way in this case.
![shared memory](/assets/img/Posts/GIOS/P2L1sharedmemory.png){: width="972" height="589" .w-50 .left} That in fact is the main advantage of this type of IPC. That the operating system is not in the path of the communication. So the processes, while they're communicating are not going to incur any kind of overheads from the operating system. The disadvantage of this approach is because the operating system is out of the way it no longer supports fixed and well defined APIs how this particular shared memory region is used. For that reason, its usage sometimes becomes more error prone, or developers simply have to re-implement code to use this shared memory region in a correct way.

#### Lesson Summary
We covered process and process management:
- Process and process-related abstractions including address space and PCB
- Basic mechanisms for managing process resources (context, switching, process creation, scheduling, interprocess communication)


## P2L2 - Threads and Concurrency 

### 1. Visual Metaphor

A thread is like a worker in a toyshop

- worker in a toyshop is a:
1. active entity
2. executing unit of toy order
3. works simultaneously with others
4. many workers completing toy orders
5. requires coordination (sharing of tools, parts, workstations, etc)

- a thread:
1. is an active entity (executing unit of a process)
2. works simultaneously with others (many threads executing)
3. requires coordination (sharing of I/O devices, CPUs, memory)

### 2. Process versus thread
![process v thread](/assets/img/Posts/GIOS/P2L2/img1.png){: width="972" height="589" .w-50 .left}


A single threaded process is represented by its address space (i.e. code/data/files). The address space will contain all of the virtual to physical address mappings for the process, for its code, its data, its heap section files, for everything. The process is also represented by its execution context (i.e. registers/stack) that contains information about the values of the registers, the stack pointer, program counter, etc. The operating system represents all this information in a process control block (PCB = code/data/files + regs/stack).

Threads represent multiple independent execution contexts. They're part of the same virtual address space, which means that they will share all of the virtual to physical address mappings. They will share all the code, data, files. However, they will potentially execute different instructions, access different portions of that address space, operate on different portions of the input, and differ in other ways. This means that each thread will need to have a different program counter, stack pointer, stack, thread-specific registers. So for each and every thread, we have to have separate data structures to represent this per-thread information.

extra context - https://stackoverflow.com/questions/9501526/what-is-the-difference-b-w-tcbthread-control-block-pcbprocess

### 3. Why are threads useful?
- By parallelizing the program in this manner, we achieve speed up. We can process the input much faster than if only a single thread on a single CPU had to process the entire input.
- threads may execute completely different portions of the program. For instance, you may designate certain threads for certain I/O tasks like input processing, or display rendering
- one benefit from specialization is that we end up executing with a hotter cache. And that translates to gains in performance.

Why not just write a multi-process application where every single processor runs a separate process?
- If we do that, since the processes do not share an address space (AS) we have to allocate for every single one of these contexts AS and execution context (EC). So the memory requirements, if this were a multiprocessor implementation, would be that we have to have four Address Space allocations and four Execution Context allocations. 
- synchronization among processes, requires IPC mechanisms that are costlier. 
- A multithreaded implementation results in threads sharing an AS so we don’t need to allocate memory for all of the AS information for these remaining ECs. This implies that a multithreaded application is more memory efficient, it has lower memory requirements than its multi-process alternative
### 4. Are threads useful on a single CPU or when # of threads > # of CPU?

![process v thread](/assets/img/Posts/GIOS/P2L2/img3.png){: width="972" height="589" .w-50 .left}

- Threads save time in context switching compared to processes because they share and AS, so they don't need to create new virtual to physical address mappings for new process scheduling. 

### 5. Benefits to applications and OS code
By multithreading the OS’s kernel we allow the OS to support multiple EC, and this is particularly useful when there are multiple CPUs, so that the OS context can execute concurrently on different CPUs in a multiprocessor/multicore platform.
![process v thread](/assets/img/Posts/GIOS/P2L2/img4.png){: width="972" height="589" .w-50 .left}

#### Process vs Threads Quiz
•	[Threads] The first statement applies to threads. Each thread belonging to a process shares the virtual address space with other threads in that process.
•	[Processes] Because threads share the address space, the context switch among them happens faster than processes. So, processes take longer to context switch.
•	[Both]Both threads and processes have their execution context described with stack and registers.
•	[Threads]Because threads share the virtual address space, it is more likely that when multiple threads execute concurrently, the data that's needed by one thread is already in the cache, brought in by another thread. So, they typically result in hotter caches. Among processes, such sharing is really not possible.
•	[Both]Then the last answer is B. We already saw that for processes, it makes sense for the operating system to support certain inter-process communication mechanisms. And we'll see that there are mechanisms for threads to communicate and coordinate and synchronize amongst each other.

### 6. Thread Mechanisms 
 - Thread data structure (idenitfy threads, keep track of resource usage
 - mechanisms to create and manage threads
 - mechanisms to safely coordinate among threads running concurrently in the same address space

Issues with concurrent execution

threads share the same virtual to physical address mappings
![process v thread](/assets/img/Posts/GIOS/P2L2/img5.png){: width="972" height="589" .w-50 .left}

To deal with such concurrence, we need a mechanism for threads to execute exclusively, which is called mutual exclusion. Mutual exclusion is a mechanism where only one thread at a time is allowed to perform an operation. The remaining threads, if they want to perform the same operation, must wait their turn. The actual operation that must be performed in mutual exclusion may include some update to state, or access to some data structure that's shared among all these threads.

threads share the same virtual to physical address mappings
![process v thread](/assets/img/Posts/GIOS/P2L2/img6.png){: width="972" height="589" .w-50 .left}

### Threads and Thread Creation
First, we need some data structure to represent a thread. The thread type proposed by Birrell is a data structure that contains all the information that is specific to a thread and can describe a thread. This includes
-	the thread identifier that the threading system will use to identify a specific thread,
-	register values, in particular the stack of a thread,
-	the program counter and the stack pointer,
-	and any other thread specific data or attributes. 

Second - For thread creation, Birrell proposes a fork call with two parameters,
•	a proc argument, which is the procedure that the created thread will start executing,
•	and args, which are the arguments for this procedure.
*Not to be confused with UNIX fork!
![process v thread](/assets/img/Posts/GIOS/P2L2/img7.png)
When a thread T0 calls a fork a new thread T1 is created. That means that new thread data structure of this type is created and its fields are initialized such that its program counter will point to first instruction of the procedure proc, and these arguments will be available on this stack of the thread. After the fork operation completes, the process as a whole has two threads. T0, the parent thread, and T1. These can both execute concurrently. T0 will execute the next operation after the fork call, and T1 will start executing with the first instruction in proc, with the specified arguments.

Next -  we need some mechanism to determine
•	if a thread is done,
•	and if it is necessary to retrieve its result,
•	or at least to determine the status of the computation (success or error).
To deal with this issue, Birrell proposes a mechanism he calls join. It has the following semantic: child_result = join(id of child thread).
When a parent thread calls join, it will be blocked until the child thread completes. Join will return to the parent the result of the child’s computation. At that point, the child thread exits the system; any allocated data structure state for the child, all of the resources that were allocated for its execution will be freed and the child thread is terminated.

### Thread Creation Example
![process v thread](/assets/img/Posts/GIOS/P2L2/img8.png)
### Mutexes
![process v thread](/assets/img/Posts/GIOS/P2L2/img9.png)
### Mutual Exclusion
There is a danger in the previous example that the parent and the child thread will try to update the shared list at the same time potentially overwriting the list elements.
To do this the OS and threading libraries in general support a construct called mutex. 
Other threads attempting to lock the same mutex are not going to be successful. These threads will be blocked from performing the lock operation. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img10.png) The portion of the code protected by the mutex, is called critical section. In Birrell’s paper, this is any code within the curly brackets of the lock operating that he proposes to be used with mutexes. The critical section code should correspond to any kind of operation that requires that only one thread at a time to perform the operation. For instance, it can be updated to shared variable like the list, or increasing/decreasing a counter, or performing any type of operation that requires mutual execution between the threads. Other than the critical section code, the rest of the code in the program, the threads may execute them concurrently.

### Making safe_insert safe
![process v thread](/assets/img/Posts/GIOS/P2L2/img11.png)

### Mutex Quiz
![process v thread](/assets/img/Posts/GIOS/P2L2/img12.png)

### Producer/Consumer Example
For threads the first construct that Birrell advocates is mutual exclusion, and that’s a binary operation. A resource is either free and you can access it, or it is locked and you have to wait. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img13.png)
Operating this way is clearly wasteful and it would be much more efficient if we could just tell the consumer when the list is full so that it can at that point go ahead and process the list.

### Condition Variable
Birrell recognizes this common situation in multithreaded environments and argues for a new construct, a condition variable. He says that such condition variables could be used in conjunction with mutexes to control the behavior of concurrent threads. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img14.png)

### Condition Variable API
![process v thread](/assets/img/Posts/GIOS/P2L2/img15.png)

#### Condition Variable Quiz 
![process v thread](/assets/img/Posts/GIOS/P2L2/img16.png)

### Reader/Writer Problem (review)
The readers/writer problem is such that at any given point of time, zero or more of the reader's threads can access the resource concurrently, but only zero or one writer threads can access the resource at the same time. And clearly we cannot have a situation in which a reader and a writer thread are accessing the shared resource at the same time.
- One naive approach to solving this problem would be to simply protect the entire resource with a mutex and put a lock/unlock operations around it. So whenever anyone of these types of threads is accessing the resource, they will lock the mutex, perform their respective access, and then release the mutex. This is, however, too restrictive for the readers/writer problem. 

- In the simple case when there are no readers or writers accessing the resource, either an upcoming read operation or write operation can be granted.
- If the read counter is greater than zero (there are some readers already accessing this file), it's fine to allow another reader to access the file. Since readers are not modifying it, so it's okay to grant that request.
- If there is a writer that's accessing the resource, we cannot allow neither read nor write operation.
![process v thread](/assets/img/Posts/GIOS/P2L2/img17.png)

#### Reader/Writer Problem Code
![process v thread](/assets/img/Posts/GIOS/P2L2/img18.png)


### Critial Section Structure
![process v thread](/assets/img/Posts/GIOS/P2L2/img21.png)
If we closely examine each of the highlighted blocks, we will see that they have the following form:
- First we must lock the mutex, then we check on a predicate to determine whether it’s ok to actually perform the access.
- If the predicate is not met we enter the while loop and perform wait(). The wait() is associated with a condition variable and a mutex. When we come out of the wait statement, we must check the predicate in the next while loop again.
- If the predicate is met, we exit the while loop. We could perform some update to the shared state and these updates could potentially impact the condition variable for other threads. So we should need to notify them via signal() or broadcast() with the appropriate condition variable.
- Finally we must unlock the mutex. In Birrell's paper the unlock operation is implicit in the closing brace. In some other threaded environments, we must explicitly do an unlock call. 

### Critical Section Structure with Proxy
![process v thread](/assets/img/Posts/GIOS/P2L2/img22.png)
The actual read/write operations to the shared resource, the reads and writes of the shared file in this case, must be protected by the enter/exit critical sections. Each of these blocks internally follows the critical section structure that we outlined before where they lock the mutex, check for a predicate. 
The mutex is held only within these Enter/Exit Critical Section codes and unlocked at the end. This allows us to control the access through the proxy variable but allowing more than one thread to be in the critical section at a given point in time. This lets us benefit from mutexes controlling shared access.
It also allows us to deal with one-thread-a-time limit when using default mutex. 

### Avoiding Common Mistakes
![process v thread](/assets/img/Posts/GIOS/P2L2/img23.png)
- make sure to keep track of the mutex and condition variables that are specifically used with a given shared resource. 
- make sure that if a variable or a piece of code is protected with a mutex in one portion of your code, that you're always consistently protecting that same variable, or that same type of operation with the same mutex everywhere else in your code 
- Another common mistake is to use different mutexes for a single resource. Some threads read the same file by locking mutex m1, and other threads write to the same file by locking mutex m2. 
- It's also important to make sure that when you're using a signal or a broadcast you're actually signaling the correct condition variable. That's the only way that you can make sure that the correct set of threads will be notified. 
- Make sure that you're not using signal when broadcast is needed. Remember that with a signal only one thread will be woken up to proceed. On the other hand, if signal is needed but you use broadcast, that's fine. You will still end up waking up one thread or more and not affect the correctness of the program. You may just end up affecting its performance, which is not as dangerous.
![process v thread](/assets/img/Posts/GIOS/P2L2/img24.png)

### Spurious Wake up
Let's say currently there is a writer that's performing a write operation and it has locked counter_mutex.
Meanwhile there are readers waiting on a condition variable, read_phase in the wait queue. When the writer issues the broadcast operation, this broadcast can start removing reader threads from the wait queue that's associated with read_phase.
If we don’t unlock the mutex before signal and broadcast, i.e. the writer still holds the mutex, the readers that are woken up on read_phase will fail to acquire the mutex and none of these reader threads will be able to proceed. They'll be woken up from the queue that's associated with the condition variable, and they'll have to be placed on the queue that's associated with the mutex. This is what we call spurious wake-up: we signaled, we woke up the threads but that wake-up was unnecessary.
![process v thread](/assets/img/Posts/GIOS/P2L2/img25.png)
![process v thread](/assets/img/Posts/GIOS/P2L2/img26.png)

### Deadlocks

![process v thread](/assets/img/Posts/GIOS/P2L2/img27.png)
Note: The most accepted solution is to maintain the lock order: when using nested mutexes, be sure to use the mutexes in the same order when the mutexes are used by other threads.

There is more that goes into dealing with deadlocks:
-	Detecting them
-	Avoiding them
-	Recovering from them.

But for the sake of this class, remember that maintaining lock order will give you a deadlock-proof solution.

![process v thread](/assets/img/Posts/GIOS/P2L2/img28.png)

### Kernel vs User level threads
Kernel-level threads imply that the operating system itself is multi-threaded. Kernel-level threads are visible to the kernel and are managed by kernel-level components like the kernel-level scheduler. So it is the OS scheduler that decides how these kernel level threads will be mapped onto the underlying physical CPUs and which of them will be run at any given time. Some of these kernel-level threads may be there to directly support some of the processes so they can execute some user-level threads. Some other kernel level threads may be there just to run certain OS level services like daemons.
![process v thread](/assets/img/Posts/GIOS/P2L2/img29.png)

Types of models:

- The first model is a one-to-one model. Here each user-level thread has a kernel-level thread associated with it. When a process creates a user-level thread, either a kernel-level thread will be created or an existing kernel-level thread will be associated with the user-level thread.

![process v thread](/assets/img/Posts/GIOS/P2L2/img30.png)
- The second model is the many-to-one model. Here all the user-level threads are supported and mapped onto a single kernel-level thread. So at the user-level there is a thread management library that decides which one of the user-level threads will be mapped onto the kernel-level thread at any given point in time. That user-level-thread will run only until the kernel-level thread is scheduled onto a CPU.
![process v thread](/assets/img/Posts/GIOS/P2L2/img31.png)

- The third model is the many-to-many model, which allows some user-level threads to be associated with one kernel-level process, others perhaps to have a one-to-one mapping with a process. So it’s sort of the best of both worlds. The kernel knows that the process is multithreaded since it has assigned multiple kernel-level threads to it and also if one user-level thread blocks on I/O and as a result the kernel-level thread blocks as well, the process overall will have other kernel-level threads onto which the remaining user-level threads will be scheduled. 

![process v thread](/assets/img/Posts/GIOS/P2L2/img32.png)

### Scope of Multithreading
There are different levels at which multithreading is supported, at the entire system or within a process. Each level affects the scope of the thread management system.
- At the kernel level we have system wide thread management that is supported by the OS system level thread managers, meaning that the OS thread managers will look at the entire platform when making decisions as to how to allocate resources to the threads. This is the system scope.
- At user level, a user-level thread library linked to the process manages all the threads within that single process only, so its management scope is process-wide. Different processes will be managed by different instances of the same library or even different processes may be linked to entirely different user level libraries. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img33.png)
- If the user-level threads have process scope, the OS doesn’t see all of them. So at the OS level the available resources will be managed 50%-50% among them, i.e. both webserver and database will be allocated an equal share of the kernel-level threads and the kernel level scheduler will manage these threads by splitting the underlying CPUs among them. The result is that the webserver user-level threads will have half of the amount of CPU cycles allocated to the database threads.
- If we have a system scope, all the user-level threads will be visible at the kernel-level, so the kernel will allocate kernel-level threads to every user-lever threads. Therefore, each one of the user-level threads has an equal portion of the CPU, if that happens to be the policy that the kernel implements. As a result, the process that has more user-level threads, will end up receiving a larger share of the underlying physical resources.

### Multithreading patterns

Boss-workers, Pipeline and Layered structure. 

#### Boss
Popular pattern characterized by one boss thread and some worker threads.
The boss is in charge of assigning tasks to workers. The workers perform the tasks. The boss just accepts orders and immediately passes it on to one of the workers (step 1). Each worker performs (steps 2 – 6). Since there is only one boss thread that manages all the orders, the throughput of the system is limited by the boss’s performance. So, we must keep the boss efficient and limit the amount of work the boss performs.
To summarize the a common the boss-worker model has those features:
-	Boss assigns work to workers.
-	Worker performs entire task.
-	Boss-worker communicate via produce/consumer queue.
-	Worker pool is used to manage the number of worker threads in the queue, statically or dynamically.

The benefit of this model is simplicity. One thread assigns work to all others threads while all others threads simply performs the tasks.
-	One disadvantage is the overhead caused by the worker threads pool management.
-	Another downside is that it ignores locality. The boss doesn’t keep track of what each worker is doing. If we have a situation in which a worker just completed one certain type of task, it’s more likely that the same work would be more efficient at performing a similar task in the future. But if the boss is not paying attention to what the workers are doing, it has no way of making such optimization.

- Boss worker Variant: 
all workers are created equal versus designating workers for specific tasking 
-	Locality: Performing similar task does not require context switch, i.e., it will work with hot cache
-	QoS: Better Quality of Service Management 
-	Load Balancing: It becomes complicated to calculate how many threads to be assigned to a particular task.

### Pipeline Pattern
A different way to assign work to threads in a multithreaded system is using this pipeline approach. In this pipeline approach the overall task is divided into subtasks, and each of the subtasks is performed by a separate thread. Threads are assigned subtasks in the system, and the entire complex task is executed as a pipeline of threads. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img34.png)
-	A pipeline is a sequence of stages where a thread performs a stage in the pipeline, and that's equivalent to some subtask in the end-to-end processing. 
-	To keep the pipeline balanced a stage can be executed by more than one thread. We can use the same thread pool management technique that we described in the Boss-Workers model to determine what is the right number of threads per stage.
-	Passing partial work products or results across the stages in the pipeline should be done via a shared buffer based communication. This provides for some elasticity in the implementation and avoids stalls due to temporary pipeline imbalances.
- A key benefit of the approach is the fact that it allows for highly specialized threads and this leads to improved efficiency. Like what we saw in the variant of the boss-worker model, when threads perform a more specialized task, it's more likely that the state that they require processing is present in the processor cache and such locality can ultimately lead to improved performance.
- A negative of the approach is the fact that it is fairly complex to maintain the pipeline balanced over time. When the workload pattern changes (e.g. more toys arriving) or when the resources of the pipeline change (e.g. a worker slows down or takes a break), we'll have to rebalance the entire pipeline to determine how many workers to assign to each stage. In addition to that, there is more synchronization since there are synchronization points at multiple points in the end-to-end execution. 

### Layered Pattern
A layered model is one in which each layer is assigned a group of related tasks, and the threads that are assigned to a layer can perform any one of the subtasks that correspond to it. End to end, though, a task must pass up and down through all the layers. So, unlike in the pipeline pattern, we must be able to go in both directions across the stages.
![process v thread](/assets/img/Posts/GIOS/P2L2/img35.png)

## P2L4 Thread Design Considerations

### Kernal vs User Level Threads
![thread design](/assets/img/Posts/GIOS/P2L4/img1.png)
-	Supporting threads at the kernel level means that the OS kernel itself is multithreaded. To do this, the OS kernel maintains some abstraction, for our threads data structure to represent threads, and it performs all of the operations like synchronization, scheduling, et cetera, in order to allow these threads to share the physical resources.
-	Supporting threads at the user level means that there is a user-level library that is linked with the application, and this library provides all of the management and runtime support for threads. It will support a data structure that is needed to implement thread abstraction and provide all the scheduling, synchronization and other mechanisms that are needed to make resource management decisions for these threads. In fact, different processes may use entirely different user-level libraries that have different ways to represent threads that support the different scheduling mechanisms. 

### Thread Related Data Structures : Single CPU
Single threaded process:
Let's start by looking at what happens in a single threaded process.
The PCB (Process Control Block) contains
-	the address space (the virtual to physical address mappings)
-	its stack
-	its register
Whenever this process makes a system call, it tracks into the kernel, executes in the context of a kernel thread.

Many to One:
If the process is multithreaded, this will be like the many-to-one model.
There is only one kernel level thread and there is a user level threading library that manages these user threads.
This user-level library will need some way to represent threads so that it can track their resource use and make decisions regarding scheduling and synchronization. So it should have some ULT data structure that contains:
-	User-level thread ID
-	User-level thread registers
-	User-level thread stack

Many to Many:
We start splitting up the PCB: 
The KLT data structure will store:
•	Kernel-level stack
•	Kernel-level register pointer
The “original PCB” now only contains
•	The virtual address mappings
•	Some additional information that's relevant for the entire process (all of the kernel-level threads).
From the perspective of the ULT library, each KLT looks like a CPU. So the threading library looks at the user-level threads and decides which one of the user-level threads will be scheduled onto the underlying kernel-level threads.

### Thread Data Structures : At Scale
![thread design](/assets/img/Posts/GIOS/P2L4/img2.png)
Now, let's say we have multiple such processes. 
-	ULT data structures
-	PCB data structure
-	KLT data structures

Those data structures are related to each other.
-	The threading library keeps track of all of the ULT that represent the single process. There is a relationship between them and the PCB.
-	For each process we need to keep track of which KLTs execute this process.
-	Vice versa, for each KLT we have to make sure we know what address space is in the PCB.

If the system has multiple CPUs, we need a CPU data and maintain a relationship between the KLT and the CPU data structure.
-	What is the CPU that a KLT has affinity to, last strand it was scheduled on
-	For a CPU, a pointer to its current thread or a pointer to the threads that typically run there, and similar information.

When the kernel itself is multithreaded, we said we can have multiple kernel-level threads supporting a single user-level process. When the kernel needs to schedule, or context switch, among kernel-level threads that belong to different processes, it can quickly determine that they point to a different process control block. So they will have different virtual address mappings, and therefore can easily decide that it needs to completely invalidate the existing address mappings and restore new ones.
In the process, it will save the entire PCB of the first kernel-level thread, and then if it's context switching to the second one, it will restore the entire PCB of the second one.

If there are multiple processes, then we need these data structures for each process. There must be relationships kept amongst the different data structures.
-	The user level thread maintains a pointer to the PCB and vice versa.
-	The kernel level thread also keeps a mapping to the PCB and vice versa.
### Hard and Light Process State
When we're context switching among these two kernel level threads
-	there is a portion of this process control block information that we want to preserve, like all of the virtual address mappings.
-	there is portion that's really specific to the particular kernel level thread and it depends on what is the user level thread that's currently executing. It's something that the threading library directly impacts.

The hard process state that's relevant for all of the user level threads that execute within that process.
The light process state that is only relevant for a subset of the user level threads that are currently associated with a particular kernel-level thread.
![thread design](/assets/img/Posts/GIOS/P2L4/img3.png)

### Rationale for Multiple Data structures 
![thread design](/assets/img/Posts/GIOS/P2L4/img4.png)
Single PCB:
-	Scalability is limited due to the size.
-	Overheads are limited because they need to have private copies.
-	Performance is affected because everything has to be saved and restored. 
-	Flexibility is affected by the fact that updates are a little bit more difficult.

Multiple Data Structures:
-	We gain in scalability
-	Overhead is reduced because we don’t have to have separate copies for everyone.
-	We have improvements in performance because context switch time can be reduced.
-	We have more flexibility

### User Level Structures in Solaris 2.0 (SunOS 5.0)
![thread design](/assets/img/Posts/GIOS/P2L4/img5.png)

This is a diagram from figure 1 in the Stein and Shah paper “Implementing Lightweight Threads” and it illustrates the threading model supported in the operating system.
-	From the bottom up, the OS is intended for multiprocessor systems with multiple CPUs.
-	The kernel itself is multithreaded - there are multiple kernel level threads
-	At user level, the processes can be single or multithreaded. Both many-to-many and one-to-one mappings are supported.
-	Each kernel level thread that’s executing a user level thread has a lightweight process (LWP) data structure associated with it.
-	From the user level library’s perspective, these LWP represent the virtual CPUs onto which the ULT will be scheduled.
-	At the kernel level, there will be a kernel-level scheduler that will be managing the kernel level threads and scheduling them onto the physical CPUs.

We will now look a little more closely at the user level thread data structures. They’re described in the “Implementing lightweight threads” paper by Stein and Shah. This does not describe the POSIX threads (pthread), but it’s a similar type of user level threading library.

![thread design](/assets/img/Posts/GIOS/P2L4/img6.png)
When a thread is created, the library returns a thread ID. But this is not a direct pointer to the actual thread data structure. Instead, it’s an index in a table of pointers. It is the table pointer that in turn points to the actual thread data structure. The nice thing about this is that if there is a problem with the thread:
-	With the thread ID being a pointer, that pointer would just point to some corrupt memory, and we can’t really figure out what’s going on.
-	With the thread ID stored in the table entry, we can encode some information in the table entry that can provide some meaningful feedback or an error message.

The thread data structure contains several fields. In particular, the thread local storage contains the variables that are defined in the thread functions which are known at compile time. The compiler can allocate private storage on a per thread basis. Stack size might be defined by library defaults or user provided. 
The size of the ULT data structure is known at compile time therefore the thread data structures can be created such that it is contiguous in memory. This would improve locality and memory access (faster speed).It can make it easy for the scheduler to find the next thread. It just has to basically multiply the thread integers with the size of the data structure.
 It’s possible that as the stack gets growing, one thread will end up overwriting the data structure of another thread. The solution is a red zone. This refers to a portion of the virtual address space that’s not allocated, so if a thread is running and its stack is increasing, if it tries to write to an address that falls into this red zone region, then the operating system will cause a fault.

### Kernel Level Structures in Solaris 2.0
![thread design](/assets/img/Posts/GIOS/P2L4/img7.png)
Each Process data structure will contain information on
-	What are all the KLT that execute within that process address space?
-	What are the mappings that are valid between the virtual and physical memory?
-	What are the user credentials? For instance, if this process is trying to access a file, we have to make sure that that particular user has access to that file.
-	What are the signal handlers that are valid for this process (this information is about how to respond to certain events that can occur in the operating system)

Next the lightweight process (LWP) data structure. This contains information that’s relevant for a sub-subset of the process, such as:
-	User-level registers
-	System call arguments.
-	Resource usage information.
-	Signal mask.
The information in LWP is similar to information in ULT data structure, but LWP is visible to the kernel while ULT is not. So when the OS level schedulers need to make scheduling decisions, they can see this information and act upon it. If we want to find out the aggregate data usage for the entire process, we need to walk through all the lightweight processes that are associated with it. The LWP does not always have to be always present in memory, we only need it when a ULT need to run.

The kernel level thread data structure includes the kernel level information
-	Registers
-	Stack pointers
-	Scheduling info (class. etc)
-	Pointers to data structures associated with this kernel (e.g. LWP, CPU structure, etc)

The kernel level thread is always needed. They’re operating level services that need to access some information even when a thread is not active. 

Next is the CPU data structure. It has information like the
-	Current thread that’s currently scheduled.
-	Lists of the other KLTs that ran there.
-	Some information about how to execute the procedure for dispatching a thread, or how to respond to various interrupts on the referral devices.
Note that if we have information about the CPU data structure, through it we can find that information about all of the different data structures that are needed to rebuild the entire process state.

Below is how the Eykholt paper on multithreading the OS kernel describes the relationship between all of these data structures. 
-	A process data structure has information about the user, points to the address space, and also points to a list of kernel level thread structures.
-	Each of the KLT structures points to the LWP that it corresponds to, its stack, etc.
-	The LWP and stack portion is actually swappable.
-	Not shown in this figure is the CPU data strucutre.
-	Some other pointers are also not shown (like from the thread going back to the process etc). 
![thread design](/assets/img/Posts/GIOS/P2L4/img8.png)


### Basic Thread Management Interaction

Example: Consider we have a multithreaded process which has 4 user-level threads. However, at any given point of time, the actual level of concurrency is just 2. If our operating system has a limit on the number of kernel threads that it can support, it would be nice if the user-level process declares that it only needs two threads. When the process starts, the kernel will first give it a default number of kernel-level threads and the accompanying lightweight threads (e.g. only 1).
Then the process will request additional kernel-level threads, by a system call called set_concurrency(). In response to this system call, the kernel will create additional threads and it will allocate those to this process. 
Now let’s consider this scenario in which the two user-level threads that were mapped on the underlying kernel-level threads block. If the ULTs need to perform some I/O operation, they were moved onto the wait queue associated with the I/O event. As a result, the KLTs associated with the ULTs are blocked as well. Now the whole process is blocked, because it only has two kernel-level threads, both of them are blocked.
The reason why this is happening is because the user-level library doesn't know what is happening in the kernel, it doesn't know that the kernel threads are about to block.
The solution is to:
-	Let the kernel notify the user-level library before it blocks the kernel-level threads.
-	Then the user-level library can look at its run queue.
-	If there are any runnable user-level threads, the user-level library can make a system call to request more KLT or LWP.
-	In response to this system call, the kernel can allocate more kernel-level thread, and the library can start scheduling the remaining user-level threads onto the new KLT/LWP.

At a later time when the I/O operation completes, the kernel will notice that one of the kernel-level threads is pretty much constantly idle.So maybe the kernel can tell the user-level library that you no longer have access to this kernel-level thread, so you can't schedule on it.

This example shows that
-	The user-level library doesn't know what's happening in the kernel.
-	The kernel doesn't know what's happening at the user level either.
-	To correct for these issues the Solaris threading implementation introduced certain system calls and special signals to pass or request certain things among these two layers. 
Solaris allows for system calls and special signals to allow kernel and ULT library to interact and coordinate. 

### Lack of Thread Management Visibility
The kernel and the user level library don’t have visibility into each other’s activities:
-	The kernel sees all the kernel level threads, CPUs, and the kernel level.
-	At the user level, the user-level library sees the ULT that are part of that process and the KLT that are assigned to that process.
![thread design](/assets/img/Posts/GIOS/P2L4/img9.png)

One to One:
If the user level threads and the kernel level threads are using the one-to-one model, then every ULT will have one KLT associated with it. The user-level library will see many KLTs. But it will be the kernel that will manage those KLTs.

The user level library can request that one specific ULT to be bound to one KLT. The term “bound” was introduced in the Solaris paper. Clearly in one-to-one model every user level thread is bound to a kernel level thread. This can be done in many-to-one or many-to-many models as well.
(This “bound thread” is similar to the case where a KLT is permanently associated with a CPU in a multi-CPU system, which is call thread pinning). 

![thread design](/assets/img/Posts/GIOS/P2L4/img10.png)

The user level library is part of the user process’s address space. The program counter needs to jump to the user level library scheduler when:
-	The user level thread may explicitly yield
-	The timer set by user level library expires.
-	The user level library scheduler performs synchronization operation. For example. when we call a lock, that thread may not be able to run if it needs to be blocked; when we call an unlock operation, then we need to evaluate what is the new runnable thread that the scheduler should allocate on the CPU. In general, whenever we have a situation where a blocking ULT becomes runnable, we jump into the scheduler code.
-	The user level library scheduler is also triggered in response to certain events/signals that come either from timers or directly from the kernel.

![thread design](/assets/img/Posts/GIOS/P2L4/img11.png)

### Issues on Multiple CPUS

In a multi-CPU system, the kernel level threads that support a single process may be running on multiple CPUs, even concurrently. So we may have a situation when the user-level library that's operating in the context of one thread on one CPU needs to somehow impact what is running on another thread on another CPU.

![thread design](/assets/img/Posts/GIOS/P2L4/img12.png)
For example, we have three threads, T3, T2, and T1. The thread priority is that T3>T2>T1.
-	ULT2 is currently running in the context of one KLT on one CPU and currently holds a mutex.
-	ULT3 with the highest priority is waiting on the mutex and is blocked.
-	ULT1 is running on another KLT the other CPU.

At a later point, ULT2 releases the mutex (but is still running on the CPU).

-	As a result ULT3 becomes runnable and we have to make sure the ones with the highest priority are the ones that get executed.
-	We will involve the user level thread library so that it can preempt ULT1 and schedule ULT3 on that CPU.
-	We need to context switch ULT1. Because ULT1 is running on a different CPU, so we need to notify the other CPU to update its registers and program counter.
-	We cannot directly modify the register of one CPU when executing on another CPU.
-	This means that we need to send some signal/interrupt from the context of KLT on CPU1 to KLT on CPU2.
-	Once that signal happens, the user-level library on CPU2 will determine that it needs to schedule the highest priority ULT3 and block ULT1.

So when we have multi-CPU, multi-kernel and multi-user-level threads, interaction between the management of user level and kernel level becomes more complicated than the situation when there is only one CPU. 

### Synchronization Issues

![thread design](/assets/img/Posts/GIOS/P2L4/img13.png)
The owner of the mutex is running on one CPU1. When we request the same mutex from CPU2, it is possible that by the time we take ULT4, context switch it and place it on the queue that's associated with this mutex, ULT1 critical section might have already completed its execution. In such cases, we are better off just spinning on this CPU: just burning a few cycles, waiting a little bit until ULT1 actually releases the mutex.

-	So for super short critical sections, we do spin instead of block.

-	For long critical sections we will have the default behavior where a thread is actually properly blocked will be placed on a queue that's associated with a mutex until the mutex is freed.

We call such mutexes, which sometimes result in the thread spinning and other times blocking, adaptive mutexes.
![thread design](/assets/img/Posts/GIOS/P2L4/img14.png)
Once a thread exits, it should be destroyed and its data structure, stack, etc., should be freed.
However, since thread creation takes some time (data structures need to be created and initialized), it makes sense to reuse these data structures, essentially as if we're reusing the threads.
The way this is done is when a thread exits it's not immediately destroyed, the data structures are not immediately freed. Instead the thread is marked as it's on a death row.
Periodically a special reaper thread will perform garbage collection which means that it will actually go ahead and free up all of the data structures that are associated with the threads on the death row.
If a request for a thread comes in before the thread has been properly destroyed from the death row then its data structure and stack can be reused.
This will lead to performance gains since we don't have to wait for all the allocations.

### Interrupts vs Signals
![thread design](/assets/img/Posts/GIOS/P2L4/img15.png)
Interrupts are events that are generated externally to the CPU by components that are other than the CPU. Interrupts represent notifications sent to the CPU that some external event has occurred. This can be from an I/O device like a network device delivering an interrupt that a network packet has arrived, or from timers notifying the CPU that a timeout has occurred, or from other CPUs. 
Signals are events that are triggered by the software that's running on the CPU. They're either generated by software, or the CPU hardware itself triggers certain events that are basically interpreted as signals. What type of signals can occur on a given platform depends on the operating system. 

There are some aspects of interrupts and signals that are similar.

-	Both interrupts and signals have a unique identifier. And its value depends on the hardware in the case of interrupts, or on the operating system in the case of signals.

-	Both interrupts and signals can be masked to disable/suspend the notification that the signal/interrupt is delivering.  The interrupt mask is associated with a CPU because interrupts are delivered to the CPU as a whole. Whereas the signal mask is associated with a process because signals are delivered to individual processes.

-	Interrupt handlers are specified for the entire system by the operating system, but signal handlers can be specified per process.

### Visual Metaphor
![thread design](/assets/img/Posts/GIOS/P2L4/img16.png)
An interrupt is generated by an event that's external to the CPU and so is the snow storm external to  the toy shop.

The signal is generated from within the CPU, so is battery is directly caused by the toy shop worker fixing a toy.

-	First, each of these types of warnings need to be handled in specific ways.
-	Second, both of them can be ignored.
-	And last, we can think about both of them as being expected or unexpected.
 

In a toy shop, handling these types of events may be specified via safety protocols or certain hazard plans. This is not uncommon. There may be, however, situations in which it is appropriate to just continue working. And finally, situations like the fact that the battery died are frequent. They happen regularly, so they're expected. Whether or not it is expected for a snowstorm to occur, that will really depend on where the toy shop is located.

If we think about interrupts or signals, both of them are handled in a specific way and that is defined by the signal handler. Next, both interrupts and signals can be masked, as we said. And in that way, we can ignore them. And finally, as we previously discussed, these types of events can appear synchronously or asynchronously.

### Interrupt Handling
![thread design](/assets/img/Posts/GIOS/P2L4/img17.png)
Device interrupts the CPU by sending a signal through the interconnect that connects the device in the CPU complex. In the past we used dedicated wires. Most modern devices use MSI (Message Signal Interrupter) that can be carried on the same interconnect (e.g. PCI-E).

Based on the pins where it occurs or the MSI message, the interrupt can be uniquely identified.

Then based on the interrupt number (INT-#), the hardware defined message is then looked up in a table and the corresponding handler is called. The program counter is moved to that address of the handler code and the handler routine is then executed.

What types of interrupts can occur depends on the hardware while how it is handled is specified by the operating system.

### Signal Handling
Basically the same as interrupt discussion above except signal is not generated by external entity.
The OS defines the possible signals, e.g. SIGNAL-11, instead of the hardware.

There are default actions defined by the OS in response to signals, but each process can have their own defined signal handlers which respond in a user defined way in response to the OS signal.
![thread design](/assets/img/Posts/GIOS/P2L4/img18.png)
![thread design](/assets/img/Posts/GIOS/P2L4/img19.png)

### Why Disable Interrupts or Signals
One problem with interrupts/signals
- executed in the context of thread that was interrupted
- handled on thread stack which leads to discussion on why it should sometimes be disabled.
Example:
In the example, we have a thread and its PC and stack are shown in the right.
At some point of the execution, an interrupt occurs.
As a result, the program counter will change and it will start to point to the first instruction in the handler.
The stack pointer will remain the same. This could be nested if there are multiple singles/interrupts. In a nested fashion, they will keep executing on the stack of the thread which was interrupted.
The handling routine may need to access some state that some other thread is accessing, so we need to use mutex. However, if the thread that was interrupted already has the mutex that was needed by the handler routine, we have a deadlock situation.  The interrupted thread will not release its mutex until the handler routine completes its execution on its stack. The handler routine is blocked by the mutex.

To prevent this situation, one solution is to keep the handler code simple which means we can prohibit the handler code from using the mutex. If there is no possibility for the handler to be blocked on some mutex operation, then the deadlock situations will not occur. The problem with the method is too restrictive.
We could also introduce masks which will dynamically allow us to enable or disable whether the handling code will interrupt the mutex. The mask is a sequence of bits where each bit corresponds to a specific interrupt or signal. The value 0/1 indicates whether this specific interrupt or signal is disabled or enabled. When an event occurs, first the mask is checked. If the event is enabled, then we proceed with the handler procedure. If the event is disabled, then the interrupt/signal will remain suspended until a later time when the mask value changes. 

To solve the deadlock situation
-	The thread disables the interrupt.
-	The thread acquires the mutex. 
-	The thread executes the critical section.
-	The thread releases the mutex once it completes.
-	The thread re-enables the interrupt. The operation will allow the execution of the pending handler code.
The deadlock is avoided.

While an interrupt or signal is pending, other instances may occur. They will remain pending as well.
Once the event is re-enabled, the handler routine will typically only execute once. If we want to ensure the handler routine is executed more than once, it is not just sufficient to generate the signal more than once. 

### More on Signal Masks

Interrupt masks are maintained on a per CPU basis. What this means is that if the interrupt mask disables a particular interrupt, the hardware support for routing interrupt will just not deliver that interrupt to the CPU.

The signal mask depends on what the user level process. So the signal masks are per execution context. If a signal mask is disabled, the kernel sees that, and in that case, it will not interrupt the corresponding thread, i.e., the execution context.

### Interrupts on Multicore Systems
On the multi-CPU systems, the interrupt routing logic will direct the interrupt to any one of the CPUs that has that interrupt enabled. 

The reason that we put this crown here is, in multi-CPU systems we can only allow one of the CPUs to handle interrupts. It will be the only CPU that has interrupts enabled.

We will be able to avoid any of the overheads or perturbations related to interrupt handling from any of the other cores. The result will be improved performance. 

Instead of all CPUs receiving an interrupt, let only one CPU receive it. This reduces total cost.

### Types of Signals
Generally, there are two types of signals.
-	One-shot signal (overwrite behavior) One property of these signals is that we know that if there are multiple instances of the same signal, they will be handled at least once. So we have a situation, where if only one kind of that signal occurred, versus n signals of that same kind occurred, that only one execution of the actual signal is performed. 
-	Real Time Signals (queue behavior). real time signals that are supported in an operating system like Linux. And their behavior is that if a signal is called n times, then the handler is guaranteed to be called n times as well. 

### Interrupt Handler as Threads
The thread scheduler can schedule the original thread back on the CPU and that will continue executing.
Eventually the original thread will unlock the mutex, and the interrupt handler thread will be executed.
The way it happens is as follows.
-	Whenever a signal/interrupt occurs, it interrupts the execution of the thread.
-	By default the handling routine will start executing in the context of the interrupted thread using its stack, etc.
-	If the handling routine is going to perform synchronizing operation, that handler routine will execute on a separate thread. When the locking operation is reached, it will be placed in a wait queue associated with the mutex. The original thread will be scheduled.
-	When the unlock operation happens, we will go back and dequeue the handler routine from the mutex queue.

It's a dyanmic decision
- if the handler doesn't lock -> execute on interrupted threads stack
- if handler can block -> turn into real thread
it's expensive! so the optimization is to pre-creat and pre-initialize thread sturctures for interrupt routines

### Interrupts Top vs Bottom Half
When the interrupt first occurs, we are in the initial/top part of this interrupt handler. It may be necessary to disable certain interrupts to prevent deadlock situations.
When the interrupt is passed to a separate thread (bottom part), then we can re-enable those interrupts, because now the handler is in a separate and interrupt can be handled as any other thread in the system. There will not be any deadlock potential anymore.

-	The top will handle a minimum amount of processing. It is required to be non-blocking. The top half executes immediately when the interrupt occurs.
	
-	The bottom is allowed to perform any arbitrary/complex processing. The bottom half is like any other thread can be scheduled for a later time and can be blocked.

### Performance of Threads as Interrupts
Although creating a thread per interrupt is expensive, interrupts are actually quite rare, while critical sections are quite common.
With separate interrupt handling threads, the Top half avoids expensive calls to mask and unmask interrupts or change its priority before and after each critical section.
If every 10 seconds you have 100 critical sections and 1 interrupt, you save 12 * 100 instructions per mutex and pay the cost of 40 instructions for creating the 1 interrupt thread.
This gives you an overall gain of 1200 - 40 = 1160 instructions.


## Thread Performance Considerations - P2L5
![thread improvements](/assets/img/Posts/GIOS/P2L5/img1.png)

execution time = customer wait time, a.k.a.how long the average line is.

The image shows 6 worker threads but the calculations are done for only 5 worker threads. Also, In Boss-Worker model ((5*120) + (5*240) * 360)/11
should be ((5*120) + (5*240) + 360)/11

If we take a look at the Pipeline model, the first order took 120 ms to complete. 6 pipeline stages times 20 ms. The next one was already in the pipeline and once the first order completed, it had to finish the last stage of the pipeline, so its completion time will be 20 ms longer, so 140 ms. The one that came after that, another 20 ms longer for 160 ms, and so on until the very last order which will take 320 ms. So the average completion time for the Pipeline model is 220 ms.

### Are Threads Useful?
We know - They allow us to gain speed up because we can parallelize problems, they allow us to benefit from a hot cache because we can specialize what a particular thread is doing on a given CPU, and they lead to implementations that have lower memory requirements (efficiency) and where its cheaper to synchronize compared to multi-process implementations of the same problem. We said that threads are useful even on a single CPU because they let us hide the latency of I/O operations. 
However we might care about other things and rely on metrics to establish what's important for our software. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img2.png)

### Visual Metaphor
One example is throughput. The toy shop manager would want to make sure that this is as high as possible. Other things that may be important for the toy shop manager include how long does it take to react to a new order on average? Or what is the percentage of the workbenches that are used over a period of time? 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img3.png)

### Metrics!
![thread improvements](/assets/img/Posts/GIOS/P2L5/img4.png)

### Metrics Again!
![thread improvements](/assets/img/Posts/GIOS/P2L5/img5.png)
Highlighted metrics:

- wait time
- throughput
- platform efficiency
- performance per dollar
- performance per watt

You may have hear of the term SLA (Service Level Agreement), enterprise applications will give typically SLAs to their customers. One example, for instance, will be that you will get a response within 3 seconds. 

### Performance Metrics Summary
a metric is some measurable quantity that we can use to reason about the behavior of the system. Ideally, we will obtain these metrics, we will gather these measurements, running experiments using real software deployments on the real machines using real workloads. However, sometimes that really not an option, we cannot wait to actually deploy the software before we start measuring something about it or analyzing its behavior. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img6.png)
We refer to these experimental settings as a testbed so the testbed that tells us where the experiments were carried out and what were the relevant metrics that were measured.

### Are Threads Useful?

Depends on metrics and workload, however that is not an acceptable answer for the course. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img7.png)

### Multi Process versus Multi Threaded 
Comparing the two models using a web server as an example.
Simple steps in a web server for example:
![thread improvements](/assets/img/Posts/GIOS/P2L5/img8.png)

### Multi Process Web Server
This (Process 1 below) then clearly represents a single threaded process. One easy way to achieve concurrency is to have multiple instances of the same process and that way we have a multi-process (MP) implementation. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img9.png)

### Multi Threaded Web Server
An alternative to the multi-process model is to develop the web server as a multi-threaded (MT) application. So here (pointing at looping lines on the left side of the illustration below) we have multiple execution contexts, multiple threads within the same address space, and every single one of them is processing a request
![thread improvements](/assets/img/Posts/GIOS/P2L5/img10.png)

### Event Driven Model
The model we’ll talk about is called event-driven model. An event-driven application can be characterized as follows. The application is implemented in a single address space, there is basically only a single process and a single thread of control
![thread improvements](/assets/img/Posts/GIOS/P2L5/img11.png)
The main part of the process is an event dispatcher that continuously, in a loop, looks for incoming events and then based on those events invokes one or more of the registered [event] handlers. Here events correspond to some of the following things. Receipt of a request from the client browsers, the message received from the network. Completion of send, so once the server responds to the client request, the fact that the send completed, that’s another event as far as the system is concerned. Completion of a disk read operation, that’s another event that the system will need to know how to handle.

![thread improvements](/assets/img/Posts/GIOS/P2L5/img12.png)


### Concurrent Execution in Event-Driven Model
The way the event-driven model achieves concurrency is by interleaving the processing of multiple requests within the same execution context. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img13.png)

although we have only one execution context, only one thread, if we take a look, we have concurrent execution of multiple client requests. It just happens to be interleaved, given that there’s one execution context, however, there are multiple, at the same time multiple client requests being handled.

### Event-Driven Model Why

![thread improvements](/assets/img/Posts/GIOS/P2L5/img14.png)
threads can be useful because they help hide latency. The main take-away from that discussion was that, if a thread is going to wait more than twice the amount of time it takes to perform a context switch (t_idle > 2 * t_ctx_switch), then it makes sense to go ahead and context switch to another. So in the event-driven model, a request will be processed in the context of a single thread as long as it doesn’t have to wait.
![thread improvements](/assets/img/Posts/GIOS/P2L5/img15.png)

### Event-Driven Model Problems
![thread improvements](/assets/img/Posts/GIOS/P2L5/img16.png)
One way to circumvent this problem is to use asynchronous I/O operations. Asynchronous calls have the property that when the system call is made, the kernel captures enough information about the caller and where and how the data should be returned once it becomes available. Async calls also provide the caller with an opportunity to proceed executing something and then come back at a later time to check if the results of the asynchronous operation are already available. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img17.png)
What if Async Calls are not available?
![thread improvements](/assets/img/Posts/GIOS/P2L5/img18.png)
To deal with this problem, Vivek Pai’s paper proposed the use of helpers. When a handler needs to issue an I/O operation that can block, it passes it to the helper and returns to the event dispatcher. The helper will be the one that will handle the blocking I/O operation and interact with the dispatcher as necessary. 
At the time of the writing of the paper, another limitation was that not all kernels where multi-threaded, so basically not all kernels supported the one-to-one model that we talked about. In order to deal with this limitation, the decision in the paper was to make these helper entities processes. Therefore, they call this model AMPED (Asymmetric Multi-Process Event-Driven Model), it’s an event-driven model, it has multiple processes and these processes are asymmetric, the helper ones only deal with blocking I/O operations, and then the main one performs everything else. In principle, this same kind of idea could have applied in a multi-threaded scenario where the helpers are threads, not processes, so AMTED (Asymmetric Multi-Threaded Event-Driven Model), and in fact there is a follow-on on the Flash work that actually does this exact thing, the AMTED model.
The key benefits of the asymmetric model that we described, is that it resolves some of the limitations of the pure event-driven model in terms of what is required from the operating system, the dependence on asynchronous I/O calls and threading support. In addition, this model lets us achieve concurrency with a smaller memory footprint than either the MP or MTing model. In the MP/MT model a worker has to perform everything for a full request, so its memory requirements will be much more significant than the memory requirements of a helper entity. In addition, with the AMPED model we will have a helper entity only for the number of concurrent blocking I/O operations whereas in the MT or MP models we will have as many concurrent entities, as many processes or as many threads, as there are actual concurrent requests regardless of whether they block or not.

The downside is that although this works well with the server type applications, it is not necessarily as generally applicable to arbitrary applications. In addition, there are also some complexities with the routing of events in multi-CPU systems.

### Flash Web Server
Flash is an event-driven web server that follows the AMPED model, so basically it has asymmetric helper processes to deal with the blocking I/O operations
![thread improvements](/assets/img/Posts/GIOS/P2L5/img19.png)


Now we will outline some additional detail regarding some of the optimization that Flash applies, and this help us later understand some of the performance comparisons. The important thing is that these optimizations are really relevant to any web server. First of all, Flash performs application-level caching at multiple levels, and it does this on both data and computation. What we mean by this is, it common to cache files, this is what we call data caching
![thread improvements](/assets/img/Posts/GIOS/P2L5/img20.png)
Also Flash does some optimizations that take advantage of the networking hardware, and of the network interface card. For instance, all of the data structures are aligned so that it’s easy to perform DMA operations without copying data. Similarly, they use DMA operations that have scatter-gather support, and that really means that the header and the actual data don’t have to be aligned, one next to the other in memory, they can be sent from different memory locations, so there is a copy that’s avoided. 

### Apache Web Server
![thread improvements](/assets/img/Posts/GIOS/P2L5/img21.png)
From a very high-level the software architecture of Apache looks like this (box above). The core component provides the basic server-like capabilities, so this is accepting connections and managing concurrency. The various modules correspond to different types of functionalities that is executed on each request. Specific Apache deployment can be configured to include different types of modules. For instance, it can have certain security features, some management of dynamic content, or even some of the modules are really responsible for more basic http request processing.

The flow of control is sort of similar to the event-driven model that we saw in the sense that each request passes through all of the modules. Like in the event-driven model each request ultimately passes through all the handlers. However, Apache is a combination of a MP and a MT model. In Apache, a single process, a single instance, is internally a MTed Boss-Workers process that has dynamic management of the number of threads. 

### Experimental Methodology
![thread improvements](/assets/img/Posts/GIOS/P2L5/img22.png)
For the flash paper - what were they comparing?
![thread improvements](/assets/img/Posts/GIOS/P2L5/img23.png)
What were the workloads used?
![thread improvements](/assets/img/Posts/GIOS/P2L5/img24.png)
How were metrics used? 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img25.png)

### Experimental Results

![thread improvements](/assets/img/Posts/GIOS/P2L5/img26.png)

Now since real clients don’t behave like this synthetic workload, we need to look at what happens with some of the realistic traces, the Owlnet and the CS trace. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img27.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img28.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img29.png)

### Summary of Performance Results

To summarize, the performance results for Flash show the following. When the data is in cache, the basic SPED model performs much better than the AMPED Flash because it doesn’t require the test for memory presence which was necessary in the AMPED Flash. Both SPED and the AMPED Flash are better than the MTed or MP models because they don’t incur any of the synchronization or context switching overheads that are necessary with these (MT/MP) models.

When the workload is disk-bound, however, AMPED performs much better than the single process event-driven (SPED) model because the single process model blocks since there’s no support for asynchronous I/O. AMPED Flash performs better than both the MTed and the MP model because it has much more memory efficient implementation and it doesn’t require the same level of context switching as in these (MT/MP) models. Again, only the number of concurrent I/O bound requests result in concurrent processes or concurrent threads in this model.

The model is not necessarily suitable for every single type of server process, there are certain challenges with event-driven architecture, we said some of these can come from the fact that we need to take advantage of multiple cores and we need to be able to route events to the appropriate core, in other cases perhaps the processing itself is not as suitable for this type of architecture. But if you look at some of the high performance server implementations that are in use today you will see that a lot of them do in fact use an event-driven model combined with asynchronous I/O support.
![thread improvements](/assets/img/Posts/GIOS/P2L5/img30.png)

### Advice on Designing Experiments

There is actually a lot of thought and planning that should go into designing relevant experiments.
![thread improvements](/assets/img/Posts/GIOS/P2L5/img31.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img32.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img33.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img34.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img35.png)

### Advice on Running Experiments 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img36.png)

## Scheduling - P3L1

### Lesson Preview
-	First we will look at how the OS manages CPU and how it decides how processes and their threads will get to execute on those CPUs. *This is done by the scheduler.*
-	We will review some of the basic scheduling mechanisms,  some of the scheduling algorithms and data structures that they use.
-	We will look in more detail at some  of the scheduling algorithms used in the Linux operating systems: the completely fair scheduler and the O(1) scheduler. 
-	We will also look at certain aspects of scheduling that are common for multi-CPU platforms. This includes multi core platforms and platforms with hardware level multi-threading. For this we will use the paper “Chip multi-threaded processors need a new operating system scheduler” to demonstrate some advanced features which modern schedulers should incorporate. 

### Visual Metaphor 
![scheduling](/assets/img/Posts/GIOS/P3L1/img1.png)

-	For an OS scheduler we can choose a simple approach to schedule tasks in a first come, first serve manner (FIFO). The benefit is that the scheduling is simple and that we don’t spend a lot of overhead in the OS scheduler itself.  
-	Using a similar analogy to what the toy shop manager was thinking of when he chose to dispatch the simple orders first, an OS scheduler can assign and dispatch simple tasks first. The outcome of this kind of scheduling can be that the throughput of the system overall is maximized and there are schedulers that actually follow this kind of algorithm and those are called Shortest Jobs First (SJF).  Simple in terms of the task is equal to running time so shortest job first. 
-	Finally the scheduling logic behind assigning complex tasks first is similar to the case in the toy shop.  Here the scheduler’s goals are to maximize all aspects of the platform so to utilize well both the CPU’s as well as any other devices memory or other resources that are present on that platform.

### Scheduling Overview
*the CPU scheduler decides how and when processes access the shared CPU in the system*
-	Threads may become ready so they may enter the ready queue after an I/O operation they have been waiting on has completed
-	or after they have been woken up from a wake interrupt. 
-	A thread will enter the ready queue when it’s created.
-	A thread that was interrupted while executing on the CPU will immediately enter the ready queue.
So to schedule something the scheduler will have to look at all of the tasks in the ready queue and decide which is the one that is able dispatch to run on the CPU.
![scheduling](/assets/img/Posts/GIOS/P3L1/img2.png)

Whenever the CPU becomes idle, we have to run the scheduler. For instance, if a thread that was executing on the CPU makes an I/O request and now it has to wait in the I/O queue for that particular device, the CPU is idle. At this moment we run the OS scheduler. The goal is to pick another task to run on the CPU as soon as possible and not to keep the CPU idle for too long.  
A common way in which scheduler shares the CPU is to give each of the tasks in the system some amount of time in the CPU.  When a time slice expires, we also need to run the scheduler so as to pick the next task to be scheduled on the CPU.  
*The objective of the OS scheduler is to choose the next task to run from the queue of ready tasks in the system.*
- What should be selected (depends) and how does the scheduler accomplish this? (Run Queue)

### Run to Completion Scheduling Algorithm 

![scheduling](/assets/img/Posts/GIOS/P3L1/img3.png)

The first and simplest algorithm is the first come first serve. In this algorithm tasks are scheduled on the CPU in the same order in which they arrive. 

![scheduling](/assets/img/Posts/GIOS/P3L1/img4.png)

The first come first serve is simple but the wait time for the tasks is poor even if there is just one long task in the system that has arrived ahead of some shorter tasks.  To deal with this we can look at another: *Shortest Job First.* 

![scheduling](/assets/img/Posts/GIOS/P3L1/img5.png)

### Preemptive Scheduling: SJF + Preempt

![scheduling](/assets/img/Posts/GIOS/P3L1/img6.png)
We need to use some kind of heuristics to estimate the execution time of a task. In a sense, history is a good predictor of what will happen so we will use the past execution time to predict future execution time. We call this scenario in which we compute the averages over a period of the past a windowed average.

### Preemptive Scheduling: Priority

SJF considers the execution time of tasks to decide how to schedule them and when to preempt a particular task. Another criterion for driving those decisions may be that tasks priorities. 
![scheduling](/assets/img/Posts/GIOS/P3L1/img7.png)
In this example we compared the priority by looking at the tables. In principle the scheduler will somehow need to quickly recognize the tasks that are ready to execute but also their priorities.
![scheduling](/assets/img/Posts/GIOS/P3L1/img8.png)
-	We can achieve this by having multiple run queue structures.  Different run queue structures for each priority level.  The scheduler selects a task from the run queue that corresponds to the highest priority level.
-	Another option is to have some kind of ordered data structure like the tree in SJF. However in this case with priority scheduling this tree would need to be ordered based on the priority of the task.
![scheduling](/assets/img/Posts/GIOS/P3L1/img9.png)
One danger with priority based scheduling is starvation.
We can have a situation in which a low priority task is basically infinitely stuck in a run queue just because there are constantly higher priority tasks that show up in some of the other parts of the run queue.
One mechanism that we use to protect against starvation is so called priority aging. What that means is that the priority of the task isn’t just a fixed priority. Instead it is some kind of function of the actual priority plus one other factor and that is the time that the thread or task spent in the run queue.
The idea is that the longer a task spends in a run queue, the higher the priority should become. So eventually, the task will become the highest priority task in the system and it will run. In this manner starvation will be prevented.

### Priority Inversion
![scheduling](/assets/img/Posts/GIOS/P3L1/img10.png)
So based on the priority in the system, we were expecting the order of completion be T1 , T2, T3. However the actual order of execution was: T2, T3, T1. The priorities of these task were inverted. This is what we call priority inversion. 
A solution to this problem would have been to temporarily boost the priority of the mutex owner. What that means, is that at this particular point when the highest priority thread needs to acquire the lock that’s owned by a lower priority thread, the priority of T3 is temporarily boosted to the same level as T1. Then T1 will still be blocked, however instead of scheduling T2, we will schedule T3 because its priority has been boosted. So we wouldn’t have to wait until T2 completes before T1 can run again. This technique of boosting the priority of the mutex owner also demonstrates why it’s useful to keep track of the current mutex owner.


### Round Robin Scheduling

When it comes to running tasks that have the same priority, we have other options aside from the first come first serve or shortest job first.  A popular option is so called Round Robin scheduling.
![scheduling](/assets/img/Posts/GIOS/P3L1/img11.png)
If T1 had not been waiting on I/O, then the execution will be based on the order in which T1, T2 and T3 were placed on the queue.  Each of the tasks execute one by one in a round robin manner and the queue is traversed in a round robin manner one by one. 
![scheduling](/assets/img/Posts/GIOS/P3L1/img12.png)

 We can further generalize Round Robin scheduling to include priority.
-	In this case P1 < P2 < P3, when higher priority task arrives, the lower priority task will be preempted.
-	If P1 < P2 = P3, the scheduler will run the round robin until the task is finished. 
![scheduling](/assets/img/Posts/GIOS/P3L1/img13.png)

Further modification made for Round Robin scheduling is not to wait for tasks to yield explicitly, but to interrupt them and mix all the tasks in the system at the same time. This mechanism is called time-slicing.  
-	For example we can give each of the task a time slice of 1 time unit. 
-	After 1 time unit, we will interrupt them, preempt them, and schedule the next task in the queue in round robin manner.
![scheduling](/assets/img/Posts/GIOS/P3L1/img14.png)

### Timesharing and Time slices

-	A time slice is the maximum amount of uninterrupted time that can be given to a task.  It is also referred to as a time quantum.
![scheduling](/assets/img/Posts/GIOS/P3L1/img15.png)

Consider the simple first come first serve (FCFS) and shortest job first (SJF) schedulers. They both have a same mix of tasks with same arrival times but results in different metrics.
The metrics for FCFS also applied to a Round Robin (RR) scheduler without time slices. If these tasks don’t perform any I/O, they just execute for some fixed amount of time, RR would have scheduled them one after another the way they showed up in the queue as FCFS).
![scheduling](/assets/img/Posts/GIOS/P3L1/img16.png)

For RR scheduler with time slices, let’s first look at the time slice = 1. The execution of these task will look as follows.
![scheduling](/assets/img/Posts/GIOS/P3L1/img17.png)

So far we have assumed that these tasks would immediately start their execution after the previous one was interrupted. However, there’s some real overhead. 
![scheduling](/assets/img/Posts/GIOS/P3L1/img18.png)
As long as the time slice values are significantly larger than the context switching time, we should be able to minimize these overheads. 

### How Long Should a Timeslice Be?
We saw previously that the use of time slice delivers certain benefits. We’re able to start the execution of tasks sooner and therefore we are able to achieve an overall schedule of the task that’s more responsive.  But that came with certain overheads. The balance between can be adjusted by the length of the time slice.
We will see that to answer this question, how long should a time slice be?

### CPU Bound Timeslice length
![scheduling](/assets/img/Posts/GIOS/P3L1/img19.png)
(ERRATA: average comp time for 1s is 21.35s; average wait time for 5s is 2.55s)

-	Timeslice = 1 second
-	throughput = 2 / (10 + 10 + 19*0.1) = 0.091 tasks/second
-	avg. wait time = (0 + (1+0.1)) / 2 = 0.55 seconds
-	avg. comp. time = ((19 + 18 * 0.1) + (20 + 19 * 0.1))  / 2 = 21.35 seconds
-	Timeslice = 5 seconds
-	throughput = 2 / (10 + 10 + 3*0.1) = 0.098 tasks/second
-	avg. wait time = (0 + (5+0.1)) / 2 = 2.55 seconds
-	avg. comp. time = 17.75 seconds

Looking at these metrics
-	For throughput we are better off choosing a higher time slice value.
-	For completion time, we are better using higher time slice values.
-	For avg wait, we are better with a lower time slice value.
*For CPU bound tasks, we’re better off choosing a larger time slice.* This is the winning combination.

### I/O Bound Timeslice length
![scheduling](/assets/img/Posts/GIOS/P3L1/img20.png)
![scheduling](/assets/img/Posts/GIOS/P3L1/img21.png)
Full Calculation
-	Timeslice = 5 second
-	throughput = 2 / 24.3 = 0.082 tasks/second
-	avg. wait time = 5.1 / 2 = 2.55 seconds
-	avg. comp. time = (11.2 + (11.2 + 0.1 + 1 * 9 + 0.5 * 8)) / 2 = 17.75 seconds

### Summarizing Timeslice
![scheduling](/assets/img/Posts/GIOS/P3L1/img22.png)

### Run Queue Data Structure
![scheduling](/assets/img/Posts/GIOS/P3L1/img23.png)
We said earlier that the run queue is only logically a queue. It could be represented by multiple queues with different priorities or it could be a tree or some other type of data structure. Regardless of the data structure, it should be easy for the scheduler to find the next thread to run given the scheduling criteria.
![scheduling](/assets/img/Posts/GIOS/P3L1/img24.png)

A multi-queue data structure has multiple separate queues.
-	In the first run queue we’ll place the most I/O intensive tasks, and time slice = 8 ms.
-	For tasks that are medium I/O intensive, we use a second queue and in this queue time slice = 16ms.
-	For all CPU intensive tasks we’ll use a third queue where time slice = infinite.
It will be like first come first serve policy (FCFS).  From the scheduler’s perspective, the I/O intensive tasks have the highest priorities which means the scheduler will always check the first queue first. The CPU bound tasks are assigned lowest priorities so the third queue will be the last one checked.
So depending on the type of tasks,  we place them into appropriate queues with different time slice values and the scheduler could select which task to run next based on highest priority, medium, and then lowest. In this way, we provide both the time slicing benefits for I/O bound tasks and avoid the time slicing overhead for the CPU bound tasks.  

![scheduling](/assets/img/Posts/GIOS/P3L1/img25.png)
![scheduling](/assets/img/Posts/GIOS/P3L1/img26.png)
This resulting data structure is called the multi-level feedback queue. For the design of this data structure along with other work on time sharing system, Fernando Corbato received the Turing Award which is the highest award for Computer Science. We shouldn’t trivialize the contribution of this data structure and say that it’s equivalent to priority queues.
![scheduling](/assets/img/Posts/GIOS/P3L1/img27.png)
-	First of all there are different scheduling policies that are associated with each of these different levels that are part of this data structure. 
-	More uniquely however, this data structure incorporates this feedback mechanism that allows us to adjust which one of these levels we place a task onto and to find out what is the best time sharing schedule for the subtasks in the system.

### Linux O(1) Scheduler 
![scheduling](/assets/img/Posts/GIOS/P3L1/img28.png)

-	Longer sleep time indicates that the task is interactive: it spends more time waiting on user input or similar event. When longer sleeps are detected, we need to increase the priority of the task. We do this by actually decrease its priority level value by 5 so next time this interactive task will execute with higher priority.
-	Smaller sleep times indicate that the task is CPU intensive, so we want to lower its priority by increasing its priority level by 5 (up to maximum). So next time the task will execute in a lower priority class.  

![scheduling](/assets/img/Posts/GIOS/P3L1/img29.png)
The O(1) scheduler was replaced with the Completely Fair Scheduler. The CFS became the default scheduler in kernel 2.6.23.  Ironically, both of these schedulers are developed by the same person. Both the O(1) and CFS scheduler are part of the standard Linux distribution. CFS is the default.  But you can switch to O(1) scheduler to execute your tasks. 
![scheduling](/assets/img/Posts/GIOS/P3L1/img30.png)

### Linux CFS Scheduler 
As we said, one problem with the O(1) scheduler in Linux is that once tasks are placed on the expired list, they won’t be scheduled until all remaining tasks from the active list have a chance to execute. As a result, the performance of interactive tasks is affected. There’s a lot of jitter.

The main idea behind the CFS is that it uses a red-black tree as a run queue structure. Red-black tree belong to the family of dynamic tree structures. When nodes are added or removed from the tree, the tree will self-balance itself so that all the paths from the root to the leaves are approximately the same cost.(see red-black tree explanation)
Tasks are ordered in the tree based on the amount of time they spend running on the CPU and that’s called Virtual Runtime. CFS tracks this virtual runtime in a nanosecond granularity. As we can see in this figure:
-	Each of the internal nodes in the tree corresponds to a task
-	The nodes to the left of the task correspond to tasks which had less time on the CPU, they had spent less virtual time and therefore they need to be scheduled sooner.
-	The nodes to the right of a task are those that have consumed more virtual time and they don’t have to be scheduled very soon.
-	The leaves in the tree really don’t play any role in the scheduler.
![scheduling](/assets/img/Posts/GIOS/P3L1/img31.png)

The CFS scheduling algorithm can be summarized as follows.
-	CFS always schedules the task which has the least vruntime on the CPU so that typically would be the leftmost node in the tree.
-	Periodically, CFS will increase vruntime of the task that’s currently executing. It will compare vruntime of the current task with that of the leftmost task in the tree.
-	To account for differences in the task priorities or in their niceness value, CFS changes the effective rate at which the tasks vruntime progresses. For lower priority tasks, time passes more quickly and their vruntime progresses faster. 
-	CFS uses really runqueue for all of the priority levels.

In terms of performance:
-	Selecting a task from this run queue to execute takes O(1) time, takes a constant amount of time since it’s just a matter of selecting the leftmost node in the tree (leftmost node is always cached).
-	At the same time, adding a task to the run queue takes logarithmic time relative to the total number of tasks in the system. Given the typical levels of load in current systems, O(log(N)) time is acceptable.

### Scheduling on Multiprocessors
![scheduling](/assets/img/Posts/GIOS/P3L1/img32.png)
In Shared Memory multi-Processing (SMP):
-	There are multiple CPUs.
-	Each CPU has its own private caches like L1 and L2.
-	The last level cache may or may not be shared among the CPUs.
-	There is a system memory DRAM that is shared across all CPUs. (Here we show just one memory component but it’s possible that there are multiple memory components. The point is that the system memory is shared among all CPUs)
![scheduling](/assets/img/Posts/GIOS/P3L1/img33.png)
In the current multicore world, each CPU can have multiple internal cores.
-	Each core will have private caches.
-	The entire multi-core CPU will have some shared last level cache
-	There will be some shared system memory.

We said in our earlier lectures that the performance of threads and processes is highly dependent on whether the state that the thread needs is in the cache or in memory.
-	Let’s say a thread was executing on CPU1 first. Over time, this thread was likely to bring a lot of states that it needs into the last level cache (LLC) associated with the CPU and the private caches on the CPU. In this case, the caches are hot and it helps with the performance.
-	If the thread is to be scheduled on CPU2, none of its state will be there. The thread will operate with a cold cache and it will have to bring all states back in, and that will affect performance.
![scheduling](/assets/img/Posts/GIOS/P3L1/img34.png)

Therefore what we want to achieve with scheduling on multi-CPU systems is to try to schedule the thread back on the same CPU where it executed on before.  It is more likely that its cache will be hot. We call this cache affinity. We want the scheduler to keep a task on the same CPU for as long as possible.
![scheduling](/assets/img/Posts/GIOS/P3L1/img35.png)

![scheduling](/assets/img/Posts/GIOS/P3L1/img36.png)
In addition to having multiple processors, it is also possible to have multiple memory nodes.
-	The CPU and the memory nodes will be connected via some type of interconnect. For instance on modern Intel platforms there is an interconnect Quick Pipe Interconnect (QPI).

-	One way these memory nodes can be configured is that a memory node can be technically connected to a subset of the CPUs, e.g. a socket that has multiple processors.

-	If that is the case, then the access from that set of CPUs to the memory nodes will be faster than access to memory node associated with a different set of CPUs. Both types of accesses will be made possible by the interconnect but they will take different amounts of time to access.
We call these types of platforms Non-Uniform Memory Access platforms (NUMA). So from a scheduling perspective, what would make sense is for the scheduler to divide tasks in such a way that tasks are bound to the CPUs that are closer to the memory nodes where the task states are stored. We call this type of scheduling NUMA aware scheduling.

### HyperThreading
![scheduling](/assets/img/Posts/GIOS/P3L1/img37.png)
Over time hardware architects have recognized that they can do certain things to help hide some of the overheads associated with context switching. One way this has been achieved is to make CPU have multiple sets of registers and each set of registers can describe the context of a separate thread (a separate execution entity). One term that’s used to refer to this is hyper threading.
-	Hyper threading refers to multiple hardware supported execution contexts (hyper threads).
-	There’s still just one CPU. On this CPU, only one of the threads can execute at a time.
-	The context switching between threads is very fast because the CPU only needs to switch from using one set of registers to another set of registers. Nothing has to be saved or restored.
This mechanism is also referred to as
-	Hardware multi-threading
-	Chip multi-threading
-	simultaneous multi-threading (SMT)

![scheduling](/assets/img/Posts/GIOS/P3L1/img38.png)

### Scheduling for Hyperthreading Platforms
To understand what’s required for a scheduler in SMT system, let’s look at the assumptions in Fedorova’s paper.
-	The first assumption is that a thread can issue an instruction on every single cycle. That means a CPU bound thread (which only issues instructions that need to run on CPU), will be able to achieve a maximum metric in terms of Instructions per Cycle (IPC) = 1. With only 1 CPU, we cannot have an IPC that is greater than 1.
-	The second assumption is that a memory access will take 4 cycles. What this means is that a memory bound thread will experience some idle cycles while it’s waiting for the memory access to return.
-	We also assume that the time it takes to context switch among the different hardware threads is instantaneous.
-	Lastly let’s assume that we have a SMT with 2 hardware threads.
![scheduling](/assets/img/Posts/GIOS/P3L1/img39.png)
![scheduling](/assets/img/Posts/GIOS/P3L1/img40.png)
![scheduling](/assets/img/Posts/GIOS/P3L1/img41.png)
![scheduling](/assets/img/Posts/GIOS/P3L1/img43.png)

### CPU Bound or Memory Bound?
![scheduling](/assets/img/Posts/GIOS/P3L1/img44.png)
To answer this question, we will use historic information and look at the threads’ past behavior, which is similar to what we did when trying to determine whether a thread is I/O bound or CPU bound. Previously we used sleep time for this type of differentiation of I/O vs CPU bound. However, that won’t work in this case.
-	First, the thread is not really sleeping when it’s waiting on a memory reference. The thread is active and it’s just waiting in some stage of the processor pipeline and not in some type of software queue.
-	Second, to keep track of the sleep time, we need some software methods but that’s not acceptable. We cannot execute software computation to decide whether the thread is CPU or memory bound given the fact that the context switch takes orders of cycles. The decision should be made fast.
-	Therefore we need some kind of hardware support, some information from the hardware in order to be able to answer this question.
![scheduling](/assets/img/Posts/GIOS/P3L1/img45.png)
Fortunately, modern hardware has lots of so-called hardware counters, which get updated as the processor is executing and keep information about various aspects of the execution. These include information about the cache usage (such as L1 L2 or LLC misses) or information about the number of instructions that were retired (which could be used to compute the IPC) or on newer platforms there’s also information on the power or energy usage of the CPU or other components of the system. There are a number of interfaces and tools that can be used to access these hardware counters via software.  For instance the oprofile or the Linux perf tool are available. If you look at the oprofile website it actually has a list of all of the hardware counters that are available for different architectures (hardware counters are different on different platforms).
![scheduling](/assets/img/Posts/GIOS/P3L1/img46.png)
-	They would typically use some combination of the counters that are available on the CPU, not just one in order to build a more accurate picture of the threads resource needs
-	They would also rely on some models that have been built for specific hardware platforms and that have been trained using some well-understood workloads.
-	We could run a workload that we know is memory intensive and we made some observations of the counters and now we know how to interpret them for other workloads.

### Scheduling with Hardware Counters
![scheduling](/assets/img/Posts/GIOS/P3L1/img47.png)
As a more concrete example, Fedorova speculates that a useful counter to use to detect a thread’s CPU-ness or memory-ness is Cycles per Instruction (CPI). She observes that:
-	a memory bound thread takes a lot of cycles to complete the instruction, i.e. it has a high CPI. 
-	a CPU bound thread completes an instruction every cycle or near that, i.e. CPI = 1 or a low CPI.
![scheduling](/assets/img/Posts/GIOS/P3L1/img48.png)
To explore this question:
-	She simulates a system that has 4 cores where every one of the cores is 4-way multithreaded (SMT) so there’s a total of 16 hardware contexts in her experimental test bed.

-	She wants to vary the threads that get assigned to these hardware contexts based on their CPI. So she creates a synthetic workload where her threads have a CPI of 1, 6,11, and 16. Clearly the threads with a low CPI will be the most CPU intensive. The threads with the CPI of 16 will be the most memory intensive. The overall workload mix has 4 threads of each kind.

-	Then what she wants to evaluate is what is the overall performance when a specific mix of threads gets assigned to each of these 16 hardware contexts. To understand the performance impact of different scheduling decisions, she uses the Instructions per Cycle as the metric. The system has 4 cores in total, so the maximum possible IPC is 4.
And then she conducts several experiments as shown in the figure. In every one of the experiments she manually and statically changes how the workload is distributed across the cores.
-	In the first experiment, every one of the cores runs identical mix where each hardware thread runs a task with a different CPI. On every core, the 4 hardware threads will be assigned threads that have a CPI of 1, 6, 11, and 16.

-	In the last experiment each of the cores runs different mix of threads, where on Core 0, all of the tasks are CPU intensive (CPI of 1), and on Core 3 all tasks are memory intensive (CPI of 16).

-	The second and the third experiments fall somewhere between these two extremes.
What she’s trying to do is to get some static decisions that a scheduler would have made and to understand whether it makes sense to make a scheduler that will use CPI as a metric.

### CPI Experiment Results
![scheduling](/assets/img/Posts/GIOS/P3L1/img49.png)
![scheduling](/assets/img/Posts/GIOS/P3L1/img50.png)
## Memory Management - P3L2

### Visual Metaphor
![thread improvements](/assets/img/Posts/GIOS/P3L2/img1.png)

### Memeory Management Goals

![thread improvements](/assets/img/Posts/GIOS/P3L2/img2.png)

The figure here illustrates that the virtual address space is subdivided into fixed size segments that we call pages.  The physical memory is divided into page frames of the same size.
-	In terms of allocation, the role of the OS is to map pages from the virtual memory into page frames of the physical memory.
-	In page based memory management system, the arbitration of the accesses is done by page tables.
Paging is not the only way to decouple the virtual and the physical memory. Another approach is segmentation based on segment.
-	With segmentation, the allocation process doesn’t use fixed sized pages. Instead, it uses more flexible-sized segments that can then be mapped into or swapped in/out of physical memory.
-	Arbitration uses segment registers that are typically supported on modern hardware.

![thread improvements](/assets/img/Posts/GIOS/P3L2/img3.png)

### Hardware Support
![thread improvements](/assets/img/Posts/GIOS/P3L2/img4.png)
 - every CPU is equipped with a memory management unit (MMU). The CPU issues virtual addresses to the memory management unit and it’s responsible for translating them into the appropriate physical address
 - Another way hardware supports memory management is by using designated registers during the address translation process. For instance, in a page based system there are registers that points to the currently active page table.
 - Since memory address translation happens on pretty much every memory reference, most memory management units would integrate a small cache of valid virtual to physical mappings. This is called the Translation Lookaside Buffer (TLB).  The presence of a TLB will make the entire translation process much faster since if this translation is present in this cache, there is no need to perform any additional operations to access the page table or the segment to validate the memory access.
 - the generation of the physical address from the virtual address is actually done by the hardware. The OS will maintain certain data structures such as the page tables to maintain certain information that’s necessary for the translation process but it is the hardware that performs the actual translation.

### Page Tables (important)
![thread improvements](/assets/img/Posts/GIOS/P3L2/img5.png)
page tables translate the virtual memory addresses to physical memory addresses.For each virtual address, an entry in the page table is used to determine the actual physical location that corresponds to that virtual address. 
Although the sizes in this drawing are a little bit off, the sizes of the pages of the virtual memory and the corresponding pages in physical memory are identical. By keeping the size of these two the same, we don’t have to keep track of the translation of every single individual virtual address. Instead we can only translate the first virtual address in a page to the first virtual address in a page frame in physical memory and then remaining addresses in the virtual memory page will map to the corresponding offsets in the physical memory page frame. As a result, we can reduce the number of entries we have to maintain in the page table. 

-	That means only the first portion of the virtual address is used to index into the page table. We call this part of the virtual address the virtual page number (VPN), the rest of the virtual address is the offset.
-	The VPN is used as an offset into the page table and that will find the physical frame number (PFN).
-	PFN is the physical address of the physical frame in DRAM.
-	Now to complete the full translation of the virtual address, that PFN needs to be summed with the offset in the virtual address to produce the actual physical address.
-	The resulting physical address can ultimately be used to reference the appropriate location in physical memory.

Let’s say we want to access an array data structure and to initialize it for the very first time.
-	We have already allocated the memory for that array into the virtual address space of the process, but we never accessed it before.
-	Since this portion of the address space has not been accessed before, the OS has not yet allocated physical memory for it.
-	What will happen the first time we access this memory is that the OS will realize that there isn’t physical memory that corresponds to this range of virtual memory addresses so it will take a page of physical memory (P2 in this case, a page that’s free obviously), and it will establish a mapping between this virtual address (V_k and the offset) and the physical address of page P2 in physical memory.
![thread improvements](/assets/img/Posts/GIOS/P3L2/img6.png)

(Note that the physical memory for this array is only allocated when the process is first trying to access it during this initialization routine. We refer to this as allocation on first touch. The reason for this is that we want to make sure that physical memory is allocated only when it’s really needed because sometimes programmers may create data structures that they don’t really use)

If a process hasn’t used some of its memory pages for a long time, it is likely that those pages will be reclaimed.  As a result the contents will no longer be stored in physical memory and they will be pushed on disk. Then maybe some other contents will find their way into the physical memory.

In order to detect this (whether the page has been reclaimed):
-	Page table entries don’t only consist of the physical frame number. They also have a number of bits that tell the memory management system something about the validity of the access. For instance, if the page is in memory and the mapping is valid, then this bit is 1. If the page is not in memory, then this bit is 0.

-	If the hardware MMU sees that this is a bit 0 in the page table entry, it will raise a fault. Once the hardware determines that the mapping is invalid and faults, control gets passed to the OS.

-	The OS at that point will make a number of decisions: Should the access be permitted? Where exactly is the page located? Where should it be brought into DRAM?

-	As long as a valid virtual address is being accessed, ultimately on fault, a mapping will be reestablished between a valid virtual address and a valid location in physical memory. However, it is likely that if the page was pushed on disk and brought back into memory, it will be placed in a completely different memory location. As a result, the entry in the page table needs to be correctly updated.
![thread improvements](/assets/img/Posts/GIOS/P3L2/img7.png)

So as a final note to summarize:
-	The OS creates a page table for every process that it runs.

-	The OS will maintain a page table on every single process that exists.

-	Whenever a context switch is performed, the OS has to make sure that it switches to the page table of the newly context switched process. We said that hardware assists with page table accesses by maintaining a register that points to the active page table.

-	On x86 platforms, there is a CR3 register, so on a context switch we need to update the CR3 register with the address of the new page table.

### Page Table Entry

Every page table entry would have:
-	The physical page frame number
-	The present bit which indicates whether the content of the virtual memory is present in physical memory.

There are a number of other fields that are part of each page table entry that the OS uses during memory management operations and that the hardware understands and knows how to interpret.
-	Most hardware supports a dirty bit which gets set whenever a page is written to. This is useful in file systems where files are cached in memory. We can use this dirty bit to detect which files have been written to and which need to be updated on disk.

-	There is the access bit. This can keep track of whether the page has been accessed, for read or for write.

-	Other useful information would include certain protection bits: whether a page can only be read or also written to or maybe some other operation is permissible.

![thread improvements](/assets/img/Posts/GIOS/P3L2/img8.png)
![thread improvements](/assets/img/Posts/GIOS/P3L2/img9.png)

The MMU uses the page table entries not just to perform the address translation but also relies on these bits to establish the validity of the access.

-	If the hardware determines that a physical memory access cannot be performed, it causes a page fault.
-	If this happens, then the CPU will place an error code on the stack of the kernel and generate a trap into the OS kernel.
-	That will in turn generate a page fault handler and the page fault handler will determine what action needs to be taken depending on the error code and the address that caused the fault.
-	Key pieces of information in this error code will include whether or not the fault was caused because the page was not present (then it needs to be brought in from disk), or because there is some sort of permission protection that was violated.
-	On x86 platforms, the error code information is generated from some of the flags in the page table entry
-	The faulting address is also needed for the page fault handler and it is stored in the CR2 register.

### Page Table Size 
A page table has a number of entries = the number of virtual page numbers that exists in a virtual address space.

For every one of these entries, the page table needs to hold the physical frame number as well as some other information like the permission bits. So here’s something that will make sense on a 32 bit architecture:
-	Each page table entry is 4 bytes and that includes the PFN as well as the flags.
-	The total number of page table entries will depend on the total number of VPNs.  How many VPNs we can have?  That will depend on the size of the virtual address space and of the page size itself. In this example, we have a 32 bit physical memory as well as a 32 bit virtual addresses.
-	That will be 2^32 and that will have to be divided by the actual page size. Different hardware platforms support different page sizes and let’s say we pick a page size = 4 kb.
-	In this, we will see that the page table will be 4MB and it will be 4MB for every single process.   
With many active processes in an OS today, the overall page table can be very large.
Also if we work through the same calculation for a 64 bit architecture that:
-	Has a page table entry size = 8 bytes
-	Uses the same page size = 4 kb
We come up with a really scary number of 32 petabytes per process!

![thread improvements](/assets/img/Posts/GIOS/P3L2/img10.png)

### Multilevel Page Tables
The answer to our storage issue is that we don’t design page tables in this flat manner anymore. Instead, page tables have evolved from a flat page map to a more hierarchical multilevel structure. 

-	The outer level here is referred to a page table directory. Its elements are not pointers to pages, instead they are pointers to page tables.
-	The internal page table has proper page table entries that point to pages. Their entries have the page frame numbers and all the protection bits for the physical addresses that are referenced by the corresponding virtual address.
-	An important thing to note is that the internal page tables exist only for those virtual memory regions that are actually valid.  Any kinds of holes in the virtual memory space will result in lack of internal page tables.  For those holes, there won’t be any internal page tables allocated for them.

[thread improvements](/assets/img/Posts/GIOS/P3L2/img11.png)

To find the right element in this page table structure, the virtual address is split into yet another component.  Using this address format, this is what we need to find the correct physical address.
-	The first two components of the address are used as indices into the different levels of the page table hierarchy and to produce the physical frame number PFN.
-	The first portion is used as an index into the outer page table, which will determine the page table directory entry that points to the actual page table.
-	The second index is used as an index into the internal page table. This will produce the page table entry that contains PFN.
-	The last portion of the address is still the offset. So that’s going to be used to compute the offset within the actual physical page.
-	Then we can add the PFN with the offset just like before and compute the physical address.

In this example:
-	The address format is such that it uses 10 bits for the internal page table offset (p2). That means that this internal page table can have 2^10 VPN/PFN.
-	Also we used 10 bits as the offset into the actual page (d), that means that the page size is 2^10.
-	Therefore if we do the math, every single internal page table can address 2^10 (the number of entries) times the page size (2 ^ 10) = 1 MByte of physical memory.

This means that whenever there is a gap in virtual memory that’s 1MB size, we don’t need to allocate that internal page table. This will reduce the overall size of the page table required for a particular process. This is in contrast with the single level page table design, where the page table has to be able to translate every single virtual address and it has entries for every single virtual page number.
[thread improvements](/assets/img/Posts/GIOS/P3L2/img12.png)

The scheme can be further extended to use additional layers using the same principle.
-	For instance we can add another third level that can consist of pointers to page table directories.
-	Adding yet another 4th level to this, would consist of a map of pointers to page table directories.
[thread improvements](/assets/img/Posts/GIOS/P3L2/img13.png)


### Speeding Up Translation TLB
In the single level page table design, a memory reference will actually require 2 memory references: one to access the page table entries and the second one to actually perform the memory access at the physical address.
In the 4 level page table, we will need to perform 4 memory accesses to read the page table entries to produce the physical frame number and another access to the physical memory. Obviously this can be very costly and can lead to a slowdown.

The standard technique to avoid these repeated accesses to memory is to use a page table cache. On most architectures, the MMU hardware integrated a hardware cache that’s dedicated for caching address translations and this cache is called the *Translation Lookaside Buffer (TLB)*. 

On each address translation, first the TLB cache is quickly referenced and if the resulting address can be generated from the TLB contents, then we have a TLB hit and we can bypass all of the other required memory accesses to perform the translation.

-	If we have a TLB miss, the address isn’t present in the TLB cache, then we have to perform all of the address translation steps by accessing the page tables from memory.

-	In addition to the proper address translation, the TLB entries will contain all of the necessary protection and validity bits to verify that the access is correct or if necessary to generate a fault.

-	It turns out that even a small number of entries in the TLB can result in a high TLB rate and this is because we have typically a high temporal and spatial locality in the memory references.

-	On recent x86 platforms for instance, there is a separate TLB for data and instructions and each of those have a modest number of entries: 64 for the data and 128 for the instruction TLB, per core.

-	In addition to these 2, there is also another shared second level TLB that’s shared across all cores and that one is a little bit larger (512 entries). This is for the Intel-i7 platforms and this was determined to be sufficiently effective to address the typical memory access needs of processes today.

[thread improvements](/assets/img/Posts/GIOS/P3L2/img14.png)

### Inverted Page Tables

A completely different way to organize the address translation process is to create so-called inverted page tables.  The inverted page table is a one-page table where each table entry corresponds to each page frame in the physical memory (1:1 page table entry : page mapping) and a single page table is used to represent the paging information of all processes. 
[thread improvements](/assets/img/Posts/GIOS/P3L2/img14.png)

-	Each entry in the page table contains the following fields: Page number, Process ID, Control bits and Chained pointer. This technique is called inverted paging as the indexing is done with respect to the physical frame number instead of the virtual address number.

-	To find the translation, the page table is searched based on the process ID (pid) and the virtual page number (p). When the specific pid and p is found in the page table, the index (i) of the page table entry that contains this specific pid and p will be the same as the physical frame number (due to the one page 1:1 mapping).

-	Then the index i is combined with the offset (d) to generate the address in the physical memory.

The problem with inverted page tables is that we have to perform a linear search of the page table to see which one of its entries matches the specific pid and p. Since physical memory can be arbitrarily assigned to different processes, the table isn’t really ordered. Two consecutive entries could represent memory location allocated to 2 different processes.
There really is no clever search technique to speed up this process. In practice, the TLB will cache a lot of these memory references so the “searching” is not performed very frequently. However we still have to perform it when necessary. 
To address this issue, inverted page tables are supplemented with the hashing page tables. In most general terms, a hashing page table looks like something as follows.
-	A hash is computed on a part of the addresses.

-	That is an entry into the hash table that points to a linked list of possible matches for this part of the address.

-	This allows us to speed up the process of the linear search, to narrow it down to a few possible entries, into the inverted page table.

As a result, we speed up the address translation.

[thread improvements](/assets/img/Posts/GIOS/P3L2/img15.png)

### Segmentation 

[thread improvements](/assets/img/Posts/GIOS/P3L2/img16.png)
Virtual to physical memory mappings can also be performed using segment and this process is referred to as segmentation.
With segments, the address space is divided into components of arbitrary granularity (arbitrary size).
Typically different segments will correspond to some logically meaningful components of the address space like the code, the heap, the data, etc.
A virtual address in the segmented memory mode includes a segment descriptor and an actual offset. The segment descriptor and the offset are used in combination with a descriptor table to find information regarding the physical address.
In its pure form, a segment could be represented with a continuous portion of physical memory. In that case, the segment would be defined by its base address and its limit registers which implies the segment size. So we can have segments with different sizes using this method.
In practice, segmentation and paging are used together. What this means is that the address produced using this method (linear address) will be passed to the paging unit, i.e. the multi-level hierarchical page table to ultimately compute the actual physical address to reference the appropriate memory location.

### Page Size
In the examples that we showed so far regarding the address formats, we used 10 or 12 bits for the offset.  Well this offset determines the total amount of addresses in one page, i.e., the page. In the examples in which we had 10-bit offset in the address field, these 10 bits could address 2^10 bytes in the page and therefore the page size is 1KB. Similarly, the examples that had 12-bit offset for the address format, could address 2^12 bytes, i.e. 4KB page size.
In practice, systems support different page sizes. For Linux and x86, there are several common page sizes. Page size of 4KB is pretty popular and that’s the default in the Linux x86 environment. However, page sizes can be much larger (large page size of 2MB or even huge page size of 1GB) To address a 2MB page, we need 21 bits for page offset and for a 1GB page we need 30 bits.
•	One benefit of using these larger page sizes is that so there will be fewer entries in the page table. In fact, use of these large page sizes will significantly reduce the size of the page table. Compared to the page table size with 4KB page, the large 2MB page will reduce the page table size by a factor of 512 and the huge 1GB page will reduce the page table size by another 512. 

-	The benefits of larger page sizes are that they require smaller page tables.
-	Also we can have increased number of TLB hits, because we’ll be able to translate more of the physical memory using the TLB cache.
-	The downside of the larger pages is that if this large virtual memory page is not densely populated, there will be larger unused gaps within the page. This will lead to internal fragmentation. There will be wasted memory in these allocated regions of memory.

Because of this, smaller page sizes of 4KB are commonly used. There are some scenarios where large page sizes are necessary, such as databases or memory data stores. On different systems, depending on the OS and the hardware architecture, different page sizes may be supported. For instance, on Solaris 10 the SPARC architecture have page size options of 8KB, 4MB, or 2GB.

[thread improvements](/assets/img/Posts/GIOS/P3L2/img17.png)

### Memory Allocation 
So far we have not explained how the OS allocates a particular portion of the memory to a process. This is the job of the memory allocation mechanisms in the MMU subsystem of the operating system.

NEED TO COMPLETE SECTION

## Inter-Process Communication - P3L3


# Useful Resources 
## Quizlet
- https://quizlet.com/user/quizlette8384429/folders/gios?tag=Midterm


## Textbooks and other reference materials:

- https://pages.cs.wisc.edu/~remzi/OSTEP/ Operating Systems: Three Easy Pieces

- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://broman.dev/download/The%20Linux%20Programming%20Interface.pdf The Linux Programming Interface

- https://beej.us/guide/ 
Specifically - an introduction to socket programming:  
- https://beej.us/guide/bgnet0/

- https://www.kernel.org/doc/man-pages/ The Linux man-pages project


## C References:

- https://ocw.mit.edu/courses/6-087-practical-programming-in-c-january-iap-2010/pages/lecture-notes/

- https://learnxinyminutes.com/c/

- https://www.linkedin.com/learning/career-journey?initPlanV2=true&u=2163426

- http://goshdarnfunctionpointers.com/

- http://www.cprogramming.com/tutorial/function-pointers.html


Presentation on programming with C from Chris D. (with some updates from TAs)

Youtube of the Q&A C Video from Chris. D.

Harvard CS50 course on C and their reference site.

C/Standard Library docs

C, Pointers, and other lessons on C

Thread programming:

Posix Thread Programming tutorial

## Network programming resources:

https://beej.us/guide/bgnet/html/
https://www.codeproject.com/Articles/586000/Networking-and-Socket-programming-tutorial-in-C
http://www.binarytides.com/socket-programming-c-linux-tutorial/

## Debugging:


## some useful cheatsheets: 

Vim Cheat Sheet
TMUX Cheat Sheet
GIT Cheat Sheet
Linux source cross-references:

Linux Kernel Map
Linux source cross-reference
reference_material

# Past course resources
- https://github.gatech.edu/rdiaconescu3/6200_GIOS_flashcards/blob/main/GIOS_flashcards.apkg
- https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL3UvcyFBbXBpTXN0VG5CRWpzUUtpTk8yRzdiR21PLWJyP2U9Y2R1SXRJ&cid=23119C53CB32626A&id=23119C53CB32626A%216274&parId=root&o=OneUp
- https://www.omscs-notes.com/operating-systems/midterm-exam-review-questions/
