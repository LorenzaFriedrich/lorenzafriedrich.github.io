---
title: CS 6200 - Graduate Introduction to Operating Systems
date: 2025-08-18 13:19:00 +0800
categories: [GaTech OMSCS, Operating Systems]
tags: [c, c++, powershell, linux, wsl]     # TAG names should always be lowercase
description: Overview & Projects from GIOS
---


# Welcome
#### Note
For all OMSCS courses - I will provide pseudocode and video links, but I need to keep code in a private repository to uphold the honor code. Descriptions, notes, etc are below. 

# Projects

## Setting up my test environment 
The first thing you do in GIOS is set up your test environment. There are a million different ways to do this, I settled on VS code and docker following the tutorial below: 
- [Set Up for Docker with VSCode and WSL2](https://docs.google.com/document/d/1-TLnnWRfcqFjuzlaAvsxXppVnlklKBb9FBv_stK86tg/edit?tab=t.0)


### VSCode and Docker 
GIOS repo: 
- https://github.gatech.edu/gios-fall-25/environment

#### Step 1
- Install Linux on Windows with WSL (for GIOSFA25 get UBUNTU 20.04)
- Install Docker Desktop and add GIOS image
- Resource: https://www.docker.com/ && 
https://learn.microsoft.com/en-us/windows/wsl/install
```powershell
wsl.exe --install Ubuntu-20.04
```
Notes:
1. Make sure you set your linux default WSL to the 20.04 ubuntu for GIOS (see Microsoft documentation)
2. Make sure you are on WSL 2 not 1 to avoid errors

#### Step 2
- Open WSL and get vscode on wsl
```bash
wsl
code .
```
Notes:
1. in VSCode get the Docker, Remote Development, and C/C++ extensions

#### Step 3
- Download all the absolutely necessary code provided in the GIOS repo in vscode (since it should have opened after it downloaded) or linux terminal 
Including: Add the course PPA, update the repos, install requirements (gcc should work after you've done this)

#### Step 4
- Add the tutorial's configure.yml file to a home folder you plan on using for the projects 
```yaml
name: cs6200-gios            # see Ref 1 below
services:
  cs6200-gios:               # see Ref 1 below
    image: gtomscs6200/fall25-environment:latest # note: set appropriately to current semester
    container_name: dev-env  # see Ref 1 below
    tty: true                # see Ref 2 below
    working_dir: /home/files
    restart: unless-stopped
    volumes:
      - .:/home/files:rw     # see Ref 3 below
```

- Ref 1: These names/labels can be set according to your preference

- Ref 2: https://stackoverflow.com/a/42597165

- Ref 3: general form is `<host-path>:<container-path>:rw`, where `.` here denotes the host-system 
- location in which `docker compose up -d` command is run on file `compose.yaml`, with
- corresponding bind mounting to container location `/home/files` as designated above. Note that you
- may designate `<container-path>` per your own preference. Suffix `:rw` denotes read & write modes.
- See https://docs.docker.com/compose/compose-file/ and https://docs.docker.com/compose/compose-file/compose-file-v3/#volumes for more details.

##### [Reference & Source for Step 4](https://docs.google.com/document/d/1-TLnnWRfcqFjuzlaAvsxXppVnlklKBb9FBv_stK86tg/edit?tab=t.0)
#### Step 5
Open the Dev Containers extension in VS Code. You should see cs6200-gios, and inside it, dev-env.
   
![docker img vscode](/assets/img/Posts/GIOS/dockervscode.png){: width="972" height="589" }

- If the dev environment is not running, right-click on dev-env and select Start.

- Once it’s running, right-click on dev-env again and select Attach Visual Studio Code to open the Docker container in VS Code.

You’ll know you’re inside the container if the remote connection status in the bottom left corner shows something like Container: gtomscs6200/fa25-environment.

#### Step 6
Open up a new terminal in your container in VSCode and run inotify
Resources:
- [Man Page](https://www.man7.org/linux/man-pages/man7/inotify.7.html)
- [inotify code](https://drive.google.com/file/d/1dMIPMESO3iK41j5fvkbTF4JVBU7XS0ej/view)

Make a new c file called inotify (or whatever you want), copy in the inotify code linked above and run the following

```bash
make inotify

./inotify . 
```
Output should look something like
```bash
root@54c5497edb9c:/home/files# ./inotify .
Press ENTER key to terminate.
Listening for events.
```
#### Step 7
Open up the split screen terminal windows and test out inotify
```bash
//on right side of the split terminal run the following tests:

ll //not one-one

cat inotify.c 
```
Output should look like this for >11
- LEFT
```bash
root@54c5497edb9c:/home/files# ./inotify .
Press ENTER key to terminate.
Listening for events.
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
```
- RIGHT
```bash
root@54c5497edb9c:/home/files# ll
total 56
drwxr-xr-x 6 1000 1000  4096 Aug 22 16:07 ./
drwxr-xr-x 1 root root  4096 Aug 22 15:25 ../
-rw-r--r-- 1 1000 1000   318 Aug 22 15:25 compose.yaml
-rwxr-xr-x 1 root root 17400 Aug 22 15:45 inotify*
-rw-r--r-- 1 root root  4677 Aug 22 15:44 inotify.c
drwxr-xr-x 3 1000 1000  4096 Aug 22 16:14 munit/
drwxr-xr-x 2 1000 1000  4096 Aug 22 15:21 p1/
drwxr-xr-x 2 1000 1000  4096 Aug 22 15:21 p3/
drwxr-xr-x 2 1000 1000  4096 Aug 22 15:21 p4/
```
AND this for >cat inotify.c
- LEFT
```bash
root@54c5497edb9c:/home/files# ./inotify .
Press ENTER key to terminate.
Listening for events.
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./inotify.c [file]
IN_CLOSE_NOWRITE: ./inotify.c [file]
```
- RIGHT

```bash
root@54c5497edb9c:/home/files# cat inotify.c
#include <errno.h>
#include <poll.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/inotify.h>
#include <unistd.h>
#include <string.h>

/* Read all available inotify events from the file descriptor 'fd'.
  wd is the table of watch descriptors for the directories in argv.
  argc is the length of wd and argv.
  argv is the list of watched directories.
  Entry 0 of wd and argv is unused. */

static void
handle_events(int fd, int *wd, int argc, char* argv[])
{
    /* Some systems cannot read integer variables if they are not
      properly aligned. On other systems, incorrect alignment may
      decrease performance. Hence, the buffer used for reading from
      the inotify file descriptor should have the same alignment as
      struct inotify_event. */

    char buf[4096]
        __attribute__ ((aligned(__alignof__(struct inotify_event))));
    const struct inotify_event *event;
    ssize_t len;

    /* Loop while events can be read from inotify file descriptor. */

    for (;;) {

        /* Read some events. */

        len = read(fd, buf, sizeof(buf));
        if (len == -1 && errno != EAGAIN) {
            perror("read");
            exit(EXIT_FAILURE);
        }

        /* If the nonblocking read() found no events to read, then
          it returns -1 with errno set to EAGAIN. In that case,
          we exit the loop. */

        if (len <= 0)
            break;

        /* Loop over all events in the buffer. */

        for (char *ptr = buf; ptr < buf + len;
                ptr += sizeof(struct inotify_event) + event->len) {

            event = (const struct inotify_event *) ptr;

            /* Print event type. */

            if (event->mask & IN_OPEN)
                printf("IN_OPEN: ");
            if (event->mask & IN_CLOSE_NOWRITE)
                printf("IN_CLOSE_NOWRITE: ");
            if (event->mask & IN_CLOSE_WRITE)
                printf("IN_CLOSE_WRITE: ");

            /* Print the name of the watched directory. */

            for (int i = 1; i < argc; ++i) {
                if (wd[i] == event->wd) {
                    printf("%s/", argv[i]);
                    break;
                }
            }

            /* Print the name of the file. */

            if (event->len)
                printf("%s", event->name);

            /* Print type of filesystem object. */

            if (event->mask & IN_ISDIR)
                printf(" [directory]\n");
            else
                printf(" [file]\n");
        }
    }
}

int
main(int argc, char* argv[])
{
    char buf;
    int fd, i, poll_num;
    int *wd;
    nfds_t nfds;
    struct pollfd fds[2];

    if (argc < 2) {
        printf("Usage: %s PATH [PATH ...]\n", argv[0]);
        exit(EXIT_FAILURE);
    }

    printf("Press ENTER key to terminate.\n");

    /* Create the file descriptor for accessing the inotify API. */

    fd = inotify_init1(IN_NONBLOCK);
    if (fd == -1) {
        perror("inotify_init1");
        exit(EXIT_FAILURE);
    }

    /* Allocate memory for watch descriptors. */

    wd = calloc(argc, sizeof(int));
    if (wd == NULL) {
        perror("calloc");
        exit(EXIT_FAILURE);
    }

    /* Mark directories for events
      - file was opened
      - file was closed */

    for (i = 1; i < argc; i++) {
        wd[i] = inotify_add_watch(fd, argv[i],
                                  IN_OPEN | IN_CLOSE);
        if (wd[i] == -1) {
            fprintf(stderr, "Cannot watch '%s': %s\n",
                    argv[i], strerror(errno));
            exit(EXIT_FAILURE);
        }
    }

    /* Prepare for polling. */

    nfds = 2;

    fds[0].fd = STDIN_FILENO;       /* Console input */
    fds[0].events = POLLIN;

    fds[1].fd = fd;                 /* Inotify input */
    fds[1].events = POLLIN;

    /* Wait for events and/or terminal input. */

    printf("Listening for events.\n");
    while (1) {
        poll_num = poll(fds, nfds, -1);
        if (poll_num == -1) {
            if (errno == EINTR)
                continue;
            perror("poll");
            exit(EXIT_FAILURE);
        }

        if (poll_num > 0) {

            if (fds[0].revents & POLLIN) {

                /* Console input is available. Empty stdin and quit. */

                while (read(STDIN_FILENO, &buf, 1) > 0 && buf != '\n')
                    continue;
                break;
            }

            if (fds[1].revents & POLLIN) {

                /* Inotify events are available. */

                handle_events(fd, wd, argc, argv);
            }
        }
    }

    printf("Listening for events stopped.\n");

    /* Close inotify file descriptor. */

    close(fd);

    free(wd);
    exit(EXIT_SUCCESS);
}
root@54c5497edb9c:/home/files# 
```

#### Other Unit Tests
```
git clone https://github.com/nemequ/munit.git

gcc -Wall example.c munit.c munit.h -o example

./example
```
Output should look like 
```bash
Running test suite with seed 0x5a13d21b...
/example/compare                     [ OK    ] [ 0.00006970 / 0.00000230 CPU ]
/example/rand                        [ OK    ] [ 0.00000620 / 0.00000560 CPU ]
/example/parameters                  
  foo=one, bar=red                   [ OK    ] [ 0.00000610 / 0.00000540 CPU ]
  foo=one, bar=green                 [ OK    ] [ 0.00000610 / 0.00000540 CPU ]
  foo=one, bar=blue                  [ OK    ] [ 0.00000600 / 0.00000540 CPU ]
  foo=two, bar=red                   [ OK    ] [ 0.00000600 / 0.00000550 CPU ]
  foo=two, bar=green                 [ OK    ] [ 0.00000600 / 0.00000540 CPU ]
  foo=two, bar=blue                  [ OK    ] [ 0.00000870 / 0.00000750 CPU ]
  foo=three, bar=red                 [ OK    ] [ 0.00000620 / 0.00000570 CPU ]
  foo=three, bar=green               [ OK    ] [ 0.00000640 / 0.00000570 CPU ]
  foo=three, bar=blue                [ OK    ] [ 0.00000600 / 0.00000540 CPU ]
11 of 11 (100%) tests successful, 0 (0%) test skipped.
```
#### Notes for me:
- project is held on ubuntu 20.04 callled 'gios'
```bash
gios@DESKTOP-QUQS18F://home/gios/projects$
```
## Overview - Sequence of Projects 
1. Threads
3. Concurrency
4. Synchronization

Projects will cover single-node OS mechanisms (inter-process communication, scheduling, etc) and multi-node OS mechanisms (remote procedure calls (RPC), etc) and experimental design and evaluation 
All programs are in C and Linux
## Project 1

### Warm Up 

### gfclient:
#### libcurl breakdown
The libcurl “easy” workflow

Using libcurl, the steps look like this:

1. Global init (optional)

```c
curl_global_init(CURL_GLOBAL_DEFAULT);
```


Sets up any shared resources. In project this maps to gfc_global_init().

2. Create handle

```c
CURL *curl = curl_easy_init();
```


Allocates a request object. In project: *gfc_create()*.

3. Set options on the handle
```c
curl_easy_setopt(curl, CURLOPT_URL, "http://example.com");
curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, my_write_func);
curl_easy_setopt(curl, CURLOPT_WRITEDATA, my_data);
```

Each call changes a field inside the handle.
In your project: gfc_set_server(), gfc_set_path(), gfc_set_writefunc(), gfc_set_writearg(), etc.

4. Perform the transfer
```c
res = curl_easy_perform(curl);
```

Blocking call — does DNS lookup, connects, sends request, streams data, invokes callbacks.
In project: gfc_perform().

5. Check result & get information
```c
curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE, &code);
```

In project: gfc_get_status(), gfc_get_filelen(), gfc_get_bytesreceived().

6. Cleanup handle
```c
curl_easy_cleanup(curl);
```

Frees memory for the request. In project: gfc_cleanup().

7. Global cleanup (optional)
```c
curl_global_cleanup();
```

Tears down global state. In project: gfc_global_cleanup().

### Mapping libcurl's API to GETFILE protocol

| libcurl easy function          | gfclient equivalent                                                                                         |
| ------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| `curl_global_init()`           | `gfc_global_init()`                                                                                         |
| `curl_easy_init()`             | `gfc_create()`                                                                                              |
| `curl_easy_setopt(handle, …)`  | `gfc_set_server()`, `gfc_set_port()`, `gfc_set_path()`, `gfc_set_headerfunc()`, `gfc_set_writefunc()`, etc. |
| `curl_easy_perform(handle)`    | `gfc_perform()`                                                                                             |
| `curl_easy_getinfo(handle, …)` | `gfc_get_status()`, `gfc_get_filelen()`, `gfc_get_bytesreceived()`                                          |
| `curl_easy_cleanup(handle)`    | `gfc_cleanup()`                                                                                             |
| `curl_global_cleanup()`        | `gfc_global_cleanup()`                                                                                      |

## Project 3

## Project 4

# Lecture Notes

## P1L1 - Introduction to Operating Systems 
The course covers:
- what are operating systems
- why are operating systems needed
- how are operating systems designed and implemented? 

Course Topics include OS abstractions, mechanisms, and policies for
- processes and process management
- threads and concurrency
- resource management: scheduling, memory management
- OS services for communication and I/O
- OS support for distributed services
## P1L2 - What is a Operating System?

In simplest terms, it is a piece of software that abstracts and arbitrates the hardware of a system. 

#### 2. An operating system is like a toy shop manager:

Toy Shop Manager
- Directs operational Resources by controlling use of employee time, parts, tools
- Enforces working policies - fairness safety, clean up
- Mitigates difficulty of complex tasks - simplifies operation and optimizes performance

Operating systems
- Directs operational Resources - controls use of CPU, memory, peripheral devices
- Enforces working policies - fair resouce access, limits to resource usage
- Mitigates difficulty of complex tasks - abstract hardware details (system calls)

Operating System Definition

![OS Definition](/assets/img/Posts/GIOS/P1L2whatisanOS.png){: width="972" height="589" .w-50 .left}

An operating system is a layer of systems software that:
- directly has priviledged access to the underlying hardware;
- hides the hardware complexity;
- manages hardware on behalf of one or more applications according to some predefined policies
- In addition, it ensures that applications are isolated and protected from one another 

Important definitions: abstraction versus arbitration, scheduler, device driver, file system, memory management, OS Design principals 

#### 8. Operating systems Examples
Desktop (microsoft Windows, UNIX-based OS (including mac OS X (BSD), Linux)) and Embedded (Android, iOS, Symbian) are the focus of the class 

#### 9. OS Elements
Abstractions (nouns):
- process, thread, file, socket, memory page

Mechanisms (verbs):
- create, schedule, open, write, allocate

Policies:
- least recently used (LRU), earliest deadline first (EDF)

#### 10. Design Prinipals 
- Separation of mechanism & policy: implement flexible mechanisms to support many policies (e.g. LRU, LFU, random)
- Optimize for the common case: where will the OS be used? What will the user want to execute on that machine? What are the workload requirements? 

#### 11. User/Kernel Protection Boundary 
![User Kernel](/assets/img/Posts/GIOS/P1L2Userkernelbundary.png){: width="972" height="589" .w-50 .left}
The user/kernel protection boundary exists to keep users outside of the kernel level. There is a user-kernel switch that is supported by hardware that include trap instructions (unauthorized kernel access), system call (open (file), send (socket), malloc (memory)), and signals 

#### 12. System Calls Flowchart

![System Call Flowchart](/assets/img/Posts/GIOS/P1L2 Systemcallflowchart.png){: width="972" height="589" .w-50 .left}

To make a system call, an application must
•	write arguments,
•	save all relevant data at a well-defined location (The well-defined location is necessary so that the operating system kernel, based on the system call number, can determine which, how many arguments it should retrieve and where they are)
•	make the actual system call using this specific system call number. 
The arguments can either be passed directly between the user program and the operating system, or they can be passed indirectly by specifying their address.

#### 13. Crossing the User/Kernel Protection Boundary 
User/kernel transitions are:
- hardware supported (e.g. traps on illegal instructions or memory accesses requiring special priviledge)
- involves a number of instructions (e.g. ~50-100ns on a 2 GHz machine running linux
- switches locality (affects hardware cache)
- NOT CHEAP!
#### 14. Basic OS Services

Scheduler - responsible for controlling access to the CPU

Memory Manager - responsible for allocating the underlying physical memory to one or more co-running applications. It also maskes sure that applications don't overwrite eachothers memory. 

Block device driver - responsible for access to a block device like a disk

File system - part of the higher level services that are not necessarily related to the hardware but are one of the higher level abstractions supported by the operating system. 

![Basic OS](/assets/img/Posts/GIOS/P1L2BasicOSServices.png){: width="972" height="589" }

#### 14. Monolithic OS 
![Mono OS](/assets/img/Posts/GIOS/P1L2Mono.png){: width="972" height="589" .w-50 .left}
Historically, operating systems were monolithic. This could include file systems that were specialized in sequential workloads, or file systems optimized for accessing databases. 
- Pros: The benefit of this approach is that everything is included in the operating system. The abstractions, all the services, and everything is packaged at the same time. And because of that, there's some possibilities for some compile-time optimizations.
- Cons: The downside is that there is too much state, too much code that's hard to maintain, debug, upgrade. And then its large size also poses large memory requirements, and that can always impact the performance that the applications are able to observe.

#### 15. Modular OS (more modern)
![Modular OS](/assets/img/Posts/GIOS/P1L2Modular.png){: width="972" height="589" .w-50 .left}
The modular approach is the more modern/common approach. It is used with the Linux OS. Everything can be added as a module. With this approach, you can customize which particular file system or scheduler the operating system uses. 
You can also dynamically install new modules in the operating system. 
- Pros: easier to maintain/upgrade, less resource intensive.
- Cons: Can reduce potential optimizations because of the level of indirection required because you have to go through interface specification before you can go to the implementation of a service. 

#### 16. Microkernel (embedded systems)
![Modular OS](/assets/img/Posts/GIOS/P1L2Microkernel.png){: width="972" height="589" .w-50 .left} Require only the basic primitives at the operating system level. For instance, at the OS level, the microkernel can support some basic services such as representing an executing application, its address space, and threads. 
- Pros: smol, easy to verify and test code. This is particularly important for operating systems that need to behave properly. 
- Cons: portability is questionable. Because there are so many one off instances of embedded systems, it can become difficult to find common components which leads to complexity. Also there is the potential for frequent user/kernel crossings which is costly. 

#### 17. Linux Architecture + Mac OS
Below is what the Linux environment looks like.
- 	Starting at the bottom, we have the hardware, and the Linux kernel abstracts and manages that hardware by supporting a number of abstractions and the associated mechanisms.
-	Then comes a number of standard libraries, such as those that implement the system call interfaces,
-	Then a number of utility programs that make it easier for users and developers to interact with the operating system.
-	Finally, at the very top, you have the user developed applications.

![Linux OS](/assets/img/Posts/GIOS/P1L2Linuxarch.png){: width="972" height="589" .w-50 .left}The kernel, itself, consists of several logical components, like all of the I/O management, memory management, process management. 

The Mac OSX operating system, from Apple, uses a different organization.
•	At the core is the Mach microkernel which implements key primitives like memory management, thread scheduling and inter-process communication mechanisms including what we call RPC.
•	The BSD component provides Unix interoperability via a BSD command line interface, POSIX API support as well as network I/O.
•	All application environments sit above this layer.
•	The bottom two modules are environments for development of drivers, and for kernel modules that can be dynamically loaded into the kernel.

#### Summary
In this lesson, we answered the big question, what is an operating system? And we saw that it's important because it helps abstract and arbitrate the use of the underlying hardware system.
-	We explained that to achieve this, an operating system relies on
-	abstractions, such as processes and threads
-	mechanisms that allow OS to manipulate abstractions
-	policies that specify how abstractions can be modified.
-	We saw that operating systems support a system call interface that allows applications to interact with them.
-	We looked at several alternatives in organizational structures for operating systems.
-	Then very briefly, we looked at some specific examples of operating systems, Windows, Linux, and Mac OS to see some examples of their system call interfaces or their organization.

## P2L1 - Introduction to Processes and Process Management
A process is an instance of an executing program, can be synonymous with task or job

#### Visual Metaphor
A process is like an order of toys
- State of execution (completed toys and toys waitig to be built)
- Parts amd temporary holding area (plastic pieces and containers, etc)
- May require special hardware (sewing machine, glue gun, etc)
A process equivalent:
- State of execution (program counter, stack)
- Parts amd temporary holding area (data, register state occupies state in memory)
- May require special hardware (I/O devices like disks/network devices)

#### What is a Process?
One of the roles of the OS is to manage hardware on behalf of the applications
- Application == program on disk, flash memory, etc (static entity)
- Process == state of a program when executing loaded in memory  (active entity)
An example could be a txt editor with saved lecture notes versus an untitled txt editor open, both are examples of processes

#### What does a process look like? (important)
![Process example](/assets/img/Posts/GIOS/P2L1Processexamp.png){: width="972" height="589" .w-50 .left} 
A process encapsulates all states of a running application. This includes the code, the data, all the variables that the application needs to allocate. Every single aspect of the process state has to be uniquely identified by its address. So an OS abstraction used to encapsulate all of the process state is an address space, which is shown in the image.*The address space is defined by a range of addresses from v0 to some vmax and different types of process state will appear in different regions in this address space.*

What are the different types of state in a process?
- test and data (static state when process first loads)
- heap (dynamically created during execution)
- stack (grows and shrinks during execution in a LIFO way)

#### Process Address Space
![Address example](/assets/img/Posts/GIOS/P2L1address.png){: width="972" height="589" .w-50 .left} The process representation is referred to as an address space. The potential range of addresses from vo to vmax, v in that range will be called a virtual address. They care called virtual because they don't have to correspond to actual locations in the physcial memory. Instead the memory management hardware and operating system components responsible for memory management, like page tables, maintain a mapping between the virtual addresses and the physical addresses. 

#### Process Address Space and Memory Management
Not all processes require the entire address space , on the flip side, you can run into the situation where you do not have enough memory to run your processes. To deal with this the operating system dynamically decides which portion of which address space will be present and where in physical memory. Regions of the physical memory can be occupied by different physical processes. With that said - each process the operating system must maintain some information regarding the process address space
#### Virtual Addresses Quiz

If two processes, P1 and P2, are running at the same time, what are the ranges of their virtual address space that they will have?

•	P1 has address ranges from 0 to 32,000, and P2 from 32,001 until 64,000.
•	Both P1 and P2 have address ranges from 0 to 64,000. (correct)
•	P1 has an address space range from 32,001 to 64,000, and P2 has address ranges from 0 to 32,000. 

#### How does the OS know what a process is doing?
- Program Counter (PC) maintained on the CPU while the process is executing in a register and there are other registers that are maintained on the CPU. They may have information like addresses for data, or they may have some status information that somehow affects the execution of the sequence
- CPU registers
- Stack Pointer, the top of th stack if defined by the stack pointer. Because the stack exhibits LIFO behavior, we need to know whats on top. 
All of this is maintained in the OS using a Process Control Block (PCB)

#### Process Control Block
A Process Control Block (PCB) is a data structure that the operating system maintains for every one of the processes that it manages.
![PCB](/assets/img/Posts/GIOS/P2L1PCB.png){: width="972" height="589" .w-50 .left}
From what we’ve seen so far the PCB must contain:
-	processes state like the program counter, the stack pointer, really all of the CPU registers (their values as they relate to the particular process),
-	various memory mappings that are necessary for the virtual to physical address translation for the process,
-	and other things. Some of the other useful information includes a list of open files for instance, information that’s useful for scheduling like how much time this particular process has executed on the CPU, how much time it should be allocated in the future (this can depend on the process priority), etc.

some aspects of the PCB change when a process state changes. An example being when a process requests more memory the OS will allocate more memory and establish a new valid virtual to physical memory mapping for this process. 
Other fields of the PCB change more frequently. During the execution of a program the program counter changes on every single instruction on a dedicated register. 

#### How is a PCB Used? 
![PCB how is used](/assets/img/Posts/GIOS/P2L1howispcbused.png){: width="972" height="589" .w-50 .left} Running through an example to explain. Assume you have an OS managing two processes, P1 and P2. They are created and their PCBs are stored in memory. If P1 is running on the CPU and P2 is idle, then the OS decides to interrupt P1, then P1 will become idle. At that point the OS will have to save the state information regarding P1, including the CPU registers into the PCB for P1. Then the OS needs to restore the state for P2 so it can execute (updating the CPU registers). When P2 is done or interupted, the PCB will beed to save the P2 information and the P1 PCB will need to be restored. P1 will now be running and the CPU registers will reflect the state of P1. Given that the value of the PCB for P1 corresponds exactly to the values it had when we interrupted P1 earlier. That means that P1 will resume its execution at the exact same point where it was interrupted earlier by the OS.

#### What is a context switch?

![Context Switch text](/assets/img/Posts/GIOS/P2L1ContextSwitch.png){: width="972" height="589" .w-50 .left} A context switch is when we switch the CPU from the context of one process to the context of another process. It requires an update in PCB memory or an update in the CPU cache. formally stated, a context switch is the mechanism used by the operating system to switch the execution from the context of one process to the context of another process. It can be expensive for two reasons, direct costs: the number of cycles that have to be executed to simply load and store all the values of the process control blocks to and from memory, and indriect costs: needing to access the memory when you context switch if you need any information from the previous process (cold cache), where as if you are running P1 and you need to access P1 information, the cache is readily accessible. 

#### Hot Cache Quiz
13 - Hot Cache Quiz
Here's a quick quiz about the processor cache. 
❏	When a cache is hot, it can malfunction, so we must context switch to another process. 
❏	When a cache is hot most process data is in the cache, so the process performance will be at its best. (correct)
❏	When a cache is hot, sometimes we must context switch. (correct)

#### Process Lifecycle 

![Process Lifecycle](/assets/img/Posts/GIOS/P2L1ProcessLifecycle.png){: width="972" height="589" .w-50 .left} All processes can have two states, running or idling. When a process is running, it can be interrupted and context-switched. At this point, the process is idle, but it's in what we call a ready state. It is ready to execute, except it is not the current process that is running from the CPU. At some later point, the scheduler would schedule that process again, and it will start executing on the CPU, so it will move into the running state.
![Process states](/assets/img/Posts/GIOS/P2L1Processstates.png){: width="972" height="589" .w-50 .left} Other states that a process can be in are shown in the img. There is an example to run through the process. 
1. when a process is created, it enters the new state. This is when the OS will perform admission control, and if it's determined that it's okay, the operating system will allocate and initiate a process control block (PCB) and some initial resources for this process. Provided that there are some minimum available resources, the process is admitted, and at that point, it is ready to start executing.
2. It is ready to start executing, but it isn't executing on the CPU. It will have to wait in this ready state until the scheduler is ready to move it into a running state when it schedules it on the CPU.
3. Once the scheduler gives the CPU to a ready process, that ready process is in the running state. And from here, a number of things can happen. First, the running process can be interrupted so that a context switch is performed. This would move the running process back into the ready state. Another possibility is that a running process may need to initiate some longer operation, like reading data from disk or to wait on some event like a timer or input from a keyboard. 
3. *At that point, the process enters a waiting state. When the event occurs or the I/O operation completes, the process will become ready again.*
4. Finally, when a running process finishes all operations in the program or when it encounters some kinds of error, it will exit. It will return the appropriate exit code, either success or error, and at that point, the process is terminated.

#### Process State Quiz
The CPU is able to execute a process when the process is in which of the following states?
- Running (correct)
- Ready (correct)
- New
- Waiting

#### Process Creation
In OS, a process can create child processes. All processes come from a single root and they have some relationship to one another through parent child behavior. Priviledged processes are root processes, most of the OS processes are root processes, so the OS will be loaded onto the machine after it boots up and it will create a number of initial processes. 
![Process creation](/assets/img/Posts/GIOS/P2L1Processcreate.png){: width="972" height="589" .w-50 .left} When a user logs into a system a user shell process is created and then when the user types in command like ls, or emacs, then new processes get spawned from that shell parent process. So the final relationship looks like this tree. Most operating systems support two basic mechanisms for process creation, fork and exec. A process can create a child via either one these mechanisms.
- With the fork() mechanism the OS will create a new PCB for the child and copy the exact same values from parent PCB into the child PCB.
- Exec() behaves differently. It will take a PCB structure created via fork, but it will not leave its values to match the parent’s values. 

#### Parent Process Quiz
On UNIX-based systems, init is the first process that starts after the system boots. And because all other processes can ultimately be traced to init, it's referred to as the parent of all processes.
On the Android OS, Zygote is a daemon process which has the single purpose of launching app processes. 

#### Role of the CPU Scheduler
![CPU Scheduler](/assets/img/Posts/GIOS/P2L1CPUscheduler.png){: width="972" height="589" .w-50 .left} . For the CPU to start executing a process the process must be ready first. The problem is, however, there will be multiple ready processes waiting in the ready queue. How do we pick what is the right process that should be given the CPU next, that should be scheduled on the CPU? So the question is which process do we run next? This is determined by a component call the CPU scheduler.
- The CPU Scheduler is an OS component that manages how processes use the CPU resources. It decides which one of the currently ready processes will be dispatched to the CPU to start running and how long it should run for, and it also determines how long this process should be allowed to run for.

Over time this means that in order to manage the CPU:
- The OS must be able to preempt, to interrupt the executing process and save its current context. This operation is called preemption.
- Then the OS must run the scheduling algorithm to choose one of the ready processes that should be run next.
- Once the processes are chosen, the OS must dispatch this process onto the CPU and switch into its context so that that process can finally start executing.

Given that the CPU resources are precious, the OS needs to make sure that CPU time is spent running processes and not executing scheduling algorithms and other OS operations. So OS should minimize the amount of time that it takes to perform these tasks (preemption, scheduling, and dispatching), i.e., the OS must be efficient. 

#### Length of process
How often do we run the scheduler? The more frequently we run it the more CPU time is wasted on running the scheduler vs running application processes. So another way to ask the same question is: How long should a process run? The longer we run a process the less frequently we are invoking the scheduler to execute.
- Useful CPU work == Total processing time/Total time = (2 * Tp)/(2 * Tp+2*t_scheduled) if Tp == t_scheduled => only 50% of CPU time spent of useful work!
- If Tp (processing time) is much larger than scheduling time, t_scheduled then ~ 91% of the CPU time is spent doing useful work, we want a big Tp. 
- timeslice == time Tp allocated to a process on the CPU. 
![timeslice](/assets/img/Posts/GIOS/P2L1timeslice.png){: width="972" height="589" .w-50 .left} As you can see there are a lot of design decisions and tradeoffs that we must make when we’re considering how to design a scheduler. Some of these include deciding what are appropriate timeslice values, or deciding what would be good metrics that are useful when the scheduler is choosing what’s the next process it should run.

#### What about I/O?
So far we know the OS manages how processes access resource on the hardware platform and this in addition to the CPU and memory will include I/O devices, peripherals like keyboards, network cards, disks, etc.
1. imagine a process had made an I/O request, the OS delivered that request, for instance, it was a read request to disk.
2. then the process is placed on the I/O queue that’s associated with that particular disk device. So the process is now waiting in the I/O queue.
3. the process will remain waiting in the queue until the device completes the operations, so the I/O event is complete, and responds to that particular request.
4. once the I/O request is met, the process is ready to run again and depending on the current load in the system it may be placed in the ready queue, or it may be scheduled on the CPU if there is nothing else waiting in the ready queue before it.
![I/O](/assets/img/Posts/GIOS/P2L1IO.png){: width="972" height="589" .w-50 .left} 
So to summarize, a process can make its way into the ready queue in a number of ways. 
-	A process which was waiting on an I/O event ultimately found its way into the ready queue.
-	A process which was running on the CPU but its timeslice expired goes back on the ready queue.
-	When a new process is created via the fork call it ultimately ends its way on the ready queue.
-	For a process which was waiting for interrupt, once the interrupt occurs it will also be placed on the ready queue.

#### Scheduler Responsibility Quiz
The question is, which of the following are NOT a responsibility of the CPU scheduler? The options are:
❏	maintaining the I/O queue (correct)
❏	maintaining the ready queue
❏	deciding when to context switch
❏	or deciding when to generate an event that a process is waiting on. (correct)

#### Can Processes Interact 
Another natural question can be, can processes interact? And the simple answer is YES. An operating system must provide mechanisms to allow processes to interact with one another.
![Process Interaction](/assets/img/Posts/GIOS/P2L1processinteract.png){: width="972" height="589" .w-50 .left} Remember that the operating systems go through a great deal to protect and isolate processes from one another. Each of them is a separate address space. They control the amount of CPU each process gets, which memory is allocated, and accessible to each process. So these communication mechanisms that we will talk about somehow have to be built around those protection mechanisms. These kinds of mechanisms are called inter-process communication mechanisms, or we refer to them as IPC (interprocess communication)

The IPC mechanisms:
-	Help transfer data and information from one address space to another
-	Maintain the protection and isolation that operating systems are trying to enforce
-	IPC mechanisms need to provide flexibility as well as clearly performance, because different types of interactions between processes may exhibit different properties. Periodic data exchanges, continuous stream of data flowing between the processes, or coordinated applet, to some shared single piece of information.

The Message Passing IPC:
One mechanism that operating systems support is message passing IPC. The operating system establishes a communication channel, like a shared buffer, and the processes interact with it by writing or sending a message into that buffer. Or, reading or receiving a message from that shared communication channel. 
![Process Interaction Message](/assets/img/Posts/GIOS/P2L1messagepass.png){: width="972" height="589" .w-50 .left} The benefit of this approach is that it's the operating system who will manage this channel, and it's the operating system that provides the exact same APIs, the exact same system calls for writing or sending data, and the reading or receiving data from this communication channel. The downside is the overhead. For every single piece of information that we want to pass between these two processes, we have to copy from the user space of the first process into this channel that's sitting in the OS, in the kernel memory. And then back into the address space of the second process.

The shared memory IPC:
The other type of IPC mechanism is what we call shared memory IPC. The way this works is the operating system establishes the shared memory channel, and then it maps it into the address space of both processes. The processes are then allowed to directly read and write from this memory, as if they would to any memory location that's part of their virtual address space. So the operating system is completely out of the way in this case.
![shared memory](/assets/img/Posts/GIOS/P2L1sharedmemory.png){: width="972" height="589" .w-50 .left} That in fact is the main advantage of this type of IPC. That the operating system is not in the path of the communication. So the processes, while they're communicating are not going to incur any kind of overheads from the operating system. The disadvantage of this approach is because the operating system is out of the way it no longer supports fixed and well defined APIs how this particular shared memory region is used. For that reason, its usage sometimes becomes more error prone, or developers simply have to re-implement code to use this shared memory region in a correct way.

#### Lesson Summary
We covered process and process management:
- Process and process-related abstractions including address space and PCB
- Basic mechanisms for managing process resources (context, switching, process creation, scheduling, interprocess communication)


## P2L2 - Threads and Concurrency 

### 1. Visual Metaphor

A thread is like a worker in a toyshop

- worker in a toyshop is a:
1. active entity
2. executing unit of toy order
3. works simultaneously with others
4. many workers completing toy orders
5. requires coordination (sharing of tools, parts, workstations, etc)

- a thread:
1. is an active entity (executing unit of a process)
2. works simultaneously with others (many threads executing)
3. requires coordination (sharing of I/O devices, CPUs, memory)

### 2. Process versus thread
![process v thread](/assets/img/Posts/GIOS/P2L2/img1.png){: width="972" height="589" .w-50 .left}


A single threaded process is represented by its address space (i.e. code/data/files). The address space will contain all of the virtual to physical address mappings for the process, for its code, its data, its heap section files, for everything. The process is also represented by its execution context (i.e. registers/stack) that contains information about the values of the registers, the stack pointer, program counter, etc. The operating system represents all this information in a process control block (PCB = code/data/files + regs/stack).

Threads represent multiple independent execution contexts. They're part of the same virtual address space, which means that they will share all of the virtual to physical address mappings. They will share all the code, data, files. However, they will potentially execute different instructions, access different portions of that address space, operate on different portions of the input, and differ in other ways. This means that each thread will need to have a different program counter, stack pointer, stack, thread-specific registers. So for each and every thread, we have to have separate data structures to represent this per-thread information.

extra context - https://stackoverflow.com/questions/9501526/what-is-the-difference-b-w-tcbthread-control-block-pcbprocess

### 3. Why are threads useful?
- By parallelizing the program in this manner, we achieve speed up. We can process the input much faster than if only a single thread on a single CPU had to process the entire input.
- threads may execute completely different portions of the program. For instance, you may designate certain threads for certain I/O tasks like input processing, or display rendering
- one benefit from specialization is that we end up executing with a hotter cache. And that translates to gains in performance.

Why not just write a multi-process application where every single processor runs a separate process?
- If we do that, since the processes do not share an address space (AS) we have to allocate for every single one of these contexts AS and execution context (EC). So the memory requirements, if this were a multiprocessor implementation, would be that we have to have four Address Space allocations and four Execution Context allocations. 
- synchronization among processes, requires IPC mechanisms that are costlier. 
- A multithreaded implementation results in threads sharing an AS so we don’t need to allocate memory for all of the AS information for these remaining ECs. This implies that a multithreaded application is more memory efficient, it has lower memory requirements than its multi-process alternative
### 4. Are threads useful on a single CPU or when # of threads > # of CPU?

![process v thread](/assets/img/Posts/GIOS/P2L2/img3.png){: width="972" height="589" .w-50 .left}

- Threads save time in context switching compared to processes because they share and AS, so they don't need to create new virtual to physical address mappings for new process scheduling. 

### 5. Benefits to applications and OS code
By multithreading the OS’s kernel we allow the OS to support multiple EC, and this is particularly useful when there are multiple CPUs, so that the OS context can execute concurrently on different CPUs in a multiprocessor/multicore platform.
![process v thread](/assets/img/Posts/GIOS/P2L2/img4.png){: width="972" height="589" .w-50 .left}

#### Process vs Threads Quiz
•	[Threads] The first statement applies to threads. Each thread belonging to a process shares the virtual address space with other threads in that process.
•	[Processes] Because threads share the address space, the context switch among them happens faster than processes. So, processes take longer to context switch.
•	[Both]Both threads and processes have their execution context described with stack and registers.
•	[Threads]Because threads share the virtual address space, it is more likely that when multiple threads execute concurrently, the data that's needed by one thread is already in the cache, brought in by another thread. So, they typically result in hotter caches. Among processes, such sharing is really not possible.
•	[Both]Then the last answer is B. We already saw that for processes, it makes sense for the operating system to support certain inter-process communication mechanisms. And we'll see that there are mechanisms for threads to communicate and coordinate and synchronize amongst each other.

### 6. Thread Mechanisms 
 - Thread data structure (idenitfy threads, keep track of resource usage
 - mechanisms to create and manage threads
 - mechanisms to safely coordinate among threads running concurrently in the same address space

Issues with concurrent execution

threads share the same virtual to physical address mappings
![process v thread](/assets/img/Posts/GIOS/P2L2/img5.png){: width="972" height="589" .w-50 .left}

To deal with such concurrence, we need a mechanism for threads to execute exclusively, which is called mutual exclusion. Mutual exclusion is a mechanism where only one thread at a time is allowed to perform an operation. The remaining threads, if they want to perform the same operation, must wait their turn. The actual operation that must be performed in mutual exclusion may include some update to state, or access to some data structure that's shared among all these threads.

threads share the same virtual to physical address mappings
![process v thread](/assets/img/Posts/GIOS/P2L2/img6.png){: width="972" height="589" .w-50 .left}

### Threads and Thread Creation
First, we need some data structure to represent a thread. The thread type proposed by Birrell is a data structure that contains all the information that is specific to a thread and can describe a thread. This includes
-	the thread identifier that the threading system will use to identify a specific thread,
-	register values, in particular the stack of a thread,
-	the program counter and the stack pointer,
-	and any other thread specific data or attributes. 

Second - For thread creation, Birrell proposes a fork call with two parameters,
•	a proc argument, which is the procedure that the created thread will start executing,
•	and args, which are the arguments for this procedure.
*Not to be confused with UNIX fork!
![process v thread](/assets/img/Posts/GIOS/P2L2/img7.png)
When a thread T0 calls a fork a new thread T1 is created. That means that new thread data structure of this type is created and its fields are initialized such that its program counter will point to first instruction of the procedure proc, and these arguments will be available on this stack of the thread. After the fork operation completes, the process as a whole has two threads. T0, the parent thread, and T1. These can both execute concurrently. T0 will execute the next operation after the fork call, and T1 will start executing with the first instruction in proc, with the specified arguments.

Next -  we need some mechanism to determine
•	if a thread is done,
•	and if it is necessary to retrieve its result,
•	or at least to determine the status of the computation (success or error).
To deal with this issue, Birrell proposes a mechanism he calls join. It has the following semantic: child_result = join(id of child thread).
When a parent thread calls join, it will be blocked until the child thread completes. Join will return to the parent the result of the child’s computation. At that point, the child thread exits the system; any allocated data structure state for the child, all of the resources that were allocated for its execution will be freed and the child thread is terminated.

### Thread Creation Example
![process v thread](/assets/img/Posts/GIOS/P2L2/img8.png)
### Mutexes
![process v thread](/assets/img/Posts/GIOS/P2L2/img9.png)
### Mutual Exclusion
There is a danger in the previous example that the parent and the child thread will try to update the shared list at the same time potentially overwriting the list elements.
To do this the OS and threading libraries in general support a construct called mutex. 
Other threads attempting to lock the same mutex are not going to be successful. These threads will be blocked from performing the lock operation. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img10.png) The portion of the code protected by the mutex, is called critical section. In Birrell’s paper, this is any code within the curly brackets of the lock operating that he proposes to be used with mutexes. The critical section code should correspond to any kind of operation that requires that only one thread at a time to perform the operation. For instance, it can be updated to shared variable like the list, or increasing/decreasing a counter, or performing any type of operation that requires mutual execution between the threads. Other than the critical section code, the rest of the code in the program, the threads may execute them concurrently.

### Making safe_insert safe
![process v thread](/assets/img/Posts/GIOS/P2L2/img11.png)

### Mutex Quiz
![process v thread](/assets/img/Posts/GIOS/P2L2/img12.png)

### Producer/Consumer Example
For threads the first construct that Birrell advocates is mutual exclusion, and that’s a binary operation. A resource is either free and you can access it, or it is locked and you have to wait. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img13.png)
Operating this way is clearly wasteful and it would be much more efficient if we could just tell the consumer when the list is full so that it can at that point go ahead and process the list.

### Condition Variable
Birrell recognizes this common situation in multithreaded environments and argues for a new construct, a condition variable. He says that such condition variables could be used in conjunction with mutexes to control the behavior of concurrent threads. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img14.png)

### Condition Variable API
![process v thread](/assets/img/Posts/GIOS/P2L2/img15.png)

#### Condition Variable Quiz 
![process v thread](/assets/img/Posts/GIOS/P2L2/img16.png)

### Reader/Writer Problem
# Useful Resources 
## Quizlet
- https://quizlet.com/user/quizlette8384429/folders/gios?tag=Midterm


## Textbooks and other reference materials:

- https://pages.cs.wisc.edu/~remzi/OSTEP/ Operating Systems: Three Easy Pieces

- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://broman.dev/download/The%20Linux%20Programming%20Interface.pdf The Linux Programming Interface

- https://beej.us/guide/ 
Specifically - an introduction to socket programming:  
- https://beej.us/guide/bgnet0/

- https://www.kernel.org/doc/man-pages/ The Linux man-pages project


## C References:

- https://ocw.mit.edu/courses/6-087-practical-programming-in-c-january-iap-2010/pages/lecture-notes/

- https://learnxinyminutes.com/c/

- https://www.linkedin.com/learning/career-journey?initPlanV2=true&u=2163426

- http://goshdarnfunctionpointers.com/

- http://www.cprogramming.com/tutorial/function-pointers.html


Presentation on programming with C from Chris D. (with some updates from TAs)

Youtube of the Q&A C Video from Chris. D.

Harvard CS50 course on C and their reference site.

C/Standard Library docs

C, Pointers, and other lessons on C

Thread programming:

Posix Thread Programming tutorial

## Network programming resources:

https://beej.us/guide/bgnet/html/
https://www.codeproject.com/Articles/586000/Networking-and-Socket-programming-tutorial-in-C
http://www.binarytides.com/socket-programming-c-linux-tutorial/

## Debugging:


## some useful cheatsheets: 

Vim Cheat Sheet
TMUX Cheat Sheet
GIT Cheat Sheet
Linux source cross-references:

Linux Kernel Map
Linux source cross-reference
reference_material

# Past course resources
- https://github.gatech.edu/rdiaconescu3/6200_GIOS_flashcards/blob/main/GIOS_flashcards.apkg
- https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL3UvcyFBbXBpTXN0VG5CRWpzUUtpTk8yRzdiR21PLWJyP2U9Y2R1SXRJ&cid=23119C53CB32626A&id=23119C53CB32626A%216274&parId=root&o=OneUp
- https://www.omscs-notes.com/operating-systems/midterm-exam-review-questions/
