---
title: CS 6200 - Graduate Introduction to Operating Systems
date: 2025-08-18 13:19:00 +0800
categories: [GaTech OMSCS, Operating Systems]
tags: [c, c++, powershell, linux, wsl]     # TAG names should always be lowercase
description: Overview & Projects from GIOS
---


# Welcome
#### Note
For all OMSCS courses - I will provide pseudocode and video links, but I need to keep code in a private repository to uphold the honor code. Descriptions, notes, etc are below. 

# Projects

## Setting up my test environment 
The first thing you do in GIOS is set up your test environment. There are a million different ways to do this, I settled on VS code and docker following the tutorial below: 
- [Set Up for Docker with VSCode and WSL2](https://docs.google.com/document/d/1-TLnnWRfcqFjuzlaAvsxXppVnlklKBb9FBv_stK86tg/edit?tab=t.0)


### VSCode and Docker 
GIOS repo: 
- https://github.gatech.edu/gios-fall-25/environment

#### Step 1
- Install Linux on Windows with WSL (for GIOSFA25 get UBUNTU 20.04)
- Install Docker Desktop and add GIOS image
- Resource: https://www.docker.com/ && 
https://learn.microsoft.com/en-us/windows/wsl/install
```powershell
wsl.exe --install Ubuntu-20.04
```
Notes:
1. Make sure you set your linux default WSL to the 20.04 ubuntu for GIOS (see Microsoft documentation)
2. Make sure you are on WSL 2 not 1 to avoid errors

#### Step 2
- Open WSL and get vscode on wsl
```bash
wsl
code .
```
Notes:
1. in VSCode get the Docker, Remote Development, and C/C++ extensions

#### Step 3
- Download all the absolutely necessary code provided in the GIOS repo in vscode (since it should have opened after it downloaded) or linux terminal 
Including: Add the course PPA, update the repos, install requirements (gcc should work after you've done this)

#### Step 4
- Add the tutorial's configure.yml file to a home folder you plan on using for the projects 
```yaml
name: cs6200-gios            # see Ref 1 below
services:
  cs6200-gios:               # see Ref 1 below
    image: gtomscs6200/fall25-environment:latest # note: set appropriately to current semester
    container_name: dev-env  # see Ref 1 below
    tty: true                # see Ref 2 below
    working_dir: /home/files
    restart: unless-stopped
    volumes:
      - .:/home/files:rw     # see Ref 3 below
```

- Ref 1: These names/labels can be set according to your preference

- Ref 2: https://stackoverflow.com/a/42597165

- Ref 3: general form is `<host-path>:<container-path>:rw`, where `.` here denotes the host-system 
- location in which `docker compose up -d` command is run on file `compose.yaml`, with
- corresponding bind mounting to container location `/home/files` as designated above. Note that you
- may designate `<container-path>` per your own preference. Suffix `:rw` denotes read & write modes.
- See https://docs.docker.com/compose/compose-file/ and https://docs.docker.com/compose/compose-file/compose-file-v3/#volumes for more details.

##### [Reference & Source for Step 4](https://docs.google.com/document/d/1-TLnnWRfcqFjuzlaAvsxXppVnlklKBb9FBv_stK86tg/edit?tab=t.0)
#### Step 5
Open the Dev Containers extension in VS Code. You should see cs6200-gios, and inside it, dev-env.
   

- If the dev environment is not running, right-click on dev-env and select Start.

- Once it’s running, right-click on dev-env again and select Attach Visual Studio Code to open the Docker container in VS Code.

You’ll know you’re inside the container if the remote connection status in the bottom left corner shows something like Container: gtomscs6200/fa25-environment.

#### Step 6
Open up a new terminal in your container in VSCode and run inotify
Resources:
- [Man Page](https://www.man7.org/linux/man-pages/man7/inotify.7.html)
- [inotify code](https://drive.google.com/file/d/1dMIPMESO3iK41j5fvkbTF4JVBU7XS0ej/view)

Make a new c file called inotify (or whatever you want), copy in the inotify code linked above and run the following

```bash
make inotify

./inotify . 
```
Output should look something like
```bash
root@54c5497edb9c:/home/files# ./inotify .
Press ENTER key to terminate.
Listening for events.
```
#### Step 7
Open up the split screen terminal windows and test out inotify
```bash
//on right side of the split terminal run the following tests:

ll //not one-one

cat inotify.c 
```
Output should look like this for >11
- LEFT
```bash
root@54c5497edb9c:/home/files# ./inotify .
Press ENTER key to terminate.
Listening for events.
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
```
- RIGHT
```bash
root@54c5497edb9c:/home/files# ll
total 56
drwxr-xr-x 6 1000 1000  4096 Aug 22 16:07 ./
drwxr-xr-x 1 root root  4096 Aug 22 15:25 ../
-rw-r--r-- 1 1000 1000   318 Aug 22 15:25 compose.yaml
-rwxr-xr-x 1 root root 17400 Aug 22 15:45 inotify*
-rw-r--r-- 1 root root  4677 Aug 22 15:44 inotify.c
drwxr-xr-x 3 1000 1000  4096 Aug 22 16:14 munit/
drwxr-xr-x 2 1000 1000  4096 Aug 22 15:21 p1/
drwxr-xr-x 2 1000 1000  4096 Aug 22 15:21 p3/
drwxr-xr-x 2 1000 1000  4096 Aug 22 15:21 p4/
```
AND this for >cat inotify.c
- LEFT
```bash
root@54c5497edb9c:/home/files# ./inotify .
Press ENTER key to terminate.
Listening for events.
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./ [directory]
IN_CLOSE_NOWRITE: ./ [directory]
IN_OPEN: ./inotify.c [file]
IN_CLOSE_NOWRITE: ./inotify.c [file]
```
- RIGHT

```bash
root@54c5497edb9c:/home/files# cat inotify.c
#include <errno.h>
#include <poll.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/inotify.h>
#include <unistd.h>
#include <string.h>

/* Read all available inotify events from the file descriptor 'fd'.
  wd is the table of watch descriptors for the directories in argv.
  argc is the length of wd and argv.
  argv is the list of watched directories.
  Entry 0 of wd and argv is unused. */

static void
handle_events(int fd, int *wd, int argc, char* argv[])
{
    /* Some systems cannot read integer variables if they are not
      properly aligned. On other systems, incorrect alignment may
      decrease performance. Hence, the buffer used for reading from
      the inotify file descriptor should have the same alignment as
      struct inotify_event. */

    char buf[4096]
        __attribute__ ((aligned(__alignof__(struct inotify_event))));
    const struct inotify_event *event;
    ssize_t len;

    /* Loop while events can be read from inotify file descriptor. */

    for (;;) {

        /* Read some events. */

        len = read(fd, buf, sizeof(buf));
        if (len == -1 && errno != EAGAIN) {
            perror("read");
            exit(EXIT_FAILURE);
        }

        /* If the nonblocking read() found no events to read, then
          it returns -1 with errno set to EAGAIN. In that case,
          we exit the loop. */

        if (len <= 0)
            break;

        /* Loop over all events in the buffer. */

        for (char *ptr = buf; ptr < buf + len;
                ptr += sizeof(struct inotify_event) + event->len) {

            event = (const struct inotify_event *) ptr;

            /* Print event type. */

            if (event->mask & IN_OPEN)
                printf("IN_OPEN: ");
            if (event->mask & IN_CLOSE_NOWRITE)
                printf("IN_CLOSE_NOWRITE: ");
            if (event->mask & IN_CLOSE_WRITE)
                printf("IN_CLOSE_WRITE: ");

            /* Print the name of the watched directory. */

            for (int i = 1; i < argc; ++i) {
                if (wd[i] == event->wd) {
                    printf("%s/", argv[i]);
                    break;
                }
            }

            /* Print the name of the file. */

            if (event->len)
                printf("%s", event->name);

            /* Print type of filesystem object. */

            if (event->mask & IN_ISDIR)
                printf(" [directory]\n");
            else
                printf(" [file]\n");
        }
    }
}

int
main(int argc, char* argv[])
{
    char buf;
    int fd, i, poll_num;
    int *wd;
    nfds_t nfds;
    struct pollfd fds[2];

    if (argc < 2) {
        printf("Usage: %s PATH [PATH ...]\n", argv[0]);
        exit(EXIT_FAILURE);
    }

    printf("Press ENTER key to terminate.\n");

    /* Create the file descriptor for accessing the inotify API. */

    fd = inotify_init1(IN_NONBLOCK);
    if (fd == -1) {
        perror("inotify_init1");
        exit(EXIT_FAILURE);
    }

    /* Allocate memory for watch descriptors. */

    wd = calloc(argc, sizeof(int));
    if (wd == NULL) {
        perror("calloc");
        exit(EXIT_FAILURE);
    }

    /* Mark directories for events
      - file was opened
      - file was closed */

    for (i = 1; i < argc; i++) {
        wd[i] = inotify_add_watch(fd, argv[i],
                                  IN_OPEN | IN_CLOSE);
        if (wd[i] == -1) {
            fprintf(stderr, "Cannot watch '%s': %s\n",
                    argv[i], strerror(errno));
            exit(EXIT_FAILURE);
        }
    }

    /* Prepare for polling. */

    nfds = 2;

    fds[0].fd = STDIN_FILENO;       /* Console input */
    fds[0].events = POLLIN;

    fds[1].fd = fd;                 /* Inotify input */
    fds[1].events = POLLIN;

    /* Wait for events and/or terminal input. */

    printf("Listening for events.\n");
    while (1) {
        poll_num = poll(fds, nfds, -1);
        if (poll_num == -1) {
            if (errno == EINTR)
                continue;
            perror("poll");
            exit(EXIT_FAILURE);
        }

        if (poll_num > 0) {

            if (fds[0].revents & POLLIN) {

                /* Console input is available. Empty stdin and quit. */

                while (read(STDIN_FILENO, &buf, 1) > 0 && buf != '\n')
                    continue;
                break;
            }

            if (fds[1].revents & POLLIN) {

                /* Inotify events are available. */

                handle_events(fd, wd, argc, argv);
            }
        }
    }

    printf("Listening for events stopped.\n");

    /* Close inotify file descriptor. */

    close(fd);

    free(wd);
    exit(EXIT_SUCCESS);
}
root@54c5497edb9c:/home/files# 
```

#### Other Unit Tests
```
git clone https://github.com/nemequ/munit.git

gcc -Wall example.c munit.c munit.h -o example

./example
```
Output should look like 
```bash
Running test suite with seed 0x5a13d21b...
/example/compare                     [ OK    ] [ 0.00006970 / 0.00000230 CPU ]
/example/rand                        [ OK    ] [ 0.00000620 / 0.00000560 CPU ]
/example/parameters                  
  foo=one, bar=red                   [ OK    ] [ 0.00000610 / 0.00000540 CPU ]
  foo=one, bar=green                 [ OK    ] [ 0.00000610 / 0.00000540 CPU ]
  foo=one, bar=blue                  [ OK    ] [ 0.00000600 / 0.00000540 CPU ]
  foo=two, bar=red                   [ OK    ] [ 0.00000600 / 0.00000550 CPU ]
  foo=two, bar=green                 [ OK    ] [ 0.00000600 / 0.00000540 CPU ]
  foo=two, bar=blue                  [ OK    ] [ 0.00000870 / 0.00000750 CPU ]
  foo=three, bar=red                 [ OK    ] [ 0.00000620 / 0.00000570 CPU ]
  foo=three, bar=green               [ OK    ] [ 0.00000640 / 0.00000570 CPU ]
  foo=three, bar=blue                [ OK    ] [ 0.00000600 / 0.00000540 CPU ]
11 of 11 (100%) tests successful, 0 (0%) test skipped.
```
#### Notes for me:
- project is held on ubuntu 20.04 callled 'gios'
```bash
gios@DESKTOP-QUQS18F://home/gios/projects$
```
## Overview - Sequence of Projects 
1. Threads
3. Concurrency
4. Synchronization

Projects will cover single-node OS mechanisms (inter-process communication, scheduling, etc) and multi-node OS mechanisms (remote procedure calls (RPC), etc) and experimental design and evaluation 
All programs are in C and Linux
## Project 1

### Warm Up 

### gfclient:
#### libcurl breakdown
The libcurl “easy” workflow

Using libcurl, the steps look like this:

1. Global init (optional)

```c
curl_global_init(CURL_GLOBAL_DEFAULT);
```


Sets up any shared resources. In project this maps to gfc_global_init().

2. Create handle

```c
CURL *curl = curl_easy_init();
```


Allocates a request object. In project: *gfc_create()*.

3. Set options on the handle
```c
curl_easy_setopt(curl, CURLOPT_URL, "http://example.com");
curl_easy_setopt(curl, CURLOPT_WRITEFUNCTION, my_write_func);
curl_easy_setopt(curl, CURLOPT_WRITEDATA, my_data);
```

Each call changes a field inside the handle.
In your project: gfc_set_server(), gfc_set_path(), gfc_set_writefunc(), gfc_set_writearg(), etc.

4. Perform the transfer
```c
res = curl_easy_perform(curl);
```

Blocking call — does DNS lookup, connects, sends request, streams data, invokes callbacks.
In project: gfc_perform().

5. Check result & get information
```c
curl_easy_getinfo(curl, CURLINFO_RESPONSE_CODE, &code);
```

In project: gfc_get_status(), gfc_get_filelen(), gfc_get_bytesreceived().

6. Cleanup handle
```c
curl_easy_cleanup(curl);
```

Frees memory for the request. In project: gfc_cleanup().

7. Global cleanup (optional)
```c
curl_global_cleanup();
```

Tears down global state. In project: gfc_global_cleanup().

### Mapping libcurl's API to GETFILE protocol

| libcurl easy function          | gfclient equivalent                                                                                         |
| ------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| `curl_global_init()`           | `gfc_global_init()`                                                                                         |
| `curl_easy_init()`             | `gfc_create()`                                                                                              |
| `curl_easy_setopt(handle, …)`  | `gfc_set_server()`, `gfc_set_port()`, `gfc_set_path()`, `gfc_set_headerfunc()`, `gfc_set_writefunc()`, etc. |
| `curl_easy_perform(handle)`    | `gfc_perform()`                                                                                             |
| `curl_easy_getinfo(handle, …)` | `gfc_get_status()`, `gfc_get_filelen()`, `gfc_get_bytesreceived()`                                          |
| `curl_easy_cleanup(handle)`    | `gfc_cleanup()`                                                                                             |
| `curl_global_cleanup()`        | `gfc_global_cleanup()`                                                                                      |

## Project 3

## Project 4

# Lecture Notes

## P1L1 - Introduction to Operating Systems 
The course covers:
- what are operating systems
- why are operating systems needed
- how are operating systems designed and implemented? 

Course Topics include OS abstractions, mechanisms, and policies for
- processes and process management
- threads and concurrency
- resource management: scheduling, memory management
- OS services for communication and I/O
- OS support for distributed services
## P1L2 - What is a Operating System?

In simplest terms, it is a piece of software that abstracts and arbitrates the hardware of a system. 

#### 2. An operating system is like a toy shop manager:

Toy Shop Manager
- Directs operational Resources by controlling use of employee time, parts, tools
- Enforces working policies - fairness safety, clean up
- Mitigates difficulty of complex tasks - simplifies operation and optimizes performance

Operating systems
- Directs operational Resources - controls use of CPU, memory, peripheral devices
- Enforces working policies - fair resouce access, limits to resource usage
- Mitigates difficulty of complex tasks - abstract hardware details (system calls)

Operating System Definition

![OS Definition](/assets/img/Posts/GIOS/P1L2whatisanOS.png){: width="972" height="589" .w-50 .left}

An operating system is a layer of systems software that:
- directly has priviledged access to the underlying hardware;
- hides the hardware complexity;
- manages hardware on behalf of one or more applications according to some predefined policies
- In addition, it ensures that applications are isolated and protected from one another 

Important definitions: abstraction versus arbitration, scheduler, device driver, file system, memory management, OS Design principals 

#### 8. Operating systems Examples
Desktop (microsoft Windows, UNIX-based OS (including mac OS X (BSD), Linux)) and Embedded (Android, iOS, Symbian) are the focus of the class 

#### 9. OS Elements
Abstractions (nouns):
- process, thread, file, socket, memory page

Mechanisms (verbs):
- create, schedule, open, write, allocate

Policies:
- least recently used (LRU), earliest deadline first (EDF)

#### 10. Design Prinipals 
- Separation of mechanism & policy: implement flexible mechanisms to support many policies (e.g. LRU, LFU, random)
- Optimize for the common case: where will the OS be used? What will the user want to execute on that machine? What are the workload requirements? 

#### 11. User/Kernel Protection Boundary 
![User Kernel](/assets/img/Posts/GIOS/P1L2Userkernelbundary.png){: width="972" height="589" .w-50 .left}
The user/kernel protection boundary exists to keep users outside of the kernel level. There is a user-kernel switch that is supported by hardware that include trap instructions (unauthorized kernel access), system call (open (file), send (socket), malloc (memory)), and signals 

#### 12. System Calls Flowchart

![System Call Flowchart](/assets/img/Posts/GIOS/P1L2 Systemcallflowchart.png){: width="972" height="589" .w-50 .left}

To make a system call, an application must
•	write arguments,
•	save all relevant data at a well-defined location (The well-defined location is necessary so that the operating system kernel, based on the system call number, can determine which, how many arguments it should retrieve and where they are)
•	make the actual system call using this specific system call number. 
The arguments can either be passed directly between the user program and the operating system, or they can be passed indirectly by specifying their address.

#### 13. Crossing the User/Kernel Protection Boundary 
User/kernel transitions are:
- hardware supported (e.g. traps on illegal instructions or memory accesses requiring special priviledge)
- involves a number of instructions (e.g. ~50-100ns on a 2 GHz machine running linux
- switches locality (affects hardware cache)
- NOT CHEAP!
#### 14. Basic OS Services

Scheduler - responsible for controlling access to the CPU

Memory Manager - responsible for allocating the underlying physical memory to one or more co-running applications. It also maskes sure that applications don't overwrite eachothers memory. 

Block device driver - responsible for access to a block device like a disk

File system - part of the higher level services that are not necessarily related to the hardware but are one of the higher level abstractions supported by the operating system. 

![Basic OS](/assets/img/Posts/GIOS/P1L2BasicOSServices.png){: width="972" height="589" }

#### 14. Monolithic OS 
![Mono OS](/assets/img/Posts/GIOS/P1L2Mono.png){: width="972" height="589" .w-50 .left}
Historically, operating systems were monolithic. This could include file systems that were specialized in sequential workloads, or file systems optimized for accessing databases. 
- Pros: The benefit of this approach is that everything is included in the operating system. The abstractions, all the services, and everything is packaged at the same time. And because of that, there's some possibilities for some compile-time optimizations.
- Cons: The downside is that there is too much state, too much code that's hard to maintain, debug, upgrade. And then its large size also poses large memory requirements, and that can always impact the performance that the applications are able to observe.

#### 15. Modular OS (more modern)
![Modular OS](/assets/img/Posts/GIOS/P1L2Modular.png){: width="972" height="589" .w-50 .left}
The modular approach is the more modern/common approach. It is used with the Linux OS. Everything can be added as a module. With this approach, you can customize which particular file system or scheduler the operating system uses. 
You can also dynamically install new modules in the operating system. 
- Pros: easier to maintain/upgrade, less resource intensive.
- Cons: Can reduce potential optimizations because of the level of indirection required because you have to go through interface specification before you can go to the implementation of a service. 

#### 16. Microkernel (embedded systems)
![Modular OS](/assets/img/Posts/GIOS/P1L2Microkernel.png){: width="972" height="589" .w-50 .left} Require only the basic primitives at the operating system level. For instance, at the OS level, the microkernel can support some basic services such as representing an executing application, its address space, and threads. 
- Pros: smol, easy to verify and test code. This is particularly important for operating systems that need to behave properly. 
- Cons: portability is questionable. Because there are so many one off instances of embedded systems, it can become difficult to find common components which leads to complexity. Also there is the potential for frequent user/kernel crossings which is costly. 

#### 17. Linux Architecture + Mac OS
Below is what the Linux environment looks like.
- 	Starting at the bottom, we have the hardware, and the Linux kernel abstracts and manages that hardware by supporting a number of abstractions and the associated mechanisms.
-	Then comes a number of standard libraries, such as those that implement the system call interfaces,
-	Then a number of utility programs that make it easier for users and developers to interact with the operating system.
-	Finally, at the very top, you have the user developed applications.

![Linux OS](/assets/img/Posts/GIOS/P1L2Linuxarch.png){: width="972" height="589" .w-50 .left}The kernel, itself, consists of several logical components, like all of the I/O management, memory management, process management. 

The Mac OSX operating system, from Apple, uses a different organization.
•	At the core is the Mach microkernel which implements key primitives like memory management, thread scheduling and inter-process communication mechanisms including what we call RPC.
•	The BSD component provides Unix interoperability via a BSD command line interface, POSIX API support as well as network I/O.
•	All application environments sit above this layer.
•	The bottom two modules are environments for development of drivers, and for kernel modules that can be dynamically loaded into the kernel.

#### Summary
In this lesson, we answered the big question, what is an operating system? And we saw that it's important because it helps abstract and arbitrate the use of the underlying hardware system.
-	We explained that to achieve this, an operating system relies on
-	abstractions, such as processes and threads
-	mechanisms that allow OS to manipulate abstractions
-	policies that specify how abstractions can be modified.
-	We saw that operating systems support a system call interface that allows applications to interact with them.
-	We looked at several alternatives in organizational structures for operating systems.
-	Then very briefly, we looked at some specific examples of operating systems, Windows, Linux, and Mac OS to see some examples of their system call interfaces or their organization.

## P2L1 - Introduction to Processes and Process Management
A process is an instance of an executing program, can be synonymous with task or job

#### Visual Metaphor
A process is like an order of toys
- State of execution (completed toys and toys waitig to be built)
- Parts amd temporary holding area (plastic pieces and containers, etc)
- May require special hardware (sewing machine, glue gun, etc)
A process equivalent:
- State of execution (program counter, stack)
- Parts amd temporary holding area (data, register state occupies state in memory)
- May require special hardware (I/O devices like disks/network devices)

#### What is a Process?
One of the roles of the OS is to manage hardware on behalf of the applications
- Application == program on disk, flash memory, etc (static entity)
- Process == state of a program when executing loaded in memory  (active entity)
An example could be a txt editor with saved lecture notes versus an untitled txt editor open, both are examples of processes

#### What does a process look like? (important)
![Process example](/assets/img/Posts/GIOS/P2L1Processexamp.png){: width="972" height="589" .w-50 .left} 
A process encapsulates all states of a running application. This includes the code, the data, all the variables that the application needs to allocate. Every single aspect of the process state has to be uniquely identified by its address. So an OS abstraction used to encapsulate all of the process state is an address space, which is shown in the image.*The address space is defined by a range of addresses from v0 to some vmax and different types of process state will appear in different regions in this address space.*

What are the different types of state in a process?
- test and data (static state when process first loads)
- heap (dynamically created during execution)
- stack (grows and shrinks during execution in a LIFO way)

#### Process Address Space
![Address example](/assets/img/Posts/GIOS/P2L1address.png){: width="972" height="589" .w-50 .left} The process representation is referred to as an address space. The potential range of addresses from vo to vmax, v in that range will be called a virtual address. They care called virtual because they don't have to correspond to actual locations in the physcial memory. Instead the memory management hardware and operating system components responsible for memory management, like page tables, maintain a mapping between the virtual addresses and the physical addresses. 

#### Process Address Space and Memory Management
Not all processes require the entire address space , on the flip side, you can run into the situation where you do not have enough memory to run your processes. To deal with this the operating system dynamically decides which portion of which address space will be present and where in physical memory. Regions of the physical memory can be occupied by different physical processes. With that said - each process the operating system must maintain some information regarding the process address space
#### Virtual Addresses Quiz

If two processes, P1 and P2, are running at the same time, what are the ranges of their virtual address space that they will have?

•	P1 has address ranges from 0 to 32,000, and P2 from 32,001 until 64,000.
•	Both P1 and P2 have address ranges from 0 to 64,000. (correct)
•	P1 has an address space range from 32,001 to 64,000, and P2 has address ranges from 0 to 32,000. 

#### How does the OS know what a process is doing?
- Program Counter (PC) maintained on the CPU while the process is executing in a register and there are other registers that are maintained on the CPU. They may have information like addresses for data, or they may have some status information that somehow affects the execution of the sequence
- CPU registers
- Stack Pointer, the top of th stack if defined by the stack pointer. Because the stack exhibits LIFO behavior, we need to know whats on top. 
All of this is maintained in the OS using a Process Control Block (PCB)

#### Process Control Block
A Process Control Block (PCB) is a data structure that the operating system maintains for every one of the processes that it manages.
![PCB](/assets/img/Posts/GIOS/P2L1PCB.png){: width="972" height="589" .w-50 .left}
From what we’ve seen so far the PCB must contain:
-	processes state like the program counter, the stack pointer, really all of the CPU registers (their values as they relate to the particular process),
-	various memory mappings that are necessary for the virtual to physical address translation for the process,
-	and other things. Some of the other useful information includes a list of open files for instance, information that’s useful for scheduling like how much time this particular process has executed on the CPU, how much time it should be allocated in the future (this can depend on the process priority), etc.

some aspects of the PCB change when a process state changes. An example being when a process requests more memory the OS will allocate more memory and establish a new valid virtual to physical memory mapping for this process. 
Other fields of the PCB change more frequently. During the execution of a program the program counter changes on every single instruction on a dedicated register. 

#### How is a PCB Used? 
![PCB how is used](/assets/img/Posts/GIOS/P2L1howispcbused.png){: width="972" height="589" .w-50 .left} Running through an example to explain. Assume you have an OS managing two processes, P1 and P2. They are created and their PCBs are stored in memory. If P1 is running on the CPU and P2 is idle, then the OS decides to interrupt P1, then P1 will become idle. At that point the OS will have to save the state information regarding P1, including the CPU registers into the PCB for P1. Then the OS needs to restore the state for P2 so it can execute (updating the CPU registers). When P2 is done or interupted, the PCB will beed to save the P2 information and the P1 PCB will need to be restored. P1 will now be running and the CPU registers will reflect the state of P1. Given that the value of the PCB for P1 corresponds exactly to the values it had when we interrupted P1 earlier. That means that P1 will resume its execution at the exact same point where it was interrupted earlier by the OS.

#### What is a context switch?

![Context Switch text](/assets/img/Posts/GIOS/P2L1ContextSwitch.png){: width="972" height="589" .w-50 .left} A context switch is when we switch the CPU from the context of one process to the context of another process. It requires an update in PCB memory or an update in the CPU cache. formally stated, a context switch is the mechanism used by the operating system to switch the execution from the context of one process to the context of another process. It can be expensive for two reasons, direct costs: the number of cycles that have to be executed to simply load and store all the values of the process control blocks to and from memory, and indriect costs: needing to access the memory when you context switch if you need any information from the previous process (cold cache), where as if you are running P1 and you need to access P1 information, the cache is readily accessible. 

#### Hot Cache Quiz
13 - Hot Cache Quiz
Here's a quick quiz about the processor cache. 
❏	When a cache is hot, it can malfunction, so we must context switch to another process. 
❏	When a cache is hot most process data is in the cache, so the process performance will be at its best. (correct)
❏	When a cache is hot, sometimes we must context switch. (correct)

#### Process Lifecycle 

![Process Lifecycle](/assets/img/Posts/GIOS/P2L1ProcessLifecycle.png){: width="972" height="589" .w-50 .left} All processes can have two states, running or idling. When a process is running, it can be interrupted and context-switched. At this point, the process is idle, but it's in what we call a ready state. It is ready to execute, except it is not the current process that is running from the CPU. At some later point, the scheduler would schedule that process again, and it will start executing on the CPU, so it will move into the running state.
![Process states](/assets/img/Posts/GIOS/P2L1Processstates.png){: width="972" height="589" .w-50 .left} Other states that a process can be in are shown in the img. There is an example to run through the process. 
1. when a process is created, it enters the new state. This is when the OS will perform admission control, and if it's determined that it's okay, the operating system will allocate and initiate a process control block (PCB) and some initial resources for this process. Provided that there are some minimum available resources, the process is admitted, and at that point, it is ready to start executing.
2. It is ready to start executing, but it isn't executing on the CPU. It will have to wait in this ready state until the scheduler is ready to move it into a running state when it schedules it on the CPU.
3. Once the scheduler gives the CPU to a ready process, that ready process is in the running state. And from here, a number of things can happen. First, the running process can be interrupted so that a context switch is performed. This would move the running process back into the ready state. Another possibility is that a running process may need to initiate some longer operation, like reading data from disk or to wait on some event like a timer or input from a keyboard. 
3. *At that point, the process enters a waiting state. When the event occurs or the I/O operation completes, the process will become ready again.*
4. Finally, when a running process finishes all operations in the program or when it encounters some kinds of error, it will exit. It will return the appropriate exit code, either success or error, and at that point, the process is terminated.

#### Process State Quiz
The CPU is able to execute a process when the process is in which of the following states?
- Running (correct)
- Ready (correct)
- New
- Waiting

#### Process Creation
In OS, a process can create child processes. All processes come from a single root and they have some relationship to one another through parent child behavior. Priviledged processes are root processes, most of the OS processes are root processes, so the OS will be loaded onto the machine after it boots up and it will create a number of initial processes. 
![Process creation](/assets/img/Posts/GIOS/P2L1Processcreate.png){: width="972" height="589" .w-50 .left} When a user logs into a system a user shell process is created and then when the user types in command like ls, or emacs, then new processes get spawned from that shell parent process. So the final relationship looks like this tree. Most operating systems support two basic mechanisms for process creation, fork and exec. A process can create a child via either one these mechanisms.
- With the fork() mechanism the OS will create a new PCB for the child and copy the exact same values from parent PCB into the child PCB.
- Exec() behaves differently. It will take a PCB structure created via fork, but it will not leave its values to match the parent’s values. 

#### Parent Process Quiz
On UNIX-based systems, init is the first process that starts after the system boots. And because all other processes can ultimately be traced to init, it's referred to as the parent of all processes.
On the Android OS, Zygote is a daemon process which has the single purpose of launching app processes. 

#### Role of the CPU Scheduler
![CPU Scheduler](/assets/img/Posts/GIOS/P2L1CPUscheduler.png){: width="972" height="589" .w-50 .left} . For the CPU to start executing a process the process must be ready first. The problem is, however, there will be multiple ready processes waiting in the ready queue. How do we pick what is the right process that should be given the CPU next, that should be scheduled on the CPU? So the question is which process do we run next? This is determined by a component call the CPU scheduler.
- The CPU Scheduler is an OS component that manages how processes use the CPU resources. It decides which one of the currently ready processes will be dispatched to the CPU to start running and how long it should run for, and it also determines how long this process should be allowed to run for.

Over time this means that in order to manage the CPU:
- The OS must be able to preempt, to interrupt the executing process and save its current context. This operation is called preemption.
- Then the OS must run the scheduling algorithm to choose one of the ready processes that should be run next.
- Once the processes are chosen, the OS must dispatch this process onto the CPU and switch into its context so that that process can finally start executing.

Given that the CPU resources are precious, the OS needs to make sure that CPU time is spent running processes and not executing scheduling algorithms and other OS operations. So OS should minimize the amount of time that it takes to perform these tasks (preemption, scheduling, and dispatching), i.e., the OS must be efficient. 

#### Length of process
How often do we run the scheduler? The more frequently we run it the more CPU time is wasted on running the scheduler vs running application processes. So another way to ask the same question is: How long should a process run? The longer we run a process the less frequently we are invoking the scheduler to execute.
- Useful CPU work == Total processing time/Total time = (2 * Tp)/(2 * Tp+2*t_scheduled) if Tp == t_scheduled => only 50% of CPU time spent of useful work!
- If Tp (processing time) is much larger than scheduling time, t_scheduled then ~ 91% of the CPU time is spent doing useful work, we want a big Tp. 
- timeslice == time Tp allocated to a process on the CPU. 
![timeslice](/assets/img/Posts/GIOS/P2L1timeslice.png){: width="972" height="589" .w-50 .left} As you can see there are a lot of design decisions and tradeoffs that we must make when we’re considering how to design a scheduler. Some of these include deciding what are appropriate timeslice values, or deciding what would be good metrics that are useful when the scheduler is choosing what’s the next process it should run.

#### What about I/O?
So far we know the OS manages how processes access resource on the hardware platform and this in addition to the CPU and memory will include I/O devices, peripherals like keyboards, network cards, disks, etc.
1. imagine a process had made an I/O request, the OS delivered that request, for instance, it was a read request to disk.
2. then the process is placed on the I/O queue that’s associated with that particular disk device. So the process is now waiting in the I/O queue.
3. the process will remain waiting in the queue until the device completes the operations, so the I/O event is complete, and responds to that particular request.
4. once the I/O request is met, the process is ready to run again and depending on the current load in the system it may be placed in the ready queue, or it may be scheduled on the CPU if there is nothing else waiting in the ready queue before it.
![I/O](/assets/img/Posts/GIOS/P2L1IO.png){: width="972" height="589" .w-50 .left} 
So to summarize, a process can make its way into the ready queue in a number of ways. 
-	A process which was waiting on an I/O event ultimately found its way into the ready queue.
-	A process which was running on the CPU but its timeslice expired goes back on the ready queue.
-	When a new process is created via the fork call it ultimately ends its way on the ready queue.
-	For a process which was waiting for interrupt, once the interrupt occurs it will also be placed on the ready queue.

#### Scheduler Responsibility Quiz
The question is, which of the following are NOT a responsibility of the CPU scheduler? The options are:
❏	maintaining the I/O queue (correct)
❏	maintaining the ready queue
❏	deciding when to context switch
❏	or deciding when to generate an event that a process is waiting on. (correct)

#### Can Processes Interact 
Another natural question can be, can processes interact? And the simple answer is YES. An operating system must provide mechanisms to allow processes to interact with one another.
![Process Interaction](/assets/img/Posts/GIOS/P2L1processinteract.png){: width="972" height="589" .w-50 .left} Remember that the operating systems go through a great deal to protect and isolate processes from one another. Each of them is a separate address space. They control the amount of CPU each process gets, which memory is allocated, and accessible to each process. So these communication mechanisms that we will talk about somehow have to be built around those protection mechanisms. These kinds of mechanisms are called inter-process communication mechanisms, or we refer to them as IPC (interprocess communication)

The IPC mechanisms:
-	Help transfer data and information from one address space to another
-	Maintain the protection and isolation that operating systems are trying to enforce
-	IPC mechanisms need to provide flexibility as well as clearly performance, because different types of interactions between processes may exhibit different properties. Periodic data exchanges, continuous stream of data flowing between the processes, or coordinated applet, to some shared single piece of information.

The Message Passing IPC:
One mechanism that operating systems support is message passing IPC. The operating system establishes a communication channel, like a shared buffer, and the processes interact with it by writing or sending a message into that buffer. Or, reading or receiving a message from that shared communication channel. 
![Process Interaction Message](/assets/img/Posts/GIOS/P2L1messagepass.png){: width="972" height="589" .w-50 .left} The benefit of this approach is that it's the operating system who will manage this channel, and it's the operating system that provides the exact same APIs, the exact same system calls for writing or sending data, and the reading or receiving data from this communication channel. The downside is the overhead. For every single piece of information that we want to pass between these two processes, we have to copy from the user space of the first process into this channel that's sitting in the OS, in the kernel memory. And then back into the address space of the second process.

The shared memory IPC:
The other type of IPC mechanism is what we call shared memory IPC. The way this works is the operating system establishes the shared memory channel, and then it maps it into the address space of both processes. The processes are then allowed to directly read and write from this memory, as if they would to any memory location that's part of their virtual address space. So the operating system is completely out of the way in this case.
![shared memory](/assets/img/Posts/GIOS/P2L1sharedmemory.png){: width="972" height="589" .w-50 .left} That in fact is the main advantage of this type of IPC. That the operating system is not in the path of the communication. So the processes, while they're communicating are not going to incur any kind of overheads from the operating system. The disadvantage of this approach is because the operating system is out of the way it no longer supports fixed and well defined APIs how this particular shared memory region is used. For that reason, its usage sometimes becomes more error prone, or developers simply have to re-implement code to use this shared memory region in a correct way.

#### Lesson Summary
We covered process and process management:
- Process and process-related abstractions including address space and PCB
- Basic mechanisms for managing process resources (context, switching, process creation, scheduling, interprocess communication)


## P2L2 - Threads and Concurrency 

### 1. Visual Metaphor

A thread is like a worker in a toyshop

- worker in a toyshop is a:
1. active entity
2. executing unit of toy order
3. works simultaneously with others
4. many workers completing toy orders
5. requires coordination (sharing of tools, parts, workstations, etc)

- a thread:
1. is an active entity (executing unit of a process)
2. works simultaneously with others (many threads executing)
3. requires coordination (sharing of I/O devices, CPUs, memory)

### 2. Process versus thread
![process v thread](/assets/img/Posts/GIOS/P2L2/img1.png){: width="972" height="589" .w-50 .left}


A single threaded process is represented by its address space (i.e. code/data/files). The address space will contain all of the virtual to physical address mappings for the process, for its code, its data, its heap section files, for everything. The process is also represented by its execution context (i.e. registers/stack) that contains information about the values of the registers, the stack pointer, program counter, etc. The operating system represents all this information in a process control block (PCB = code/data/files + regs/stack).

Threads represent multiple independent execution contexts. They're part of the same virtual address space, which means that they will share all of the virtual to physical address mappings. They will share all the code, data, files. However, they will potentially execute different instructions, access different portions of that address space, operate on different portions of the input, and differ in other ways. This means that each thread will need to have a different program counter, stack pointer, stack, thread-specific registers. So for each and every thread, we have to have separate data structures to represent this per-thread information.

extra context - https://stackoverflow.com/questions/9501526/what-is-the-difference-b-w-tcbthread-control-block-pcbprocess

### 3. Why are threads useful?
- By parallelizing the program in this manner, we achieve speed up. We can process the input much faster than if only a single thread on a single CPU had to process the entire input.
- threads may execute completely different portions of the program. For instance, you may designate certain threads for certain I/O tasks like input processing, or display rendering
- one benefit from specialization is that we end up executing with a hotter cache. And that translates to gains in performance.

Why not just write a multi-process application where every single processor runs a separate process?
- If we do that, since the processes do not share an address space (AS) we have to allocate for every single one of these contexts AS and execution context (EC). So the memory requirements, if this were a multiprocessor implementation, would be that we have to have four Address Space allocations and four Execution Context allocations. 
- synchronization among processes, requires IPC mechanisms that are costlier. 
- A multithreaded implementation results in threads sharing an AS so we don’t need to allocate memory for all of the AS information for these remaining ECs. This implies that a multithreaded application is more memory efficient, it has lower memory requirements than its multi-process alternative
### 4. Are threads useful on a single CPU or when # of threads > # of CPU?

![process v thread](/assets/img/Posts/GIOS/P2L2/img3.png){: width="972" height="589" .w-50 .left}

- Threads save time in context switching compared to processes because they share and AS, so they don't need to create new virtual to physical address mappings for new process scheduling. 

### 5. Benefits to applications and OS code
By multithreading the OS’s kernel we allow the OS to support multiple EC, and this is particularly useful when there are multiple CPUs, so that the OS context can execute concurrently on different CPUs in a multiprocessor/multicore platform.
![process v thread](/assets/img/Posts/GIOS/P2L2/img4.png){: width="972" height="589" .w-50 .left}

#### Process vs Threads Quiz
•	[Threads] The first statement applies to threads. Each thread belonging to a process shares the virtual address space with other threads in that process.
•	[Processes] Because threads share the address space, the context switch among them happens faster than processes. So, processes take longer to context switch.
•	[Both]Both threads and processes have their execution context described with stack and registers.
•	[Threads]Because threads share the virtual address space, it is more likely that when multiple threads execute concurrently, the data that's needed by one thread is already in the cache, brought in by another thread. So, they typically result in hotter caches. Among processes, such sharing is really not possible.
•	[Both]Then the last answer is B. We already saw that for processes, it makes sense for the operating system to support certain inter-process communication mechanisms. And we'll see that there are mechanisms for threads to communicate and coordinate and synchronize amongst each other.

### 6. Thread Mechanisms 
 - Thread data structure (idenitfy threads, keep track of resource usage
 - mechanisms to create and manage threads
 - mechanisms to safely coordinate among threads running concurrently in the same address space

Issues with concurrent execution

threads share the same virtual to physical address mappings
![process v thread](/assets/img/Posts/GIOS/P2L2/img5.png){: width="972" height="589" .w-50 .left}

To deal with such concurrence, we need a mechanism for threads to execute exclusively, which is called mutual exclusion. Mutual exclusion is a mechanism where only one thread at a time is allowed to perform an operation. The remaining threads, if they want to perform the same operation, must wait their turn. The actual operation that must be performed in mutual exclusion may include some update to state, or access to some data structure that's shared among all these threads.

threads share the same virtual to physical address mappings
![process v thread](/assets/img/Posts/GIOS/P2L2/img6.png){: width="972" height="589" .w-50 .left}

### Threads and Thread Creation
First, we need some data structure to represent a thread. The thread type proposed by Birrell is a data structure that contains all the information that is specific to a thread and can describe a thread. This includes
-	the thread identifier that the threading system will use to identify a specific thread,
-	register values, in particular the stack of a thread,
-	the program counter and the stack pointer,
-	and any other thread specific data or attributes. 

Second - For thread creation, Birrell proposes a fork call with two parameters,
•	a proc argument, which is the procedure that the created thread will start executing,
•	and args, which are the arguments for this procedure.
*Not to be confused with UNIX fork!
![process v thread](/assets/img/Posts/GIOS/P2L2/img7.png)
When a thread T0 calls a fork a new thread T1 is created. That means that new thread data structure of this type is created and its fields are initialized such that its program counter will point to first instruction of the procedure proc, and these arguments will be available on this stack of the thread. After the fork operation completes, the process as a whole has two threads. T0, the parent thread, and T1. These can both execute concurrently. T0 will execute the next operation after the fork call, and T1 will start executing with the first instruction in proc, with the specified arguments.

Next -  we need some mechanism to determine
•	if a thread is done,
•	and if it is necessary to retrieve its result,
•	or at least to determine the status of the computation (success or error).
To deal with this issue, Birrell proposes a mechanism he calls join. It has the following semantic: child_result = join(id of child thread).
When a parent thread calls join, it will be blocked until the child thread completes. Join will return to the parent the result of the child’s computation. At that point, the child thread exits the system; any allocated data structure state for the child, all of the resources that were allocated for its execution will be freed and the child thread is terminated.

### Thread Creation Example
![process v thread](/assets/img/Posts/GIOS/P2L2/img8.png)
### Mutexes
![process v thread](/assets/img/Posts/GIOS/P2L2/img9.png)
### Mutual Exclusion
There is a danger in the previous example that the parent and the child thread will try to update the shared list at the same time potentially overwriting the list elements.
To do this the OS and threading libraries in general support a construct called mutex. 
Other threads attempting to lock the same mutex are not going to be successful. These threads will be blocked from performing the lock operation. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img10.png) The portion of the code protected by the mutex, is called critical section. In Birrell’s paper, this is any code within the curly brackets of the lock operating that he proposes to be used with mutexes. The critical section code should correspond to any kind of operation that requires that only one thread at a time to perform the operation. For instance, it can be updated to shared variable like the list, or increasing/decreasing a counter, or performing any type of operation that requires mutual execution between the threads. Other than the critical section code, the rest of the code in the program, the threads may execute them concurrently.

### Making safe_insert safe
![process v thread](/assets/img/Posts/GIOS/P2L2/img11.png)

### Mutex Quiz
![process v thread](/assets/img/Posts/GIOS/P2L2/img12.png)

### Producer/Consumer Example
For threads the first construct that Birrell advocates is mutual exclusion, and that’s a binary operation. A resource is either free and you can access it, or it is locked and you have to wait. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img13.png)
Operating this way is clearly wasteful and it would be much more efficient if we could just tell the consumer when the list is full so that it can at that point go ahead and process the list.

### Condition Variable
Birrell recognizes this common situation in multithreaded environments and argues for a new construct, a condition variable. He says that such condition variables could be used in conjunction with mutexes to control the behavior of concurrent threads. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img14.png)

### Condition Variable API
![process v thread](/assets/img/Posts/GIOS/P2L2/img15.png)

#### Condition Variable Quiz 
![process v thread](/assets/img/Posts/GIOS/P2L2/img16.png)

### Reader/Writer Problem (review)
The readers/writer problem is such that at any given point of time, zero or more of the reader's threads can access the resource concurrently, but only zero or one writer threads can access the resource at the same time. And clearly we cannot have a situation in which a reader and a writer thread are accessing the shared resource at the same time.
- One naive approach to solving this problem would be to simply protect the entire resource with a mutex and put a lock/unlock operations around it. So whenever anyone of these types of threads is accessing the resource, they will lock the mutex, perform their respective access, and then release the mutex. This is, however, too restrictive for the readers/writer problem. 

- In the simple case when there are no readers or writers accessing the resource, either an upcoming read operation or write operation can be granted.
- If the read counter is greater than zero (there are some readers already accessing this file), it's fine to allow another reader to access the file. Since readers are not modifying it, so it's okay to grant that request.
- If there is a writer that's accessing the resource, we cannot allow neither read nor write operation.
![process v thread](/assets/img/Posts/GIOS/P2L2/img17.png)

#### Reader/Writer Problem Code
![process v thread](/assets/img/Posts/GIOS/P2L2/img18.png)


### Critial Section Structure
![process v thread](/assets/img/Posts/GIOS/P2L2/img21.png)
If we closely examine each of the highlighted blocks, we will see that they have the following form:
- First we must lock the mutex, then we check on a predicate to determine whether it’s ok to actually perform the access.
- If the predicate is not met we enter the while loop and perform wait(). The wait() is associated with a condition variable and a mutex. When we come out of the wait statement, we must check the predicate in the next while loop again.
- If the predicate is met, we exit the while loop. We could perform some update to the shared state and these updates could potentially impact the condition variable for other threads. So we should need to notify them via signal() or broadcast() with the appropriate condition variable.
- Finally we must unlock the mutex. In Birrell's paper the unlock operation is implicit in the closing brace. In some other threaded environments, we must explicitly do an unlock call. 

### Critical Section Structure with Proxy
![process v thread](/assets/img/Posts/GIOS/P2L2/img22.png)
The actual read/write operations to the shared resource, the reads and writes of the shared file in this case, must be protected by the enter/exit critical sections. Each of these blocks internally follows the critical section structure that we outlined before where they lock the mutex, check for a predicate. 
The mutex is held only within these Enter/Exit Critical Section codes and unlocked at the end. This allows us to control the access through the proxy variable but allowing more than one thread to be in the critical section at a given point in time. This lets us benefit from mutexes controlling shared access.
It also allows us to deal with one-thread-a-time limit when using default mutex. 

### Avoiding Common Mistakes
![process v thread](/assets/img/Posts/GIOS/P2L2/img23.png)
- make sure to keep track of the mutex and condition variables that are specifically used with a given shared resource. 
- make sure that if a variable or a piece of code is protected with a mutex in one portion of your code, that you're always consistently protecting that same variable, or that same type of operation with the same mutex everywhere else in your code 
- Another common mistake is to use different mutexes for a single resource. Some threads read the same file by locking mutex m1, and other threads write to the same file by locking mutex m2. 
- It's also important to make sure that when you're using a signal or a broadcast you're actually signaling the correct condition variable. That's the only way that you can make sure that the correct set of threads will be notified. 
- Make sure that you're not using signal when broadcast is needed. Remember that with a signal only one thread will be woken up to proceed. On the other hand, if signal is needed but you use broadcast, that's fine. You will still end up waking up one thread or more and not affect the correctness of the program. You may just end up affecting its performance, which is not as dangerous.
![process v thread](/assets/img/Posts/GIOS/P2L2/img24.png)

### Spurious Wake up
Let's say currently there is a writer that's performing a write operation and it has locked counter_mutex.
Meanwhile there are readers waiting on a condition variable, read_phase in the wait queue. When the writer issues the broadcast operation, this broadcast can start removing reader threads from the wait queue that's associated with read_phase.
If we don’t unlock the mutex before signal and broadcast, i.e. the writer still holds the mutex, the readers that are woken up on read_phase will fail to acquire the mutex and none of these reader threads will be able to proceed. They'll be woken up from the queue that's associated with the condition variable, and they'll have to be placed on the queue that's associated with the mutex. This is what we call spurious wake-up: we signaled, we woke up the threads but that wake-up was unnecessary.
![process v thread](/assets/img/Posts/GIOS/P2L2/img25.png)
![process v thread](/assets/img/Posts/GIOS/P2L2/img26.png)

### Deadlocks

![process v thread](/assets/img/Posts/GIOS/P2L2/img27.png)
Note: The most accepted solution is to maintain the lock order: when using nested mutexes, be sure to use the mutexes in the same order when the mutexes are used by other threads.

There is more that goes into dealing with deadlocks:
-	Detecting them
-	Avoiding them
-	Recovering from them.

But for the sake of this class, remember that maintaining lock order will give you a deadlock-proof solution.

![process v thread](/assets/img/Posts/GIOS/P2L2/img28.png)

### Kernel vs User level threads
Kernel-level threads imply that the operating system itself is multi-threaded. Kernel-level threads are visible to the kernel and are managed by kernel-level components like the kernel-level scheduler. So it is the OS scheduler that decides how these kernel level threads will be mapped onto the underlying physical CPUs and which of them will be run at any given time. Some of these kernel-level threads may be there to directly support some of the processes so they can execute some user-level threads. Some other kernel level threads may be there just to run certain OS level services like daemons.
![process v thread](/assets/img/Posts/GIOS/P2L2/img29.png)

Types of models:

- The first model is a one-to-one model. Here each user-level thread has a kernel-level thread associated with it. When a process creates a user-level thread, either a kernel-level thread will be created or an existing kernel-level thread will be associated with the user-level thread.

![process v thread](/assets/img/Posts/GIOS/P2L2/img30.png)
- The second model is the many-to-one model. Here all the user-level threads are supported and mapped onto a single kernel-level thread. So at the user-level there is a thread management library that decides which one of the user-level threads will be mapped onto the kernel-level thread at any given point in time. That user-level-thread will run only until the kernel-level thread is scheduled onto a CPU.
![process v thread](/assets/img/Posts/GIOS/P2L2/img31.png)

- The third model is the many-to-many model, which allows some user-level threads to be associated with one kernel-level process, others perhaps to have a one-to-one mapping with a process. So it’s sort of the best of both worlds. The kernel knows that the process is multithreaded since it has assigned multiple kernel-level threads to it and also if one user-level thread blocks on I/O and as a result the kernel-level thread blocks as well, the process overall will have other kernel-level threads onto which the remaining user-level threads will be scheduled. 

![process v thread](/assets/img/Posts/GIOS/P2L2/img32.png)

### Scope of Multithreading
There are different levels at which multithreading is supported, at the entire system or within a process. Each level affects the scope of the thread management system.
- At the kernel level we have system wide thread management that is supported by the OS system level thread managers, meaning that the OS thread managers will look at the entire platform when making decisions as to how to allocate resources to the threads. This is the system scope.
- At user level, a user-level thread library linked to the process manages all the threads within that single process only, so its management scope is process-wide. Different processes will be managed by different instances of the same library or even different processes may be linked to entirely different user level libraries. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img33.png)
- If the user-level threads have process scope, the OS doesn’t see all of them. So at the OS level the available resources will be managed 50%-50% among them, i.e. both webserver and database will be allocated an equal share of the kernel-level threads and the kernel level scheduler will manage these threads by splitting the underlying CPUs among them. The result is that the webserver user-level threads will have half of the amount of CPU cycles allocated to the database threads.
- If we have a system scope, all the user-level threads will be visible at the kernel-level, so the kernel will allocate kernel-level threads to every user-lever threads. Therefore, each one of the user-level threads has an equal portion of the CPU, if that happens to be the policy that the kernel implements. As a result, the process that has more user-level threads, will end up receiving a larger share of the underlying physical resources.

### Multithreading patterns

Boss-workers, Pipeline and Layered structure. 

#### Boss
Popular pattern characterized by one boss thread and some worker threads.
The boss is in charge of assigning tasks to workers. The workers perform the tasks. The boss just accepts orders and immediately passes it on to one of the workers (step 1). Each worker performs (steps 2 – 6). Since there is only one boss thread that manages all the orders, the throughput of the system is limited by the boss’s performance. So, we must keep the boss efficient and limit the amount of work the boss performs.
To summarize the a common the boss-worker model has those features:
-	Boss assigns work to workers.
-	Worker performs entire task.
-	Boss-worker communicate via produce/consumer queue.
-	Worker pool is used to manage the number of worker threads in the queue, statically or dynamically.

The benefit of this model is simplicity. One thread assigns work to all others threads while all others threads simply performs the tasks.
-	One disadvantage is the overhead caused by the worker threads pool management.
-	Another downside is that it ignores locality. The boss doesn’t keep track of what each worker is doing. If we have a situation in which a worker just completed one certain type of task, it’s more likely that the same work would be more efficient at performing a similar task in the future. But if the boss is not paying attention to what the workers are doing, it has no way of making such optimization.

- Boss worker Variant: 
all workers are created equal versus designating workers for specific tasking 
-	Locality: Performing similar task does not require context switch, i.e., it will work with hot cache
-	QoS: Better Quality of Service Management 
-	Load Balancing: It becomes complicated to calculate how many threads to be assigned to a particular task.

### Pipeline Pattern
A different way to assign work to threads in a multithreaded system is using this pipeline approach. In this pipeline approach the overall task is divided into subtasks, and each of the subtasks is performed by a separate thread. Threads are assigned subtasks in the system, and the entire complex task is executed as a pipeline of threads. 
![process v thread](/assets/img/Posts/GIOS/P2L2/img34.png)
-	A pipeline is a sequence of stages where a thread performs a stage in the pipeline, and that's equivalent to some subtask in the end-to-end processing. 
-	To keep the pipeline balanced a stage can be executed by more than one thread. We can use the same thread pool management technique that we described in the Boss-Workers model to determine what is the right number of threads per stage.
-	Passing partial work products or results across the stages in the pipeline should be done via a shared buffer based communication. This provides for some elasticity in the implementation and avoids stalls due to temporary pipeline imbalances.
- A key benefit of the approach is the fact that it allows for highly specialized threads and this leads to improved efficiency. Like what we saw in the variant of the boss-worker model, when threads perform a more specialized task, it's more likely that the state that they require processing is present in the processor cache and such locality can ultimately lead to improved performance.
- A negative of the approach is the fact that it is fairly complex to maintain the pipeline balanced over time. When the workload pattern changes (e.g. more toys arriving) or when the resources of the pipeline change (e.g. a worker slows down or takes a break), we'll have to rebalance the entire pipeline to determine how many workers to assign to each stage. In addition to that, there is more synchronization since there are synchronization points at multiple points in the end-to-end execution. 

### Layered Pattern
A layered model is one in which each layer is assigned a group of related tasks, and the threads that are assigned to a layer can perform any one of the subtasks that correspond to it. End to end, though, a task must pass up and down through all the layers. So, unlike in the pipeline pattern, we must be able to go in both directions across the stages.
![process v thread](/assets/img/Posts/GIOS/P2L2/img35.png)

## P2L4 Thread Design Considerations

### Kernal vs User Level Threads
![thread design](/assets/img/Posts/GIOS/P2L4/img1.png)
-	Supporting threads at the kernel level means that the OS kernel itself is multithreaded. To do this, the OS kernel maintains some abstraction, for our threads data structure to represent threads, and it performs all of the operations like synchronization, scheduling, et cetera, in order to allow these threads to share the physical resources.
-	Supporting threads at the user level means that there is a user-level library that is linked with the application, and this library provides all of the management and runtime support for threads. It will support a data structure that is needed to implement thread abstraction and provide all the scheduling, synchronization and other mechanisms that are needed to make resource management decisions for these threads. In fact, different processes may use entirely different user-level libraries that have different ways to represent threads that support the different scheduling mechanisms. 

### Thread Related Data Structures : Single CPU
Single threaded process:
Let's start by looking at what happens in a single threaded process.
The PCB (Process Control Block) contains
-	the address space (the virtual to physical address mappings)
-	its stack
-	its register
Whenever this process makes a system call, it tracks into the kernel, executes in the context of a kernel thread.

Many to One:
If the process is multithreaded, this will be like the many-to-one model.
There is only one kernel level thread and there is a user level threading library that manages these user threads.
This user-level library will need some way to represent threads so that it can track their resource use and make decisions regarding scheduling and synchronization. So it should have some ULT data structure that contains:
-	User-level thread ID
-	User-level thread registers
-	User-level thread stack

Many to Many:
We start splitting up the PCB: 
The KLT data structure will store:
•	Kernel-level stack
•	Kernel-level register pointer
The “original PCB” now only contains
•	The virtual address mappings
•	Some additional information that's relevant for the entire process (all of the kernel-level threads).
From the perspective of the ULT library, each KLT looks like a CPU. So the threading library looks at the user-level threads and decides which one of the user-level threads will be scheduled onto the underlying kernel-level threads.

### Thread Data Structures : At Scale
![thread design](/assets/img/Posts/GIOS/P2L4/img2.png)
Now, let's say we have multiple such processes. 
-	ULT data structures
-	PCB data structure
-	KLT data structures

Those data structures are related to each other.
-	The threading library keeps track of all of the ULT that represent the single process. There is a relationship between them and the PCB.
-	For each process we need to keep track of which KLTs execute this process.
-	Vice versa, for each KLT we have to make sure we know what address space is in the PCB.

If the system has multiple CPUs, we need a CPU data and maintain a relationship between the KLT and the CPU data structure.
-	What is the CPU that a KLT has affinity to, last strand it was scheduled on
-	For a CPU, a pointer to its current thread or a pointer to the threads that typically run there, and similar information.

When the kernel itself is multithreaded, we said we can have multiple kernel-level threads supporting a single user-level process. When the kernel needs to schedule, or context switch, among kernel-level threads that belong to different processes, it can quickly determine that they point to a different process control block. So they will have different virtual address mappings, and therefore can easily decide that it needs to completely invalidate the existing address mappings and restore new ones.
In the process, it will save the entire PCB of the first kernel-level thread, and then if it's context switching to the second one, it will restore the entire PCB of the second one.

If there are multiple processes, then we need these data structures for each process. There must be relationships kept amongst the different data structures.
-	The user level thread maintains a pointer to the PCB and vice versa.
-	The kernel level thread also keeps a mapping to the PCB and vice versa.
### Hard and Light Process State
When we're context switching among these two kernel level threads
-	there is a portion of this process control block information that we want to preserve, like all of the virtual address mappings.
-	there is portion that's really specific to the particular kernel level thread and it depends on what is the user level thread that's currently executing. It's something that the threading library directly impacts.

The hard process state that's relevant for all of the user level threads that execute within that process.
The light process state that is only relevant for a subset of the user level threads that are currently associated with a particular kernel-level thread.
![thread design](/assets/img/Posts/GIOS/P2L4/img3.png)

### Rationale for Multiple Data structures 
![thread design](/assets/img/Posts/GIOS/P2L4/img4.png)
Single PCB:
-	Scalability is limited due to the size.
-	Overheads are limited because they need to have private copies.
-	Performance is affected because everything has to be saved and restored. 
-	Flexibility is affected by the fact that updates are a little bit more difficult.

Multiple Data Structures:
-	We gain in scalability
-	Overhead is reduced because we don’t have to have separate copies for everyone.
-	We have improvements in performance because context switch time can be reduced.
-	We have more flexibility

### User Level Structures in Solaris 2.0 (SunOS 5.0)
![thread design](/assets/img/Posts/GIOS/P2L4/img5.png)

This is a diagram from figure 1 in the Stein and Shah paper “Implementing Lightweight Threads” and it illustrates the threading model supported in the operating system.
-	From the bottom up, the OS is intended for multiprocessor systems with multiple CPUs.
-	The kernel itself is multithreaded - there are multiple kernel level threads
-	At user level, the processes can be single or multithreaded. Both many-to-many and one-to-one mappings are supported.
-	Each kernel level thread that’s executing a user level thread has a lightweight process (LWP) data structure associated with it.
-	From the user level library’s perspective, these LWP represent the virtual CPUs onto which the ULT will be scheduled.
-	At the kernel level, there will be a kernel-level scheduler that will be managing the kernel level threads and scheduling them onto the physical CPUs.

We will now look a little more closely at the user level thread data structures. They’re described in the “Implementing lightweight threads” paper by Stein and Shah. This does not describe the POSIX threads (pthread), but it’s a similar type of user level threading library.

![thread design](/assets/img/Posts/GIOS/P2L4/img6.png)
When a thread is created, the library returns a thread ID. But this is not a direct pointer to the actual thread data structure. Instead, it’s an index in a table of pointers. It is the table pointer that in turn points to the actual thread data structure. The nice thing about this is that if there is a problem with the thread:
-	With the thread ID being a pointer, that pointer would just point to some corrupt memory, and we can’t really figure out what’s going on.
-	With the thread ID stored in the table entry, we can encode some information in the table entry that can provide some meaningful feedback or an error message.

The thread data structure contains several fields. In particular, the thread local storage contains the variables that are defined in the thread functions which are known at compile time. The compiler can allocate private storage on a per thread basis. Stack size might be defined by library defaults or user provided. 
The size of the ULT data structure is known at compile time therefore the thread data structures can be created such that it is contiguous in memory. This would improve locality and memory access (faster speed).It can make it easy for the scheduler to find the next thread. It just has to basically multiply the thread integers with the size of the data structure.
 It’s possible that as the stack gets growing, one thread will end up overwriting the data structure of another thread. The solution is a red zone. This refers to a portion of the virtual address space that’s not allocated, so if a thread is running and its stack is increasing, if it tries to write to an address that falls into this red zone region, then the operating system will cause a fault.

### Kernel Level Structures in Solaris 2.0
![thread design](/assets/img/Posts/GIOS/P2L4/img7.png)
Each Process data structure will contain information on
-	What are all the KLT that execute within that process address space?
-	What are the mappings that are valid between the virtual and physical memory?
-	What are the user credentials? For instance, if this process is trying to access a file, we have to make sure that that particular user has access to that file.
-	What are the signal handlers that are valid for this process (this information is about how to respond to certain events that can occur in the operating system)

Next the lightweight process (LWP) data structure. This contains information that’s relevant for a sub-subset of the process, such as:
-	User-level registers
-	System call arguments.
-	Resource usage information.
-	Signal mask.
The information in LWP is similar to information in ULT data structure, but LWP is visible to the kernel while ULT is not. So when the OS level schedulers need to make scheduling decisions, they can see this information and act upon it. If we want to find out the aggregate data usage for the entire process, we need to walk through all the lightweight processes that are associated with it. The LWP does not always have to be always present in memory, we only need it when a ULT need to run.

The kernel level thread data structure includes the kernel level information
-	Registers
-	Stack pointers
-	Scheduling info (class. etc)
-	Pointers to data structures associated with this kernel (e.g. LWP, CPU structure, etc)

The kernel level thread is always needed. They’re operating level services that need to access some information even when a thread is not active. 

Next is the CPU data structure. It has information like the
-	Current thread that’s currently scheduled.
-	Lists of the other KLTs that ran there.
-	Some information about how to execute the procedure for dispatching a thread, or how to respond to various interrupts on the referral devices.
Note that if we have information about the CPU data structure, through it we can find that information about all of the different data structures that are needed to rebuild the entire process state.

Below is how the Eykholt paper on multithreading the OS kernel describes the relationship between all of these data structures. 
-	A process data structure has information about the user, points to the address space, and also points to a list of kernel level thread structures.
-	Each of the KLT structures points to the LWP that it corresponds to, its stack, etc.
-	The LWP and stack portion is actually swappable.
-	Not shown in this figure is the CPU data strucutre.
-	Some other pointers are also not shown (like from the thread going back to the process etc). 
![thread design](/assets/img/Posts/GIOS/P2L4/img8.png)


### Basic Thread Management Interaction

Example: Consider we have a multithreaded process which has 4 user-level threads. However, at any given point of time, the actual level of concurrency is just 2. If our operating system has a limit on the number of kernel threads that it can support, it would be nice if the user-level process declares that it only needs two threads. When the process starts, the kernel will first give it a default number of kernel-level threads and the accompanying lightweight threads (e.g. only 1).
Then the process will request additional kernel-level threads, by a system call called set_concurrency(). In response to this system call, the kernel will create additional threads and it will allocate those to this process. 
Now let’s consider this scenario in which the two user-level threads that were mapped on the underlying kernel-level threads block. If the ULTs need to perform some I/O operation, they were moved onto the wait queue associated with the I/O event. As a result, the KLTs associated with the ULTs are blocked as well. Now the whole process is blocked, because it only has two kernel-level threads, both of them are blocked.
The reason why this is happening is because the user-level library doesn't know what is happening in the kernel, it doesn't know that the kernel threads are about to block.
The solution is to:
-	Let the kernel notify the user-level library before it blocks the kernel-level threads.
-	Then the user-level library can look at its run queue.
-	If there are any runnable user-level threads, the user-level library can make a system call to request more KLT or LWP.
-	In response to this system call, the kernel can allocate more kernel-level thread, and the library can start scheduling the remaining user-level threads onto the new KLT/LWP.

At a later time when the I/O operation completes, the kernel will notice that one of the kernel-level threads is pretty much constantly idle.So maybe the kernel can tell the user-level library that you no longer have access to this kernel-level thread, so you can't schedule on it.

This example shows that
-	The user-level library doesn't know what's happening in the kernel.
-	The kernel doesn't know what's happening at the user level either.
-	To correct for these issues the Solaris threading implementation introduced certain system calls and special signals to pass or request certain things among these two layers. 
Solaris allows for system calls and special signals to allow kernel and ULT library to interact and coordinate. 

### Lack of Thread Management Visibility
The kernel and the user level library don’t have visibility into each other’s activities:
-	The kernel sees all the kernel level threads, CPUs, and the kernel level.
-	At the user level, the user-level library sees the ULT that are part of that process and the KLT that are assigned to that process.
![thread design](/assets/img/Posts/GIOS/P2L4/img9.png)

One to One:
If the user level threads and the kernel level threads are using the one-to-one model, then every ULT will have one KLT associated with it. The user-level library will see many KLTs. But it will be the kernel that will manage those KLTs.

The user level library can request that one specific ULT to be bound to one KLT. The term “bound” was introduced in the Solaris paper. Clearly in one-to-one model every user level thread is bound to a kernel level thread. This can be done in many-to-one or many-to-many models as well.
(This “bound thread” is similar to the case where a KLT is permanently associated with a CPU in a multi-CPU system, which is call thread pinning). 

![thread design](/assets/img/Posts/GIOS/P2L4/img10.png)

The user level library is part of the user process’s address space. The program counter needs to jump to the user level library scheduler when:
-	The user level thread may explicitly yield
-	The timer set by user level library expires.
-	The user level library scheduler performs synchronization operation. For example. when we call a lock, that thread may not be able to run if it needs to be blocked; when we call an unlock operation, then we need to evaluate what is the new runnable thread that the scheduler should allocate on the CPU. In general, whenever we have a situation where a blocking ULT becomes runnable, we jump into the scheduler code.
-	The user level library scheduler is also triggered in response to certain events/signals that come either from timers or directly from the kernel.

![thread design](/assets/img/Posts/GIOS/P2L4/img11.png)

### Issues on Multiple CPUS

In a multi-CPU system, the kernel level threads that support a single process may be running on multiple CPUs, even concurrently. So we may have a situation when the user-level library that's operating in the context of one thread on one CPU needs to somehow impact what is running on another thread on another CPU.

![thread design](/assets/img/Posts/GIOS/P2L4/img12.png)
For example, we have three threads, T3, T2, and T1. The thread priority is that T3>T2>T1.
-	ULT2 is currently running in the context of one KLT on one CPU and currently holds a mutex.
-	ULT3 with the highest priority is waiting on the mutex and is blocked.
-	ULT1 is running on another KLT the other CPU.

At a later point, ULT2 releases the mutex (but is still running on the CPU).

-	As a result ULT3 becomes runnable and we have to make sure the ones with the highest priority are the ones that get executed.
-	We will involve the user level thread library so that it can preempt ULT1 and schedule ULT3 on that CPU.
-	We need to context switch ULT1. Because ULT1 is running on a different CPU, so we need to notify the other CPU to update its registers and program counter.
-	We cannot directly modify the register of one CPU when executing on another CPU.
-	This means that we need to send some signal/interrupt from the context of KLT on CPU1 to KLT on CPU2.
-	Once that signal happens, the user-level library on CPU2 will determine that it needs to schedule the highest priority ULT3 and block ULT1.

So when we have multi-CPU, multi-kernel and multi-user-level threads, interaction between the management of user level and kernel level becomes more complicated than the situation when there is only one CPU. 

### Synchronization Issues

![thread design](/assets/img/Posts/GIOS/P2L4/img13.png)
The owner of the mutex is running on one CPU1. When we request the same mutex from CPU2, it is possible that by the time we take ULT4, context switch it and place it on the queue that's associated with this mutex, ULT1 critical section might have already completed its execution. In such cases, we are better off just spinning on this CPU: just burning a few cycles, waiting a little bit until ULT1 actually releases the mutex.

-	So for super short critical sections, we do spin instead of block.

-	For long critical sections we will have the default behavior where a thread is actually properly blocked will be placed on a queue that's associated with a mutex until the mutex is freed.

We call such mutexes, which sometimes result in the thread spinning and other times blocking, adaptive mutexes.
![thread design](/assets/img/Posts/GIOS/P2L4/img14.png)
Once a thread exits, it should be destroyed and its data structure, stack, etc., should be freed.
However, since thread creation takes some time (data structures need to be created and initialized), it makes sense to reuse these data structures, essentially as if we're reusing the threads.
The way this is done is when a thread exits it's not immediately destroyed, the data structures are not immediately freed. Instead the thread is marked as it's on a death row.
Periodically a special reaper thread will perform garbage collection which means that it will actually go ahead and free up all of the data structures that are associated with the threads on the death row.
If a request for a thread comes in before the thread has been properly destroyed from the death row then its data structure and stack can be reused.
This will lead to performance gains since we don't have to wait for all the allocations.

### Interrupts vs Signals
![thread design](/assets/img/Posts/GIOS/P2L4/img15.png)
Interrupts are events that are generated externally to the CPU by components that are other than the CPU. Interrupts represent notifications sent to the CPU that some external event has occurred. This can be from an I/O device like a network device delivering an interrupt that a network packet has arrived, or from timers notifying the CPU that a timeout has occurred, or from other CPUs. 
Signals are events that are triggered by the software that's running on the CPU. They're either generated by software, or the CPU hardware itself triggers certain events that are basically interpreted as signals. What type of signals can occur on a given platform depends on the operating system. 

There are some aspects of interrupts and signals that are similar.

-	Both interrupts and signals have a unique identifier. And its value depends on the hardware in the case of interrupts, or on the operating system in the case of signals.

-	Both interrupts and signals can be masked to disable/suspend the notification that the signal/interrupt is delivering.  The interrupt mask is associated with a CPU because interrupts are delivered to the CPU as a whole. Whereas the signal mask is associated with a process because signals are delivered to individual processes.

-	Interrupt handlers are specified for the entire system by the operating system, but signal handlers can be specified per process.

### Visual Metaphor
![thread design](/assets/img/Posts/GIOS/P2L4/img16.png)
An interrupt is generated by an event that's external to the CPU and so is the snow storm external to  the toy shop.

The signal is generated from within the CPU, so is battery is directly caused by the toy shop worker fixing a toy.

-	First, each of these types of warnings need to be handled in specific ways.
-	Second, both of them can be ignored.
-	And last, we can think about both of them as being expected or unexpected.
 

In a toy shop, handling these types of events may be specified via safety protocols or certain hazard plans. This is not uncommon. There may be, however, situations in which it is appropriate to just continue working. And finally, situations like the fact that the battery died are frequent. They happen regularly, so they're expected. Whether or not it is expected for a snowstorm to occur, that will really depend on where the toy shop is located.

If we think about interrupts or signals, both of them are handled in a specific way and that is defined by the signal handler. Next, both interrupts and signals can be masked, as we said. And in that way, we can ignore them. And finally, as we previously discussed, these types of events can appear synchronously or asynchronously.

### Interrupt Handling
![thread design](/assets/img/Posts/GIOS/P2L4/img17.png)
Device interrupts the CPU by sending a signal through the interconnect that connects the device in the CPU complex. In the past we used dedicated wires. Most modern devices use MSI (Message Signal Interrupter) that can be carried on the same interconnect (e.g. PCI-E).

Based on the pins where it occurs or the MSI message, the interrupt can be uniquely identified.

Then based on the interrupt number (INT-#), the hardware defined message is then looked up in a table and the corresponding handler is called. The program counter is moved to that address of the handler code and the handler routine is then executed.

What types of interrupts can occur depends on the hardware while how it is handled is specified by the operating system.

### Signal Handling
Basically the same as interrupt discussion above except signal is not generated by external entity.
The OS defines the possible signals, e.g. SIGNAL-11, instead of the hardware.

There are default actions defined by the OS in response to signals, but each process can have their own defined signal handlers which respond in a user defined way in response to the OS signal.
![thread design](/assets/img/Posts/GIOS/P2L4/img18.png)
![thread design](/assets/img/Posts/GIOS/P2L4/img19.png)

### Why Disable Interrupts or Signals
One problem with interrupts/signals
- executed in the context of thread that was interrupted
- handled on thread stack which leads to discussion on why it should sometimes be disabled.
Example:
In the example, we have a thread and its PC and stack are shown in the right.
At some point of the execution, an interrupt occurs.
As a result, the program counter will change and it will start to point to the first instruction in the handler.
The stack pointer will remain the same. This could be nested if there are multiple singles/interrupts. In a nested fashion, they will keep executing on the stack of the thread which was interrupted.
The handling routine may need to access some state that some other thread is accessing, so we need to use mutex. However, if the thread that was interrupted already has the mutex that was needed by the handler routine, we have a deadlock situation.  The interrupted thread will not release its mutex until the handler routine completes its execution on its stack. The handler routine is blocked by the mutex.

To prevent this situation, one solution is to keep the handler code simple which means we can prohibit the handler code from using the mutex. If there is no possibility for the handler to be blocked on some mutex operation, then the deadlock situations will not occur. The problem with the method is too restrictive.
We could also introduce masks which will dynamically allow us to enable or disable whether the handling code will interrupt the mutex. The mask is a sequence of bits where each bit corresponds to a specific interrupt or signal. The value 0/1 indicates whether this specific interrupt or signal is disabled or enabled. When an event occurs, first the mask is checked. If the event is enabled, then we proceed with the handler procedure. If the event is disabled, then the interrupt/signal will remain suspended until a later time when the mask value changes. 

To solve the deadlock situation
-	The thread disables the interrupt.
-	The thread acquires the mutex. 
-	The thread executes the critical section.
-	The thread releases the mutex once it completes.
-	The thread re-enables the interrupt. The operation will allow the execution of the pending handler code.
The deadlock is avoided.

While an interrupt or signal is pending, other instances may occur. They will remain pending as well.
Once the event is re-enabled, the handler routine will typically only execute once. If we want to ensure the handler routine is executed more than once, it is not just sufficient to generate the signal more than once. 

### More on Signal Masks

Interrupt masks are maintained on a per CPU basis. What this means is that if the interrupt mask disables a particular interrupt, the hardware support for routing interrupt will just not deliver that interrupt to the CPU.

The signal mask depends on what the user level process. So the signal masks are per execution context. If a signal mask is disabled, the kernel sees that, and in that case, it will not interrupt the corresponding thread, i.e., the execution context.

### Interrupts on Multicore Systems
On the multi-CPU systems, the interrupt routing logic will direct the interrupt to any one of the CPUs that has that interrupt enabled. 

The reason that we put this crown here is, in multi-CPU systems we can only allow one of the CPUs to handle interrupts. It will be the only CPU that has interrupts enabled.

We will be able to avoid any of the overheads or perturbations related to interrupt handling from any of the other cores. The result will be improved performance. 

Instead of all CPUs receiving an interrupt, let only one CPU receive it. This reduces total cost.

### Types of Signals
Generally, there are two types of signals.
-	One-shot signal (overwrite behavior) One property of these signals is that we know that if there are multiple instances of the same signal, they will be handled at least once. So we have a situation, where if only one kind of that signal occurred, versus n signals of that same kind occurred, that only one execution of the actual signal is performed. 
-	Real Time Signals (queue behavior). real time signals that are supported in an operating system like Linux. And their behavior is that if a signal is called n times, then the handler is guaranteed to be called n times as well. 

### Interrupt Handler as Threads
The thread scheduler can schedule the original thread back on the CPU and that will continue executing.
Eventually the original thread will unlock the mutex, and the interrupt handler thread will be executed.
The way it happens is as follows.
-	Whenever a signal/interrupt occurs, it interrupts the execution of the thread.
-	By default the handling routine will start executing in the context of the interrupted thread using its stack, etc.
-	If the handling routine is going to perform synchronizing operation, that handler routine will execute on a separate thread. When the locking operation is reached, it will be placed in a wait queue associated with the mutex. The original thread will be scheduled.
-	When the unlock operation happens, we will go back and dequeue the handler routine from the mutex queue.

It's a dyanmic decision
- if the handler doesn't lock -> execute on interrupted threads stack
- if handler can block -> turn into real thread
it's expensive! so the optimization is to pre-creat and pre-initialize thread sturctures for interrupt routines

### Interrupts Top vs Bottom Half
When the interrupt first occurs, we are in the initial/top part of this interrupt handler. It may be necessary to disable certain interrupts to prevent deadlock situations.
When the interrupt is passed to a separate thread (bottom part), then we can re-enable those interrupts, because now the handler is in a separate and interrupt can be handled as any other thread in the system. There will not be any deadlock potential anymore.

-	The top will handle a minimum amount of processing. It is required to be non-blocking. The top half executes immediately when the interrupt occurs.
	
-	The bottom is allowed to perform any arbitrary/complex processing. The bottom half is like any other thread can be scheduled for a later time and can be blocked.

### Performance of Threads as Interrupts
Although creating a thread per interrupt is expensive, interrupts are actually quite rare, while critical sections are quite common.
With separate interrupt handling threads, the Top half avoids expensive calls to mask and unmask interrupts or change its priority before and after each critical section.
If every 10 seconds you have 100 critical sections and 1 interrupt, you save 12 * 100 instructions per mutex and pay the cost of 40 instructions for creating the 1 interrupt thread.
This gives you an overall gain of 1200 - 40 = 1160 instructions.


## Thread Performance Considerations - P2L5
![thread improvements](/assets/img/Posts/GIOS/P2L5/img1.png)

execution time = customer wait time, a.k.a.how long the average line is.

The image shows 6 worker threads but the calculations are done for only 5 worker threads. Also, In Boss-Worker model ((5*120) + (5*240) * 360)/11
should be ((5*120) + (5*240) + 360)/11

If we take a look at the Pipeline model, the first order took 120 ms to complete. 6 pipeline stages times 20 ms. The next one was already in the pipeline and once the first order completed, it had to finish the last stage of the pipeline, so its completion time will be 20 ms longer, so 140 ms. The one that came after that, another 20 ms longer for 160 ms, and so on until the very last order which will take 320 ms. So the average completion time for the Pipeline model is 220 ms.

### Are Threads Useful?
We know - They allow us to gain speed up because we can parallelize problems, they allow us to benefit from a hot cache because we can specialize what a particular thread is doing on a given CPU, and they lead to implementations that have lower memory requirements (efficiency) and where its cheaper to synchronize compared to multi-process implementations of the same problem. We said that threads are useful even on a single CPU because they let us hide the latency of I/O operations. 
However we might care about other things and rely on metrics to establish what's important for our software. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img2.png)

### Visual Metaphor
One example is throughput. The toy shop manager would want to make sure that this is as high as possible. Other things that may be important for the toy shop manager include how long does it take to react to a new order on average? Or what is the percentage of the workbenches that are used over a period of time? 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img3.png)

### Metrics!
![thread improvements](/assets/img/Posts/GIOS/P2L5/img4.png)

### Metrics Again!
![thread improvements](/assets/img/Posts/GIOS/P2L5/img5.png)
Highlighted metrics:

- wait time
- throughput
- platform efficiency
- performance per dollar
- performance per watt

You may have hear of the term SLA (Service Level Agreement), enterprise applications will give typically SLAs to their customers. One example, for instance, will be that you will get a response within 3 seconds. 

### Performance Metrics Summary
a metric is some measurable quantity that we can use to reason about the behavior of the system. Ideally, we will obtain these metrics, we will gather these measurements, running experiments using real software deployments on the real machines using real workloads. However, sometimes that really not an option, we cannot wait to actually deploy the software before we start measuring something about it or analyzing its behavior. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img6.png)
We refer to these experimental settings as a testbed so the testbed that tells us where the experiments were carried out and what were the relevant metrics that were measured.

### Are Threads Useful?

Depends on metrics and workload, however that is not an acceptable answer for the course. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img7.png)

### Multi Process versus Multi Threaded 
Comparing the two models using a web server as an example.
Simple steps in a web server for example:
![thread improvements](/assets/img/Posts/GIOS/P2L5/img8.png)

### Multi Process Web Server
This (Process 1 below) then clearly represents a single threaded process. One easy way to achieve concurrency is to have multiple instances of the same process and that way we have a multi-process (MP) implementation. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img9.png)

### Multi Threaded Web Server
An alternative to the multi-process model is to develop the web server as a multi-threaded (MT) application. So here (pointing at looping lines on the left side of the illustration below) we have multiple execution contexts, multiple threads within the same address space, and every single one of them is processing a request
![thread improvements](/assets/img/Posts/GIOS/P2L5/img10.png)

### Event Driven Model
The model we’ll talk about is called event-driven model. An event-driven application can be characterized as follows. The application is implemented in a single address space, there is basically only a single process and a single thread of control
![thread improvements](/assets/img/Posts/GIOS/P2L5/img11.png)
The main part of the process is an event dispatcher that continuously, in a loop, looks for incoming events and then based on those events invokes one or more of the registered [event] handlers. Here events correspond to some of the following things. Receipt of a request from the client browsers, the message received from the network. Completion of send, so once the server responds to the client request, the fact that the send completed, that’s another event as far as the system is concerned. Completion of a disk read operation, that’s another event that the system will need to know how to handle.

![thread improvements](/assets/img/Posts/GIOS/P2L5/img12.png)


### Concurrent Execution in Event-Driven Model
The way the event-driven model achieves concurrency is by interleaving the processing of multiple requests within the same execution context. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img13.png)

although we have only one execution context, only one thread, if we take a look, we have concurrent execution of multiple client requests. It just happens to be interleaved, given that there’s one execution context, however, there are multiple, at the same time multiple client requests being handled.

### Event-Driven Model Why

![thread improvements](/assets/img/Posts/GIOS/P2L5/img14.png)
threads can be useful because they help hide latency. The main take-away from that discussion was that, if a thread is going to wait more than twice the amount of time it takes to perform a context switch (t_idle > 2 * t_ctx_switch), then it makes sense to go ahead and context switch to another. So in the event-driven model, a request will be processed in the context of a single thread as long as it doesn’t have to wait.
![thread improvements](/assets/img/Posts/GIOS/P2L5/img15.png)

### Event-Driven Model Problems
![thread improvements](/assets/img/Posts/GIOS/P2L5/img16.png)
One way to circumvent this problem is to use asynchronous I/O operations. Asynchronous calls have the property that when the system call is made, the kernel captures enough information about the caller and where and how the data should be returned once it becomes available. Async calls also provide the caller with an opportunity to proceed executing something and then come back at a later time to check if the results of the asynchronous operation are already available. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img17.png)
What if Async Calls are not available?
![thread improvements](/assets/img/Posts/GIOS/P2L5/img18.png)
To deal with this problem, Vivek Pai’s paper proposed the use of helpers. When a handler needs to issue an I/O operation that can block, it passes it to the helper and returns to the event dispatcher. The helper will be the one that will handle the blocking I/O operation and interact with the dispatcher as necessary. 
At the time of the writing of the paper, another limitation was that not all kernels where multi-threaded, so basically not all kernels supported the one-to-one model that we talked about. In order to deal with this limitation, the decision in the paper was to make these helper entities processes. Therefore, they call this model AMPED (Asymmetric Multi-Process Event-Driven Model), it’s an event-driven model, it has multiple processes and these processes are asymmetric, the helper ones only deal with blocking I/O operations, and then the main one performs everything else. In principle, this same kind of idea could have applied in a multi-threaded scenario where the helpers are threads, not processes, so AMTED (Asymmetric Multi-Threaded Event-Driven Model), and in fact there is a follow-on on the Flash work that actually does this exact thing, the AMTED model.
The key benefits of the asymmetric model that we described, is that it resolves some of the limitations of the pure event-driven model in terms of what is required from the operating system, the dependence on asynchronous I/O calls and threading support. In addition, this model lets us achieve concurrency with a smaller memory footprint than either the MP or MTing model. In the MP/MT model a worker has to perform everything for a full request, so its memory requirements will be much more significant than the memory requirements of a helper entity. In addition, with the AMPED model we will have a helper entity only for the number of concurrent blocking I/O operations whereas in the MT or MP models we will have as many concurrent entities, as many processes or as many threads, as there are actual concurrent requests regardless of whether they block or not.

The downside is that although this works well with the server type applications, it is not necessarily as generally applicable to arbitrary applications. In addition, there are also some complexities with the routing of events in multi-CPU systems.

### Flash Web Server
Flash is an event-driven web server that follows the AMPED model, so basically it has asymmetric helper processes to deal with the blocking I/O operations
![thread improvements](/assets/img/Posts/GIOS/P2L5/img19.png)


Now we will outline some additional detail regarding some of the optimization that Flash applies, and this help us later understand some of the performance comparisons. The important thing is that these optimizations are really relevant to any web server. First of all, Flash performs application-level caching at multiple levels, and it does this on both data and computation. What we mean by this is, it common to cache files, this is what we call data caching
![thread improvements](/assets/img/Posts/GIOS/P2L5/img20.png)
Also Flash does some optimizations that take advantage of the networking hardware, and of the network interface card. For instance, all of the data structures are aligned so that it’s easy to perform DMA operations without copying data. Similarly, they use DMA operations that have scatter-gather support, and that really means that the header and the actual data don’t have to be aligned, one next to the other in memory, they can be sent from different memory locations, so there is a copy that’s avoided. 

### Apache Web Server
![thread improvements](/assets/img/Posts/GIOS/P2L5/img21.png)
From a very high-level the software architecture of Apache looks like this (box above). The core component provides the basic server-like capabilities, so this is accepting connections and managing concurrency. The various modules correspond to different types of functionalities that is executed on each request. Specific Apache deployment can be configured to include different types of modules. For instance, it can have certain security features, some management of dynamic content, or even some of the modules are really responsible for more basic http request processing.

The flow of control is sort of similar to the event-driven model that we saw in the sense that each request passes through all of the modules. Like in the event-driven model each request ultimately passes through all the handlers. However, Apache is a combination of a MP and a MT model. In Apache, a single process, a single instance, is internally a MTed Boss-Workers process that has dynamic management of the number of threads. 

### Experimental Methodology
![thread improvements](/assets/img/Posts/GIOS/P2L5/img22.png)
For the flash paper - what were they comparing?
![thread improvements](/assets/img/Posts/GIOS/P2L5/img23.png)
What were the workloads used?
![thread improvements](/assets/img/Posts/GIOS/P2L5/img24.png)
How were metrics used? 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img25.png)

### Experimental Results

![thread improvements](/assets/img/Posts/GIOS/P2L5/img26.png)

Now since real clients don’t behave like this synthetic workload, we need to look at what happens with some of the realistic traces, the Owlnet and the CS trace. 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img27.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img28.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img29.png)

### Summary of Performance Results

To summarize, the performance results for Flash show the following. When the data is in cache, the basic SPED model performs much better than the AMPED Flash because it doesn’t require the test for memory presence which was necessary in the AMPED Flash. Both SPED and the AMPED Flash are better than the MTed or MP models because they don’t incur any of the synchronization or context switching overheads that are necessary with these (MT/MP) models.

When the workload is disk-bound, however, AMPED performs much better than the single process event-driven (SPED) model because the single process model blocks since there’s no support for asynchronous I/O. AMPED Flash performs better than both the MTed and the MP model because it has much more memory efficient implementation and it doesn’t require the same level of context switching as in these (MT/MP) models. Again, only the number of concurrent I/O bound requests result in concurrent processes or concurrent threads in this model.

The model is not necessarily suitable for every single type of server process, there are certain challenges with event-driven architecture, we said some of these can come from the fact that we need to take advantage of multiple cores and we need to be able to route events to the appropriate core, in other cases perhaps the processing itself is not as suitable for this type of architecture. But if you look at some of the high performance server implementations that are in use today you will see that a lot of them do in fact use an event-driven model combined with asynchronous I/O support.
![thread improvements](/assets/img/Posts/GIOS/P2L5/img30.png)

### Advice on Designing Experiments

There is actually a lot of thought and planning that should go into designing relevant experiments.
![thread improvements](/assets/img/Posts/GIOS/P2L5/img31.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img32.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img33.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img34.png)
![thread improvements](/assets/img/Posts/GIOS/P2L5/img35.png)

### Advice on Running Experiments 
![thread improvements](/assets/img/Posts/GIOS/P2L5/img36.png)




# Useful Resources 
## Quizlet
- https://quizlet.com/user/quizlette8384429/folders/gios?tag=Midterm


## Textbooks and other reference materials:

- https://pages.cs.wisc.edu/~remzi/OSTEP/ Operating Systems: Three Easy Pieces

- chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://broman.dev/download/The%20Linux%20Programming%20Interface.pdf The Linux Programming Interface

- https://beej.us/guide/ 
Specifically - an introduction to socket programming:  
- https://beej.us/guide/bgnet0/

- https://www.kernel.org/doc/man-pages/ The Linux man-pages project


## C References:

- https://ocw.mit.edu/courses/6-087-practical-programming-in-c-january-iap-2010/pages/lecture-notes/

- https://learnxinyminutes.com/c/

- https://www.linkedin.com/learning/career-journey?initPlanV2=true&u=2163426

- http://goshdarnfunctionpointers.com/

- http://www.cprogramming.com/tutorial/function-pointers.html


Presentation on programming with C from Chris D. (with some updates from TAs)

Youtube of the Q&A C Video from Chris. D.

Harvard CS50 course on C and their reference site.

C/Standard Library docs

C, Pointers, and other lessons on C

Thread programming:

Posix Thread Programming tutorial

## Network programming resources:

https://beej.us/guide/bgnet/html/
https://www.codeproject.com/Articles/586000/Networking-and-Socket-programming-tutorial-in-C
http://www.binarytides.com/socket-programming-c-linux-tutorial/

## Debugging:


## some useful cheatsheets: 

Vim Cheat Sheet
TMUX Cheat Sheet
GIT Cheat Sheet
Linux source cross-references:

Linux Kernel Map
Linux source cross-reference
reference_material

# Past course resources
- https://github.gatech.edu/rdiaconescu3/6200_GIOS_flashcards/blob/main/GIOS_flashcards.apkg
- https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL3UvcyFBbXBpTXN0VG5CRWpzUUtpTk8yRzdiR21PLWJyP2U9Y2R1SXRJ&cid=23119C53CB32626A&id=23119C53CB32626A%216274&parId=root&o=OneUp
- https://www.omscs-notes.com/operating-systems/midterm-exam-review-questions/
